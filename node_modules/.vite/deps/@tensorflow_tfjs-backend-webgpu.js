import {
  shared_exports
} from "./chunk-DRB2A7XU.js";
import {
  Abs,
  Acos,
  Acosh,
  Add,
  AddN,
  All,
  Any,
  ArgMax,
  ArgMin,
  Asin,
  Asinh,
  Atan,
  Atan2,
  Atanh,
  AvgPool,
  AvgPoolGrad,
  BatchMatMul,
  BatchToSpaceND,
  Bincount,
  Cast,
  Ceil,
  ClipByValue,
  Complex,
  Concat,
  Conv2D,
  Conv2DBackpropFilter,
  Conv2DBackpropInput,
  Cos,
  Cosh,
  CropAndResize,
  Cumprod,
  Cumsum,
  DataStorage,
  DenseBincount,
  DepthToSpace,
  DepthwiseConv2dNative,
  Diag,
  Dilation2D,
  Einsum,
  Elu,
  Equal,
  Erf,
  Exp,
  ExpandDims,
  Expm1,
  FFT,
  Fill,
  FlipLeftRight,
  Floor,
  FloorDiv,
  FromPixels,
  FusedBatchNorm,
  FusedConv2D,
  FusedDepthwiseConv2D,
  GatherNd,
  GatherV2,
  Greater,
  GreaterEqual,
  IFFT,
  Identity,
  Imag,
  IsFinite,
  IsInf,
  IsNan,
  KernelBackend,
  LRN,
  LeakyRelu,
  Less,
  LessEqual,
  LinSpace,
  Log,
  Log1p,
  LogicalAnd,
  LogicalNot,
  LogicalOr,
  Max,
  MaxPool,
  Maximum,
  Mean,
  Min,
  Minimum,
  MirrorPad,
  Mod,
  Multiply,
  Neg,
  NonMaxSuppressionV3,
  NonMaxSuppressionV5,
  NotEqual,
  OneHot,
  OnesLike,
  Pack,
  PadV2,
  Pow,
  Prelu,
  Prod,
  Range,
  Real,
  RealDiv,
  Reciprocal,
  Relu,
  Relu6,
  Reshape,
  ResizeBilinear,
  ResizeNearestNeighbor,
  Reverse,
  RotateWithOffset,
  Round,
  Rsqrt,
  ScatterNd,
  SearchSorted,
  Select,
  Selu,
  Sigmoid,
  Sign,
  Sin,
  Sinh,
  Slice,
  Softmax,
  Softplus,
  SpaceToBatchND,
  SparseToDense,
  SplitV,
  Sqrt,
  Square,
  SquaredDifference,
  Step,
  StridedSlice,
  StringNGrams,
  Sub,
  Sum,
  Tan,
  Tanh,
  Tile,
  TopK,
  Transform,
  Transpose,
  Unpack,
  ZerosLike,
  _FusedMatMul,
  backend_util_exports,
  broadcast_util_exports,
  buffer,
  engine,
  env,
  kernel_impls_exports,
  registerBackend,
  registerKernel,
  slice_util_exports,
  sumOutType,
  upcastType,
  util_exports,
  zeros
} from "./chunk-A5UY2CDY.js";
import {
  __export
} from "./chunk-AC2VUBZ6.js";

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/flags_webgpu.js
var ENV = env();
ENV.registerFlag("WEBGPU_DEFERRED_SUBMIT_BATCH_SIZE", () => 15);
ENV.registerFlag("WEBGPU_CPU_FORWARD", () => true);
ENV.registerFlag("WEBGPU_MATMUL_PROGRAM_TYPE", () => -1);
ENV.registerFlag("WEBGPU_USE_NAIVE_CONV2D_TRANSPOSE", () => false);
ENV.registerFlag("WEBGPU_USE_LOW_POWER_GPU", () => false);
ENV.registerFlag("WEBGPU_CPU_HANDOFF_SIZE_THRESHOLD", () => 1e3);
ENV.registerFlag("WEBGPU_USE_PROFILE_TOOL", () => false);
ENV.registerFlag("WEBGPU_IMPORT_EXTERNAL_TEXTURE", () => true);
ENV.registerFlag("WEBGPU_USE_NAIVE_CONV2D_DEBUG", () => false);
ENV.registerFlag("WEBGPU_THRESHOLD_TO_INCREASE_WORKGROUPS_FOR_MATMUL", () => 0);
ENV.registerFlag("WEBGPU_CONV_SEPARATE_IM2COL_SHADER", () => false);

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/adapter_info.js
var AdapterInfo = class {
  constructor(adapterInfo) {
    if (adapterInfo) {
      this.vendor = adapterInfo.vendor;
      this.architecture = adapterInfo.architecture;
      this.intelGPUGeneration = this.getIntelGPUGeneration();
    }
  }
  getIntelGPUGeneration() {
    if (this.isIntel()) {
      if (this.architecture.startsWith("gen")) {
        return Number(this.architecture.match(/\d+/));
      } else if (this.architecture.startsWith("xe")) {
        return 12;
      }
    }
    return 0;
  }
  isIntel() {
    return this.vendor === "intel";
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/buffer_manager.js
var BufferManager = class {
  constructor(device) {
    this.device = device;
    this.numUsedBuffers = 0;
    this.numFreeBuffers = 0;
    this.freeBuffers = /* @__PURE__ */ new Map();
    this.usedBuffers = /* @__PURE__ */ new Map();
    this.numBytesUsed = 0;
    this.numBytesAllocated = 0;
  }
  acquireUploadBuffer(size, usage) {
    return this.acquireBuffer(size, usage, true);
  }
  acquireBuffer(size, usage, mappedAtCreation = false) {
    const key = getBufferKey(size, usage);
    if (!this.freeBuffers.has(key)) {
      this.freeBuffers.set(key, []);
    }
    if (!this.usedBuffers.has(key)) {
      this.usedBuffers.set(key, []);
    }
    this.numBytesUsed += size;
    this.numUsedBuffers++;
    if (this.freeBuffers.get(key).length > 0) {
      this.numFreeBuffers--;
      const newBuffer2 = this.freeBuffers.get(key).shift();
      this.usedBuffers.get(key).push(newBuffer2);
      return newBuffer2;
    }
    this.numBytesAllocated += size;
    const newBuffer = this.device.createBuffer({ size, usage, mappedAtCreation });
    this.usedBuffers.get(key).push(newBuffer);
    return newBuffer;
  }
  releaseBuffer(buffer2, size, usage) {
    if (this.freeBuffers.size === 0) {
      return;
    }
    const key = getBufferKey(size, usage);
    if (!this.freeBuffers.has(key)) {
      this.freeBuffers.set(key, []);
    }
    this.freeBuffers.get(key).push(buffer2);
    this.numFreeBuffers++;
    this.numUsedBuffers--;
    const bufferList = this.usedBuffers.get(key);
    const bufferIndex = bufferList.indexOf(buffer2);
    if (bufferIndex < 0) {
      throw new Error("Cannot release a buffer that was never provided by this buffer manager");
    }
    bufferList.splice(bufferIndex, 1);
    this.numBytesUsed -= size;
  }
  releaseUploadBuffer(buffer2, size, usage) {
    buffer2.mapAsync(GPUMapMode.WRITE).then(() => {
      this.releaseBuffer(buffer2, size, usage);
    }, (err) => {
    });
  }
  getNumUsedBuffers() {
    return this.numUsedBuffers;
  }
  getNumFreeBuffers() {
    return this.numFreeBuffers;
  }
  dispose() {
    this.freeBuffers.forEach((buffers, key) => {
      buffers.forEach((buffer2) => {
        buffer2.destroy();
      });
    });
    this.usedBuffers.forEach((buffers, key) => {
      buffers.forEach((buffer2) => {
        buffer2.destroy();
      });
    });
    this.freeBuffers = /* @__PURE__ */ new Map();
    this.usedBuffers = /* @__PURE__ */ new Map();
    this.numUsedBuffers = 0;
    this.numFreeBuffers = 0;
    this.numBytesUsed = 0;
    this.numBytesAllocated = 0;
  }
};
function getBufferKey(size, usage) {
  return `${size}_${usage}`;
}

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/texture_manager.js
var TextureManager = class {
  constructor(device) {
    this.device = device;
    this.numUsedTextures = 0;
    this.numFreeTextures = 0;
    this.freeTextures = /* @__PURE__ */ new Map();
    this.usedTextures = /* @__PURE__ */ new Map();
    this.numBytesUsed = 0;
    this.numBytesAllocated = 0;
  }
  acquireTexture(width, height, format, usage) {
    const bytesPerElement = getBytesPerElement(format);
    const byteSize = width * height * bytesPerElement;
    const key = getTextureKey(width, height, format, usage);
    if (!this.freeTextures.has(key)) {
      this.freeTextures.set(key, []);
    }
    if (!this.usedTextures.has(key)) {
      this.usedTextures.set(key, []);
    }
    this.numBytesUsed += byteSize;
    this.numUsedTextures++;
    if (this.freeTextures.get(key).length > 0) {
      this.numFreeTextures--;
      const newTexture2 = this.freeTextures.get(key).shift();
      this.usedTextures.get(key).push(newTexture2);
      return newTexture2;
    }
    this.numBytesAllocated += byteSize;
    const newTexture = this.device.createTexture({
      size: [width, height],
      format,
      usage
    });
    this.usedTextures.get(key).push(newTexture);
    return newTexture;
  }
  releaseTexture(texture, width, height, format, usage) {
    if (this.freeTextures.size === 0) {
      return;
    }
    const key = getTextureKey(width, height, format, usage);
    if (!this.freeTextures.has(key)) {
      this.freeTextures.set(key, []);
    }
    this.freeTextures.get(key).push(texture);
    this.numFreeTextures++;
    this.numUsedTextures--;
    const textureList = this.usedTextures.get(key);
    const textureIndex = textureList.indexOf(texture);
    if (textureIndex < 0) {
      throw new Error("Cannot release a texture that was never provided by this texture manager");
    }
    textureList.splice(textureIndex, 1);
    const bytesPerElement = getBytesPerElement(format);
    const byteSize = width * height * bytesPerElement;
    this.numBytesUsed -= byteSize;
  }
  getNumUsedTextures() {
    return this.numUsedTextures;
  }
  getNumFreeTextures() {
    return this.numFreeTextures;
  }
  dispose() {
    this.freeTextures.forEach((textures, key) => {
      textures.forEach((texture) => {
        texture.destroy();
      });
    });
    this.usedTextures.forEach((textures, key) => {
      textures.forEach((texture) => {
        texture.destroy();
      });
    });
    this.freeTextures = /* @__PURE__ */ new Map();
    this.usedTextures = /* @__PURE__ */ new Map();
    this.numUsedTextures = 0;
    this.numFreeTextures = 0;
    this.numBytesUsed = 0;
    this.numBytesAllocated = 0;
  }
};
function getTextureKey(width, height, format, usage) {
  return `${width}_${height}_${format}_${usage}`;
}
function getBytesPerElement(format) {
  if (format === "rgba8unorm") {
    return 16;
  } else {
    throw new Error(`${format} is not supported!`);
  }
}

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/shader_util.js
function symbolicallyComputeStrides(indicesArr, variableName) {
  if (Math.max(...indicesArr) > 3) {
    throw new Error("Cannot symbolically compute strides for rank > 4 tensor.");
  }
  const numCoords = indicesArr.length;
  const shape = indicesArr.map((d) => `${variableName}[${d}]`);
  const strides = new Array(numCoords - 1);
  strides[numCoords - 2] = shape[numCoords - 1];
  for (let i = numCoords - 3; i >= 0; --i) {
    strides[i] = `(${strides[i + 1]} * ${shape[i + 1]})`;
  }
  return strides;
}
var atomicAddSnippet = (ptr, v, type) => {
  if (type === "int32") {
    return `atomicAdd(${ptr}, bitcast<i32>(${v}));`;
  } else {
    return `
          {
            var oldValue = 0;
            loop {
              let newValueF32 = bitcast<f32>(oldValue) + (${v});
              let newValue = bitcast<i32>(newValueF32);
              let res = atomicCompareExchangeWeak(${ptr}, oldValue, newValue);
              if res.exchanged {
                break;
              }
              oldValue = res.old_value;
            }
          }`;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/webgpu_program.js
var compileProgram = (device, program, inputsData, output) => {
  const outputData = { dtype: output.dtype, shape: output.shape };
  const source = makeShader(inputsData, outputData, program);
  const module = device.createShaderModule({ code: source, label: program.constructor.name });
  const pipeline = device.createComputePipeline({
    compute: { module, entryPoint: "_start" },
    label: program.constructor.name,
    layout: "auto"
  });
  return pipeline;
};
function getCoordsDataType(rank) {
  if (rank <= 1) {
    return "i32";
  } else if (rank === 2) {
    return `vec2<i32>`;
  } else if (rank === 3) {
    return `vec3<i32>`;
  } else if (rank === 4) {
    return `vec4<i32>`;
  } else if (rank === 5) {
    return `vec5`;
  } else if (rank === 6) {
    return `vec6`;
  } else {
    throw Error(`GPU for rank ${rank} is not yet supported`);
  }
}
function getCoordsXYZ(index) {
  if (index === 0) {
    return "x";
  } else if (index === 1) {
    return "y";
  } else if (index === 2) {
    return "z";
  } else if (index === 3) {
    return "w";
  } else if (index === 4) {
    return "u";
  } else if (index === 5) {
    return "v";
  } else {
    throw Error(`Index ${index} is not yet supported`);
  }
}
function getMainHeaderString(...params) {
  let snippet;
  switch (params.length) {
    case 0:
      snippet = `
        fn main()
      `;
      break;
    case 1:
      snippet = `
        fn main(${params[0]} : i32)
      `;
      break;
    default:
      throw Error("Unreachable");
  }
  return snippet;
}
function getStartHeaderString(useGlobalIndex, program) {
  let snippet;
  snippet = `
     ${getWorkgroupSizeString(program)}
      fn _start(@builtin(local_invocation_id) LocalId : vec3<u32>,
                @builtin(global_invocation_id) GlobalId : vec3<u32>,
                @builtin(local_invocation_index) LocalIndex: u32,
                @builtin(workgroup_id) WorkgroupId : vec3<u32>,
                @builtin(num_workgroups) NumWorkgroups : vec3<u32>) {
        localId = LocalId;
        localIndex = LocalIndex;
        globalId = GlobalId;
        numWorkgroups = NumWorkgroups;
        workgroupId = WorkgroupId;
        ${useGlobalIndex ? `main(getGlobalIndex());` : `main();`};
      }
    `;
  return snippet;
}
function getWorkgroupSizeString(program) {
  return `
  @compute @workgroup_size(${program.workgroupSize[0]}, ${program.workgroupSize[1]}, ${program.workgroupSize[2]})
`;
}
function makeShader(inputInfo, outputData, program) {
  const prefixSnippets = [];
  const flatWorkgroupSize = program.workgroupSize[0] * program.workgroupSize[1] * program.workgroupSize[2];
  prefixSnippets.push(`

      var<private> localId: vec3<u32>;
      var<private> localIndex: u32;
      var<private> globalId: vec3<u32>;
      var<private> numWorkgroups: vec3<u32>;
      var<private> workgroupId: vec3<u32>;

      // Only used when the y/z dimension of workgroup size is 1.
      fn getGlobalIndex() -> i32 {
        ${isFlatDispatch(program) ? `  return i32(globalId.x);` : `  return i32((workgroupId.z * numWorkgroups.x * numWorkgroups.y +
                workgroupId.y * numWorkgroups.x + workgroupId.x) * ${flatWorkgroupSize}u +
                localIndex);
        `}
      }
    `);
  if (program.isFromPixels) {
    prefixSnippets.push(`
        struct Uniform {
          size            : i32,
          numChannels     : i32,
          outShapeStrides : vec2<i32>,
        };

        @group(0) @binding(0) var<storage, read_write> result: array<${mapToWgslTypes(outputData.dtype, program.isVec4)}>;
        @group(0) @binding(2) var<uniform> uniforms: Uniform;
      `);
    const useGlobalIndex2 = isFlatDispatchLayout(program);
    return [
      commonSnippet,
      prefixSnippets.join("\n"),
      getCoordsFromIndexSnippet(outputData.shape),
      program.getUserCode(),
      getStartHeaderString(useGlobalIndex2, program)
    ].join("\n");
  }
  let uniformDeclaration = "struct Uniforms { NAN : f32, INFINITY : f32, ";
  program.variableNames.forEach((x, i) => {
    const perDataType = getCoordsDataType(inputInfo[i].shape.length);
    uniformDeclaration += `${x.charAt(0).toLowerCase() + x.slice(1)}Shape : ${perDataType}, `;
  });
  const outputDataType = getCoordsDataType(outputData.shape.length);
  uniformDeclaration += `outShape : ${outputDataType}, `;
  const stridesLength = outputData.shape.length - 1;
  const stridesDataType = getCoordsDataType(stridesLength);
  uniformDeclaration += `
         outShapeStrides: ${stridesDataType}, `;
  if (program.size) {
    uniformDeclaration += "size : i32, ";
  }
  if (program.uniforms) {
    uniformDeclaration += program.uniforms;
  }
  uniformDeclaration += "};";
  uniformDeclaration = insertAlignment(uniformDeclaration);
  prefixSnippets.push(uniformDeclaration);
  if (program.atomic) {
    prefixSnippets.push(`
      @group(0) @binding(0) var<storage, read_write> result: array<atomic<i32>>;
    `);
  } else {
    prefixSnippets.push(`
      @group(0) @binding(0) var<storage, read_write> result: array<${mapToWgslTypes(outputData.dtype, program.isVec4)}>;
    `);
  }
  program.variableNames.forEach((x, i) => {
    prefixSnippets.push(`
      @group(0) @binding(${1 + i}) var<storage, read> ${x}: array<${program.variableTypes ? program.variableTypes[i] : mapToWgslTypes(inputInfo[i].dtype, program.isVec4)}>;
        `);
  });
  if (uniformDeclaration !== "") {
    prefixSnippets.push(`
      @group(0) @binding(${1 + program.variableNames.length}) var<uniform> uniforms: Uniforms;
      `);
  }
  const coordsSnippet = getOutputCoordsSnippet(outputData.shape, program.dispatchLayout);
  const sources = [
    commonSnippet,
    prefixSnippets.join("\n") + isInfSnippet,
    getCoordsFromIndexSnippet(outputData.shape),
    coordsSnippet,
    getOutputIndexFromCoordsSnippet(outputData.shape.length)
  ];
  if (!program.atomic) {
    sources.push(setOutputSnippet(outputData.shape, outputData.dtype, program.isVec4));
  }
  const inputSnippet = inputInfo.map((x, i) => getInputSnippet(x, outputData.shape, program.variableTypes ? program.variableTypes[i] === "vec4<f32>" : program.isVec4, program.dispatchLayout.x.length === outputData.shape.length)).join("\n");
  sources.push(inputSnippet);
  sources.push(program.getUserCode());
  const useGlobalIndex = isFlatDispatchLayout(program);
  sources.push(getStartHeaderString(useGlobalIndex, program));
  const source = sources.join("\n");
  return source;
}
function makeShaderKey(program, shapes, inputsData, output) {
  let key = program.shaderKey;
  if (program.isFromPixels) {
    return key;
  }
  const types = inputsData.map((d) => d.dtype).concat(output.dtype);
  const broadcastDims = inputsData.map((d) => backend_util_exports.getBroadcastDims(d.shape, output.shape));
  const inputShapesEqualsOutShape = inputsData.map((d) => util_exports.arraysEqual(d.shape, output.shape)).join("_");
  const broadcastDimsKey = broadcastDims.map((d) => d.join("_")).join(";");
  const flatDispatchString = isFlatDispatch(program) ? "flatDispatch" : "";
  key += "_" + (program.workgroupSize ? program.workgroupSize.join(",") : "") + shapes.map((shape) => shape.length).join(",") + types.join(",") + program.variableNames.join(",") + broadcastDimsKey + inputShapesEqualsOutShape + flatDispatchString;
  return key;
}
var commonSnippet = `
  struct vec5 {x: i32, y: i32, z: i32, w: i32, u: i32};
  struct vec6 {x: i32, y: i32, z: i32, w: i32, u: i32, v: i32};

  // Checks whether coordinates lie within the bounds of the shape.
  fn coordsInBounds2D(coord : vec2<i32>, shape : vec2<i32>) -> bool {
    return all(coord >= vec2<i32>(0)) && all(coord < shape);
  }
  fn coordsInBounds3D(coord : vec3<i32>, shape : vec3<i32>) -> bool {
    return all(coord >= vec3<i32>(0)) && all(coord < shape);
  }
  fn coordsInBounds4D(coord : vec4<i32>, shape : vec4<i32>) -> bool {
    return all(coord >= vec4<i32>(0)) && all(coord < shape);
  }

  fn getIndexFromCoords1D(coord : i32, shape : i32) -> i32 {
    return coord;
  }
  fn getIndexFromCoords2D(coords : vec2<i32>, shape : vec2<i32>) -> i32 {
    return dot(coords, vec2<i32>(shape.y, 1));
  }
  fn getIndexFromCoords3D(coords : vec3<i32>, shape : vec3<i32>) -> i32 {
    return dot(coords, vec3<i32>(shape.y * shape.z, shape.z, 1));
  }
  fn getIndexFromCoords4D(coords : vec4<i32>, shape : vec4<i32>) -> i32 {
    return dot(coords, vec4<i32>(
        shape.y * shape.z * shape.w, shape.z * shape.w, shape.w, 1));
  }
  fn getIndexFromCoords5D(coords : vec5, shape : vec5) -> i32 {
    let shapeStrides: vec5 = vec5(shape.y * shape.z * shape.w * shape.u, shape.z * shape.w * shape.u, shape.w * shape.u, shape.u, 1);
    return coords.x*shapeStrides.x + coords.y*shapeStrides.y + coords.z*shapeStrides.z + coords.w*shapeStrides.w + coords.u*shapeStrides.u;
  }
  fn getIndexFromCoords6D(coords : vec6, shape : vec6) -> i32 {
    let shapeStrides: vec6 = vec6(shape.y * shape.z * shape.w * shape.u * shape.v, shape.z * shape.w * shape.u * shape.v, shape.w * shape.u * shape.v, shape.u * shape.v, shape.v, 1);
    return coords.x*shapeStrides.x + coords.y*shapeStrides.y + coords.z*shapeStrides.z + coords.w*shapeStrides.w + coords.u*shapeStrides.u + coords.v*shapeStrides.v;
  }

  fn idiv(a: i32, b: i32, sign: f32) -> i32 {
    var res: i32 = a / b;
    let modulo: i32 = a % b;
    if (sign < 0. && modulo != 0) {
      res = res - 1;
    }
    return res;
  }

  // NaN defination in IEEE 754-1985 is :
  //   - sign = either 0 or 1.
  //   - biased exponent = all 1 bits.
  //   - fraction = anything except all 0 bits (since all 0 bits represents infinity).
  // https://en.wikipedia.org/wiki/IEEE_754-1985#Representation_of_non-numbers
  fn isnan(val: f32) -> bool {
    let floatToUint: u32 = bitcast<u32>(val);
    return (floatToUint & 0x7fffffffu) > 0x7f800000u;
  }
  fn isnanVec4(val : vec4<f32>) -> vec4<bool> {
    let floatToUint: vec4<u32> = bitcast<vec4<u32>>(val);
    return (floatToUint & vec4<u32>(0x7fffffffu)) > vec4<u32>(0x7f800000u);
  }
`;
var isInfSnippet = `
  fn isinf(val: f32) -> bool {
    return abs(val) == uniforms.INFINITY;
  }
`;
function getCoordsFromIndexSnippet(shape) {
  const rank = shape.length;
  if (rank <= 1) {
    return `fn getCoordsFromIndex(index : i32) -> i32 { return index; }`;
  }
  const strides = util_exports.computeStrides(shape);
  const dtype = getCoordsDataType(rank);
  const coords2 = [];
  for (let i = 0; i < rank; i++) {
    coords2.push(`d${i}`);
  }
  if (strides.length === 1) {
    return `    fn getCoordsFromIndex(index : i32) -> vec2<i32> {
      let d0 = index / uniforms.outShapeStrides; let d1 = index - d0 * uniforms.outShapeStrides;
      return vec2<i32>(d0, d1);
    }`;
  }
  let snippet;
  snippet = "var index2 = index;" + strides.map((_, i) => {
    const line1 = `let ${coords2[i]} = index2 / uniforms.outShapeStrides.${getCoordsXYZ(i)}`;
    const line2 = i === strides.length - 1 ? `let ${coords2[i + 1]} = index2 - ${coords2[i]} * uniforms.outShapeStrides.${getCoordsXYZ(i)}` : `index2 = index2 - ${coords2[i]} * uniforms.outShapeStrides.${getCoordsXYZ(i)}`;
    return `${line1}; ${line2};`;
  }).join("");
  return `
    fn getCoordsFromIndex(index : i32) -> ${dtype} {
      ${snippet}
      return ${dtype}(${coords2.join(",")});
    }
  `;
}
function getInputAtCoordsSnippet(inputInfo, isVec4) {
  const texName = inputInfo.name;
  const rank = inputInfo.shape.length;
  const type = getCoordsDataType(rank);
  const funcName = "get" + texName.charAt(0).toUpperCase() + texName.slice(1);
  const dims = ["d0", "d1", "d2", "d3", "d4", "d5"].slice(0, rank);
  const inputs = dims.map((d) => `${d} : i32`).join(", ");
  if (rank < 1) {
    if (isVec4) {
      return `
        fn ${funcName}() -> vec4<f32> {
          return vec4<f32>(${texName}[0]);
        }
      `;
    }
    return `
      fn ${funcName}() ->f32 {
        return f32(${texName}[0]);
      }
    `;
  }
  const shapeStr = `uniforms.${texName.charAt(0).toLowerCase() + texName.slice(1)}Shape`;
  let rankStr = `${rank}D`;
  if (rank === 0) {
    rankStr = "1D";
  }
  if (isVec4) {
    return `
      fn ${funcName}(${inputs}) -> vec4<f32> {
        return vec4<f32>(${texName}[getIndexFromCoords${rankStr}(${type}(${dims.join(",")}),
          ${shapeStr}) / 4]);
      }
      `;
  }
  return `
    fn ${funcName}(${inputs}) -> f32 {
      return f32(${texName}[getIndexFromCoords${rankStr}(${type}(${dims.join(",")}),
        ${shapeStr})]);
    }
   `;
}
function getInputByOutputSnippet(inputInfo, outShape, isVec4, isFlatDispatchLayout2) {
  const texName = inputInfo.name;
  const texFuncSnippet = texName.charAt(0).toUpperCase() + texName.slice(1);
  const funcName = "get" + texFuncSnippet + "ByOutput";
  const inRank = inputInfo.shape.length;
  const outRank = outShape.length;
  const type = getCoordsDataType(outRank);
  if (util_exports.arraysEqual(inputInfo.shape, outShape) && isFlatDispatchLayout2) {
    if (isVec4) {
      return `
      fn ${funcName}Index(globalIndex : i32) -> vec4<f32> {
        return vec4<f32>(${texName}[globalIndex]);
      }

      fn ${funcName}Coords(coords : ${type}) -> vec4<f32> {
        return vec4<f32>(${texName}[${outRank > 1 ? "getOutputIndexFromCoords(coords)" : "coords"} / 4]);
      }
      `;
    } else {
      return `
    fn ${funcName}Index(globalIndex : i32) -> f32 {
      return f32(${texName}[globalIndex]);
    }

    fn ${funcName}Coords(coords : ${type}) -> f32 {
      return f32(${texName}[${outRank > 1 ? "getOutputIndexFromCoords(coords)" : "coords"}]);
    }
    `;
    }
  }
  const broadcastDims = backend_util_exports.getBroadcastDims(inputInfo.shape, outShape);
  const rankDiff = outRank - inRank;
  let coordsSnippet = "";
  if (inRank === 0) {
    if (isVec4) {
      return `
    fn ${funcName}Index(globalIndex : i32) -> vec4<f32> {
      return get${texFuncSnippet}();
    }

    fn ${funcName}Coords(coords : ${type}) -> vec4<f32> {
      return get${texFuncSnippet}();
    }
  `;
    }
    return `
    fn ${funcName}Index(globalIndex : i32) -> f32{
      return get${texFuncSnippet}();
    }

    fn ${funcName}Coords(coords : ${type}) -> f32{
      return get${texFuncSnippet}();
    }
  `;
  } else {
    if (outRank < 2 && broadcastDims.length >= 1) {
      coordsSnippet = "coords = 0;";
    } else {
      coordsSnippet = broadcastDims.map((d) => `coords.${getCoordsXYZ(d + rankDiff)} = 0;`).join("\n");
    }
  }
  let unpackedCoordsSnippet = "";
  if (outRank < 2 && inRank > 0) {
    unpackedCoordsSnippet = "coords";
  } else {
    if (outRank > 1) {
      const coordsType = getCoordsDataType(inRank);
      const coordsValues = inputInfo.shape.map((s, i) => `coords.${getCoordsXYZ(i + rankDiff)}`).join(", ");
      unpackedCoordsSnippet = `${coordsType}(${coordsValues})`;
    } else {
      unpackedCoordsSnippet = "coords";
    }
  }
  const shapeStr = `uniforms.${texName.charAt(0).toLowerCase() + texName.slice(1)}Shape`;
  const rankStr = `${inRank}D`;
  if (isVec4) {
    return `
    fn ${funcName}Index(globalIndex : i32) -> vec4<f32> {
      var coords = getCoordsFromIndex(globalIndex);
      ${coordsSnippet}
      return ${texName}[getIndexFromCoords${rankStr}(${unpackedCoordsSnippet}, ${shapeStr}) / 4];
    }

    fn ${funcName}Coords(coordsIn : ${type}) -> vec4<f32> {
      var coords = coordsIn;
      ${coordsSnippet}
      return ${texName}[getIndexFromCoords${rankStr}(${unpackedCoordsSnippet}, ${shapeStr}) / 4];
    }
  `;
  }
  return `
  fn ${funcName}Index(globalIndex : i32) -> f32 {
    var coords = getCoordsFromIndex(globalIndex);
    ${coordsSnippet}
    return f32(${texName}[getIndexFromCoords${rankStr}(${unpackedCoordsSnippet}, ${shapeStr})]);
  }

  fn ${funcName}Coords(coordsIn : ${type}) -> f32 {
    var coords = coordsIn;
    ${coordsSnippet}
    return f32(${texName}[getIndexFromCoords${rankStr}(${unpackedCoordsSnippet}, ${shapeStr})]);
  }
`;
}
function getInputSnippet(inputInfo, outShape, isVec4, isFlatDispatchLayout2) {
  let res = getInputAtCoordsSnippet(inputInfo, isVec4);
  const inShape = inputInfo.shape;
  if (inShape.length <= outShape.length) {
    res += getInputByOutputSnippet(inputInfo, outShape, isVec4, isFlatDispatchLayout2);
  }
  return res;
}
function getOutputCoordsSnippet(outShape, dispatchLayout) {
  const { x, y = [], z = [] } = dispatchLayout;
  const outRank = outShape.length;
  const rank = x.length + y.length + z.length;
  if (rank !== outRank) {
    return "";
  }
  if (x.length === outRank) {
    const dtype2 = getCoordsDataType(outRank);
    const snippet2 = `fn getOutputCoords() -> ${dtype2}{
    let globalIndex = getGlobalIndex();
    return getCoordsFromIndex(globalIndex);
  }
  `;
    return snippet2;
  }
  let gatherDimensionsStr = "";
  const dims = [x, y, z];
  for (let i = 0; i < dims.length; i++) {
    const arr = dims[i];
    if (arr.length === 0) {
      continue;
    }
    if (arr.length === 1) {
      gatherDimensionsStr += `let d${arr[0]} = i32(globalId[${i}]);`;
    } else {
      const strides = symbolicallyComputeStrides(arr, "uniforms.outShape");
      gatherDimensionsStr += `var index${i} = i32(globalId[${i}]);`;
      for (let j = 0; j < strides.length; j++) {
        gatherDimensionsStr += `let d${arr[j]} = index${i} / ${strides[j]};`;
        if (j === strides.length - 1) {
          gatherDimensionsStr += `let d${arr[j + 1]} = index${i} - d${arr[j]} * ${strides[j]};`;
        } else {
          gatherDimensionsStr += `index${i} = index${i} - d${arr[j]} * ${strides[j]};`;
        }
      }
    }
  }
  const dimensions = [];
  for (let i = 0; i < rank; i++) {
    dimensions.push(`d${i}`);
  }
  const dtype = getCoordsDataType(rank);
  let snippet = `fn getOutputCoords() -> ${dtype} {
  ${gatherDimensionsStr}
`;
  if (dimensions.length === 0) {
    snippet += `return ${dtype}(0); }`;
  } else {
    snippet += `return ${dtype}(${dimensions.join(",")}); }`;
  }
  return snippet;
}
function getOutputIndexFromCoordsSnippet(outRank) {
  let snippet = "";
  switch (outRank) {
    case 0:
    case 1:
      snippet += `
        fn getOutputIndexFromCoords(coords : i32) -> i32 {
          return coords;
        }
        `;
      break;
    case 2:
      snippet += `
        fn getOutputIndexFromCoords(coords : vec2<i32>) -> i32 {
          return dot(coords, vec2<i32>(uniforms.outShapeStrides, 1));
        }
        `;
      break;
    case 3:
      snippet += `
        fn getOutputIndexFromCoords(coords : vec3<i32>) -> i32 {
          return dot(coords, vec3<i32>(uniforms.outShapeStrides.x, uniforms.outShapeStrides.y, 1));
        }
        `;
      break;
    case 4:
      snippet += `
        fn getOutputIndexFromCoords(coords : vec4<i32>) -> i32 {
          return dot(coords, vec4<i32>(
            uniforms.outShapeStrides.x, uniforms.outShapeStrides.y, uniforms.outShapeStrides.z, 1));
        }
        `;
      break;
    case 5:
      snippet += `
        fn getOutputIndexFromCoords(coords : vec5) -> i32 {
          return coords.x * uniforms.outShapeStrides.x +
              coords.y * uniforms.outShapeStrides.y +
              coords.z * uniforms.outShapeStrides.z +
              coords.w * uniforms.outShapeStrides.w +
              coords.u;
        }
        `;
      break;
    case 6:
      snippet += `
        fn getOutputIndexFromCoords(coords : vec6) -> i32 {
          return coords.x * uniforms.outShapeStrides.x +
              coords.y * uniforms.outShapeStrides.y +
              coords.z * uniforms.outShapeStrides.z +
              coords.w * uniforms.outShapeStrides.w +
              coords.u * uniforms.outShapeStrides.u +
              coords.v;
        }
        `;
      break;
    default:
      util_exports.assert(false, () => `Unsupported ${outRank}D shape`);
      break;
  }
  return snippet;
}
function isFlatDispatch(program) {
  return program.dispatch[1] === 1 && program.dispatch[2] === 1;
}
function mapToWgslTypes(type, isVec4) {
  if (type === "float32") {
    return isVec4 ? "vec4<f32>" : "f32";
  } else if (type === "int32") {
    return isVec4 ? "vec4<i32>" : "i32";
  } else if (type === "bool") {
    return isVec4 ? "vec4<i32>" : "i32";
  }
  return type;
}
function setOutputSnippet(outShape, outBufferType, isVec4) {
  const outRank = outShape.length;
  const wgslType = mapToWgslTypes(outBufferType, isVec4);
  let snippet;
  if (isVec4) {
    snippet = `fn setOutputAtIndex(flatIndex : i32, value : vec4<f32>) {
      result[flatIndex] = ${wgslType}(value);
    }
    fn setOutputAtIndexI32(flatIndex : i32, value : vec4<i32>) {
      result[flatIndex] = ${wgslType}(value);
    }`;
  } else {
    snippet = `fn setOutputAtIndex(flatIndex : i32, value : f32) {
      result[flatIndex] = ${wgslType}(value);
    }
    fn setOutputAtIndexI32(flatIndex : i32, value : i32) {
      result[flatIndex] = ${wgslType}(value);
    }`;
  }
  if (outRank >= 2) {
    const dims = ["d0", "d1", "d2", "d3", "d4", "d5"].slice(0, outRank);
    const type = getCoordsDataType(outRank);
    if (isVec4) {
      snippet += `
      fn setOutputAtCoords(${dims.map((d) => `${d} : i32`).join(", ")}, value : vec4<f32>) {
        let flatIndex = getOutputIndexFromCoords(${type}(${dims.join(", ")}));
        setOutputAtIndex(flatIndex / 4, value);
      }
      fn setOutputAtCoordsI32(${dims.map((d) => `${d} : i32`).join(", ")}, value : vec4<i32>) {
        let flatIndex = getOutputIndexFromCoords(${type}(${dims.join(", ")}));
        setOutputAtIndexI32(flatIndex / 4, value);
      }
    `;
    } else {
      snippet += `
      fn setOutputAtCoords(${dims.map((d) => `${d} : i32`).join(", ")}, value : f32) {
        let flatIndex = getOutputIndexFromCoords(${type}(${dims.join(", ")}));
        setOutputAtIndex(flatIndex, value);
      }
      fn setOutputAtCoordsI32(${dims.map((d) => `${d} : i32`).join(", ")}, value : i32) {
        let flatIndex = getOutputIndexFromCoords(${type}(${dims.join(", ")}));
        setOutputAtIndexI32(flatIndex, value);
      }
    `;
    }
  }
  return snippet;
}
function insertAlignment(uniformShader) {
  const curInsertRe = /(\w+)\s*:\s*vec(5|6)/g;
  uniformShader = uniformShader.replace(curInsertRe, (match) => {
    return "@align(16) " + match;
  });
  const preInsertRe = /vec(5|6)\s*,\s*(\w+)/g;
  uniformShader = uniformShader.replace(preInsertRe, (_, p1, p2) => {
    return `vec${p1}, @align(16) ${p2}`;
  });
  return uniformShader;
}
function isFlatDispatchLayout(program) {
  if (program.dispatchLayout.hasOwnProperty("y") && program.dispatchLayout.y.length !== 0) {
    return false;
  }
  if (program.dispatchLayout.hasOwnProperty("z") && program.dispatchLayout.z.length !== 0) {
    return false;
  }
  return true;
}

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/webgpu_util.js
var webgpu_util_exports = {};
__export(webgpu_util_exports, {
  GPUBytesPerElement: () => GPUBytesPerElement,
  MatMulProgramType: () => MatMulProgramType,
  assertNotComplex: () => assertNotComplex,
  computeDispatch: () => computeDispatch,
  computeWorkPerThreadForConv2d: () => computeWorkPerThreadForConv2d,
  computeWorkgroupInfoForMatMul: () => computeWorkgroupInfoForMatMul,
  computeWorkgroupSizeForConv2d: () => computeWorkgroupSizeForConv2d,
  flatDispatchLayout: () => flatDispatchLayout,
  isWebGPUSupported: () => isWebGPUSupported,
  tilesFitEvenlyIntoShape: () => tilesFitEvenlyIntoShape
});
var arrayProduct = (arr) => {
  let product = 1;
  for (let i = 0; i < arr.length; i++) {
    product *= arr[i];
  }
  return product;
};
function tilesFitEvenlyIntoShape(tileSize, shape) {
  if (tileSize.length !== shape.length) {
    throw new Error(`Cannot compute whether rank ${tileSize.length} tiles fit evenly into rank ${shape.length} shape - ranks must match.`);
  }
  return shape.every((dim, dimIdx) => dim % tileSize[dimIdx] === 0);
}
function computeDispatch(layout, outputShape, workgroupSize = [1, 1, 1], elementsPerThread = [1, 1, 1]) {
  const [dispatchX, dispatchY, dispatchZ] = [
    Math.ceil(arrayProduct(layout.x.map((d) => outputShape[d])) / (workgroupSize[0] * elementsPerThread[0])),
    layout.y ? Math.ceil(arrayProduct(layout.y.map((d) => outputShape[d])) / (workgroupSize[1] * elementsPerThread[1])) : 1,
    layout.z ? Math.ceil(arrayProduct(layout.z.map((d) => outputShape[d])) / (workgroupSize[2] * elementsPerThread[2])) : 1
  ];
  return [dispatchX, dispatchY, dispatchZ];
}
function computeWorkgroupInfoForMatMul(dimAOuter, dimInner, dimBOuter, transposeA = false) {
  const workgroupSize = [8, 8, 1];
  const elementsPerThread = [4, 4, 1];
  if (!transposeA) {
    if (dimAOuter <= 8) {
      elementsPerThread[1] = 1;
    }
    if (dimInner <= 16 && dimBOuter <= 16) {
      workgroupSize[0] = 4;
    }
  }
  return { workgroupSize, elementsPerThread };
}
function computeWorkgroupSizeForConv2d(layout, outputShape, isVec4 = false) {
  if (isVec4) {
    return [8, 8, 1];
  }
  const dim0 = arrayProduct(layout.x.map((d) => outputShape[d]));
  const dim1 = arrayProduct(layout.y.map((d) => outputShape[d]));
  if (dim0 <= 4) {
    return [4, 16, 1];
  }
  if (dim1 <= 4) {
    return [16, 4, 1];
  }
  return [16, 16, 1];
}
function computeWorkPerThreadForConv2d(layout, outputShape, isVec4 = false) {
  if (isVec4) {
    return [4, 4, 1];
  }
  const dim0 = arrayProduct(layout.x.map((d) => outputShape[d]));
  const dim1 = arrayProduct(layout.y.map((d) => outputShape[d]));
  if (dim0 <= 4) {
    return [1, 2, 1];
  }
  if (dim1 <= 4) {
    return [2, 1, 1];
  }
  return [2, 2, 1];
}
function flatDispatchLayout(shape) {
  return { x: shape.map((d, i) => i) };
}
function GPUBytesPerElement(dtype) {
  if (dtype === "float32" || dtype === "int32" || dtype === "bool" || dtype === "string") {
    return 4;
  } else if (dtype === "complex64") {
    return 8;
  } else {
    throw new Error(`Unknown dtype ${dtype}`);
  }
}
function isWebGPUSupported() {
  return (typeof window !== "undefined" || typeof WorkerGlobalScope !== "undefined") && !!navigator.gpu;
}
function assertNotComplex(tensor, opName) {
  if (!Array.isArray(tensor)) {
    tensor = [tensor];
  }
  tensor.forEach((t) => {
    if (t != null) {
      util_exports.assert(t.dtype !== "complex64", () => `${opName} does not support complex64 tensors in the WebGPU backend.`);
    }
  });
}
var MatMulProgramType;
(function(MatMulProgramType2) {
  MatMulProgramType2[MatMulProgramType2["MatMulReduceProgram"] = 0] = "MatMulReduceProgram";
  MatMulProgramType2[MatMulProgramType2["MatMulSplitKProgram"] = 1] = "MatMulSplitKProgram";
  MatMulProgramType2[MatMulProgramType2["MatMulSmallOutputSizeProgram"] = 2] = "MatMulSmallOutputSizeProgram";
  MatMulProgramType2[MatMulProgramType2["MatMulPackedProgram"] = 3] = "MatMulPackedProgram";
  MatMulProgramType2[MatMulProgramType2["MatMulMax"] = 4] = "MatMulMax";
})(MatMulProgramType || (MatMulProgramType = {}));

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/backend_webgpu.js
var CPU_HANDOFF_SIZE_THRESHOLD = env().getNumber("WEBGPU_CPU_HANDOFF_SIZE_THRESHOLD");
var reshapeDispatch = (device, program) => {
  const MAX_COMPUTE_PER_DIMENSION_DISPATCH_SIZE = device.limits.maxComputeWorkgroupsPerDimension;
  const layout = program["dispatchLayout"];
  const dispatch = program["dispatch"];
  if (dispatch.every((d) => d <= MAX_COMPUTE_PER_DIMENSION_DISPATCH_SIZE)) {
    return dispatch;
  }
  util_exports.assert(dispatch[0] > MAX_COMPUTE_PER_DIMENSION_DISPATCH_SIZE && layout.y === void 0 && layout.z === void 0, () => "Dispatch size exceeds WebGPU limits in Y or Z dimension.");
  let dispatchAverage = Math.ceil(Math.sqrt(dispatch[0]));
  if (dispatchAverage > MAX_COMPUTE_PER_DIMENSION_DISPATCH_SIZE) {
    dispatchAverage = Math.ceil(Math.cbrt(dispatch[0]));
    util_exports.assert(dispatchAverage <= MAX_COMPUTE_PER_DIMENSION_DISPATCH_SIZE, () => "Total dispatch size exceeds WebGPU maximum.");
    return [dispatchAverage, dispatchAverage, dispatchAverage];
  } else {
    return [dispatchAverage, dispatchAverage, 1];
  }
};
var WebGPUBackend = class extends KernelBackend {
  constructor(device, adapterInfo) {
    super();
    this.commandQueueOwnedIds = /* @__PURE__ */ new WeakSet();
    this.dispatchNumberInEncoder = 0;
    this.disposed = false;
    this.downloadWaitMs = 0;
    this.tensorDataPendingDisposal = [];
    this.stagingPendingDisposal = [];
    this.uniformPendingDisposal = [];
    this.uploadWaitMs = 0;
    if (!isWebGPUSupported()) {
      throw new Error("WebGPU is not supported on this device");
    }
    this.pipelineCache = {};
    this.device = device;
    this.queue = device.queue;
    this.currentCommandEncoder = null;
    this.currentComputePass = null;
    this.supportTimeQuery = device.features.has("timestamp-query-inside-passes");
    this.adapterInfo = new AdapterInfo(adapterInfo);
    this.thresholdToIncreaseWorkgroups = this.adapterInfo.intelGPUGeneration >= 12 ? 16 : 8;
    this.bufferManager = new BufferManager(this.device);
    this.textureManager = new TextureManager(this.device);
    this.tensorMap = new DataStorage(this, engine());
    if (this.supportTimeQuery) {
      this.querySet = this.device.createQuerySet({
        type: "timestamp",
        count: 2
      });
    }
    if (env().getBool("WEBGPU_USE_PROFILE_TOOL")) {
      this.dummyCanvas = document.createElement("canvas");
      this.dummyCanvas.width = 1;
      this.dummyCanvas.height = 1;
      this.dummyContext = this.dummyCanvas.getContext("webgpu");
      this.dummyContext.configure({
        device,
        format: "bgra8unorm"
      });
      document.body.appendChild(this.dummyCanvas);
    }
  }
  nextDataId() {
    return WebGPUBackend.nextDataId++;
  }
  floatPrecision() {
    return 32;
  }
  defaultGpuBufferUsage() {
    return GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC | GPUBufferUsage.COPY_DST;
  }
  /**
   * Dispose the memory if the dataId has 0 refCount. Return true if the memory
   * is released or memory is not managed in this backend, false if memory is
   * not cleared.
   * @param dataId
   * @oaram force Optional, remove the data regardless of refCount
   */
  disposeData(dataId, force = false) {
    if (this.tensorDataPendingDisposal.indexOf(dataId) >= 0) {
      return false;
    }
    if (!this.tensorMap.has(dataId)) {
      return true;
    }
    const tensorData = this.tensorMap.get(dataId);
    this.decRef(dataId);
    if (!force && tensorData.refCount > 0) {
      return false;
    }
    if (this.commandQueueOwnedIds.has(dataId)) {
      this.tensorDataPendingDisposal.push(dataId);
      return false;
    }
    const { complexTensorInfos } = this.tensorMap.get(dataId);
    if (complexTensorInfos != null) {
      this.disposeData(complexTensorInfos.real.dataId, force);
      this.disposeData(complexTensorInfos.imag.dataId, force);
    }
    this.releaseResource(dataId);
    this.tensorMap.delete(dataId);
    return true;
  }
  memory() {
    return {
      numBytesInGPU: this.bufferManager.numBytesUsed,
      numBytesAllocatedInGPU: this.bufferManager.numBytesAllocated,
      unreliable: false
    };
  }
  releaseResource(dataId) {
    const tensorData = this.tensorMap.get(dataId);
    if (!tensorData || !tensorData.resourceInfo) {
      return;
    }
    if (tensorData.external) {
      tensorData.resourceInfo = null;
      return;
    }
    if ("texture" in tensorData.resourceInfo) {
      const textureInfo = tensorData.resourceInfo;
      if (textureInfo.texture instanceof GPUTexture) {
        this.textureManager.releaseTexture(textureInfo.texture, textureInfo.width, textureInfo.height, textureInfo.format, textureInfo.usage);
      }
      textureInfo.texture = null;
    } else {
      const bufferInfo = tensorData.resourceInfo;
      this.bufferManager.releaseBuffer(bufferInfo.buffer, bufferInfo.size, bufferInfo.usage);
      bufferInfo.buffer = null;
    }
    tensorData.resourceInfo = null;
  }
  /** Return refCount of a `TensorData`. */
  refCount(dataId) {
    if (this.tensorMap.has(dataId)) {
      const tensorData = this.tensorMap.get(dataId);
      return tensorData.refCount;
    }
    return 0;
  }
  /** Increase refCount of a `TensorData`. */
  incRef(dataId) {
    const tensorData = this.tensorMap.get(dataId);
    tensorData.refCount++;
  }
  /** Decrease refCount of a `TensorData`. */
  decRef(dataId) {
    if (this.tensorMap.has(dataId)) {
      const tensorData = this.tensorMap.get(dataId);
      tensorData.refCount--;
    }
  }
  write(values, shape, dtype) {
    if (dtype === "complex64" && values != null) {
      throw new Error(`Cannot write to a complex64 dtype. Please use tf.complex(real, imag).`);
    }
    const dataId = { id: this.nextDataId() };
    this.tensorMap.set(dataId, { dtype, shape, values, refCount: 1 });
    return dataId;
  }
  move(dataId, values, shape, dtype, refCount) {
    if (dtype === "complex64") {
      throw new Error(`Cannot write to a complex64 dtype. Please use tf.complex(real, imag).`);
    }
    this.tensorMap.set(dataId, { dtype, shape, values, refCount });
  }
  submitQueue() {
    this.ensureComputePassEnded();
    this.queue.submit([this.currentCommandEncoder.finish()]);
    this.currentCommandEncoder = null;
    this.dispatchNumberInEncoder = 0;
    this.commandQueueOwnedIds = /* @__PURE__ */ new WeakSet();
    this.tensorDataPendingDisposal.forEach((d) => {
      this.releaseResource(d);
      this.tensorMap.delete(d);
    });
    this.uniformPendingDisposal.forEach((d) => this.bufferManager.releaseBuffer(d.buffer, d.size, d.usage));
    this.stagingPendingDisposal.forEach((d) => this.bufferManager.releaseUploadBuffer(d.buffer, d.size, d.usage));
    this.tensorDataPendingDisposal = [];
    this.uniformPendingDisposal = [];
    this.stagingPendingDisposal = [];
  }
  ensureCommandEncoderReady() {
    if (!this.currentCommandEncoder) {
      this.currentCommandEncoder = this.device.createCommandEncoder();
    }
  }
  ensureComputePassEnded() {
    if (this.currentComputePass) {
      this.currentComputePass.end();
      this.currentComputePass = null;
    }
  }
  getComputePass() {
    if (!this.currentComputePass) {
      this.currentComputePass = this.currentCommandEncoder.beginComputePass();
    }
    return this.currentComputePass;
  }
  async getBufferData(buffer2, size) {
    const staging = this.bufferManager.acquireBuffer(size, GPUBufferUsage.COPY_DST | GPUBufferUsage.MAP_READ);
    this.ensureCommandEncoderReady();
    this.ensureComputePassEnded();
    this.currentCommandEncoder.copyBufferToBuffer(buffer2, 0, staging, 0, size);
    this.submitQueue();
    await staging.mapAsync(GPUMapMode.READ);
    const values = staging.getMappedRange().slice(0);
    staging.unmap();
    if (staging != null) {
      this.bufferManager.releaseBuffer(staging, size, GPUBufferUsage.COPY_DST | GPUBufferUsage.MAP_READ);
    }
    if (env().getBool("WEBGPU_USE_PROFILE_TOOL")) {
      util_exports.assert(this.dummyContext !== void 0, () => `Fail to get context for profiling tool`);
      this.dummyContext.getCurrentTexture();
    }
    return values;
  }
  convertAndCacheOnCPU(dataId, data) {
    const tensorData = this.tensorMap.get(dataId);
    this.releaseResource(dataId);
    tensorData.values = data;
    return tensorData.values;
  }
  // TODO: Remove once this is fixed:
  // https://github.com/tensorflow/tfjs/issues/1595
  readSync(dataId) {
    const tensorData = this.tensorMap.get(dataId);
    const { values } = tensorData;
    if (values == null) {
      throw new Error("WebGPU readSync is only available for CPU-resident tensors.");
    }
    return values;
  }
  async read(dataId) {
    if (!this.tensorMap.has(dataId)) {
      throw new Error(`Tensor ${dataId} was not registered!`);
    }
    const tensorData = this.tensorMap.get(dataId);
    const { values } = tensorData;
    if (values != null) {
      return this.convertAndCacheOnCPU(dataId, values);
    }
    let vals;
    if (tensorData.dtype === "complex64") {
      const ps = await Promise.all([
        this.read(tensorData.complexTensorInfos.real.dataId),
        this.read(tensorData.complexTensorInfos.imag.dataId)
      ]);
      const realValues = ps[0];
      const imagValues = ps[1];
      vals = backend_util_exports.mergeRealAndImagArrays(realValues, imagValues);
    } else {
      const bufferInfo = tensorData.resourceInfo;
      const data = await this.getBufferData(bufferInfo.buffer, bufferInfo.size);
      vals = util_exports.convertBackendValuesAndArrayBuffer(data, tensorData.dtype);
    }
    this.convertAndCacheOnCPU(dataId, vals);
    return vals;
  }
  // The source GPUBuffer and destination GPUBuffer have the same size and
  // usage.
  copyBuffer(srcBuffer, size, usage) {
    const dstBuffer = this.bufferManager.acquireBuffer(size, usage);
    this.ensureCommandEncoderReady();
    this.ensureComputePassEnded();
    this.currentCommandEncoder.copyBufferToBuffer(srcBuffer, 0, dstBuffer, 0, size);
    this.submitQueue();
    return dstBuffer;
  }
  /**
   * Create a TF.js tensor out of an existing WebGPU buffer.
   */
  createTensorFromGPUData(values, shape, dtype) {
    let buffer2 = values.buffer;
    if (dtype === "complex64") {
      throw new Error(`Cannot write to a complex64 dtype. `);
    }
    const dataId = { id: this.nextDataId() };
    this.tensorMap.set(dataId, { dtype, shape, values: null, refCount: 1, external: values.zeroCopy });
    const tensorData = this.tensorMap.get(dataId);
    const size = GPUBytesPerElement(tensorData.dtype) * util_exports.sizeFromShape(tensorData.shape);
    if (values.buffer.size < size) {
      throw new Error(`GPUBuffer size(${values.buffer.size}) is smaller than tensor size(${size})!`);
    } else if ((values.buffer.usage & (GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC)) !== (GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC)) {
      throw new Error("GPUBuffer.usage should include GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC!");
    }
    if (values.zeroCopy !== true) {
      buffer2 = this.copyBuffer(buffer2, size, buffer2.usage);
    }
    tensorData.resourceInfo = { size: buffer2.size, usage: buffer2.usage, buffer: buffer2 };
    return engine().makeTensorFromDataId(dataId, shape, dtype, this);
  }
  /**
   * Read tensor to a new GPUBuffer.
   * @param dataId The source tensor.
   */
  readToGPU(dataId) {
    const srcTensorData = this.tensorMap.get(dataId);
    const { values, dtype, shape, resourceInfo } = srcTensorData;
    if (dtype === "complex64") {
      throw new Error("Does not support reading buffer for complex64 dtype.");
    }
    if (resourceInfo == null) {
      if (values != null) {
        throw new Error("Data is not on GPU but on CPU.");
      } else {
        throw new Error("There is no data on GPU or CPU.");
      }
    }
    const size = resourceInfo.size;
    const buffer2 = this.bufferManager.acquireBuffer(size, resourceInfo.usage);
    this.ensureCommandEncoderReady();
    this.ensureComputePassEnded();
    this.currentCommandEncoder.copyBufferToBuffer(resourceInfo.buffer, 0, buffer2, 0, size);
    this.submitQueue();
    const tensorInfo = this.makeTensorInfo(shape, dtype);
    const tensorRef = engine().makeTensorFromTensorInfo(tensorInfo);
    const tensorData = this.tensorMap.get(tensorInfo.dataId);
    tensorData.resourceInfo = { size, usage: this.defaultGpuBufferUsage(), buffer: buffer2 };
    return { tensorRef, buffer: buffer2, bufSize: size };
  }
  bufferSync(t) {
    const data = this.readSync(t.dataId);
    if (t.dtype === "string") {
      try {
        const strings = data.map((d) => util_exports.decodeString(d));
        return buffer(t.shape, t.dtype, strings);
      } catch (_a) {
        throw new Error("Failed to decode encoded string bytes into utf-8");
      }
    }
    return buffer(t.shape, t.dtype, data);
  }
  async time(f) {
    if (!this.supportTimeQuery) {
      console.warn(`This device doesn't support timestamp-query-inside-passes extension. Start Chrome browser with flag --disable-dawn-features=disallow_unsafe_apis then try again. Otherwise, zero will be shown for the kernel time when profiling mode is enabled. Using performance.now is not workable for webgpu since it doesn't support synchronous data read from GPU.`);
    }
    const oldActiveTimers = this.activeTimers;
    const newActiveTimers = [];
    let outerMostTime = false;
    if (this.programTimersStack == null) {
      this.programTimersStack = newActiveTimers;
      outerMostTime = true;
    } else {
      this.activeTimers.push(newActiveTimers);
    }
    this.activeTimers = newActiveTimers;
    f();
    const flattenedActiveTimerQueries = util_exports.flatten(this.activeTimers.map((d) => d.query)).filter((d) => d != null);
    const flattenedActiveTimerNames = util_exports.flatten(this.activeTimers.map((d) => d.name)).filter((d) => d != null);
    this.activeTimers = oldActiveTimers;
    if (outerMostTime) {
      this.programTimersStack = null;
    }
    const res = {
      uploadWaitMs: this.uploadWaitMs,
      downloadWaitMs: this.downloadWaitMs,
      kernelMs: null,
      wallMs: null
    };
    const kernelMs = await Promise.all(flattenedActiveTimerQueries);
    res["kernelMs"] = util_exports.sum(kernelMs);
    res["getExtraProfileInfo"] = () => kernelMs.map((d, i) => ({ name: flattenedActiveTimerNames[i], ms: d })).map((d) => `${d.name}: ${d.ms}`).join(", ");
    this.uploadWaitMs = 0;
    this.downloadWaitMs = 0;
    return res;
  }
  makeTensorInfo(shape, dtype, values) {
    if (dtype === "string" && values != null && values.length > 0 && util_exports.isString(values[0])) {
      values = values.map((d) => util_exports.encodeString(d));
    }
    const dataId = this.write(values, shape, dtype);
    return { dataId, shape, dtype };
  }
  tensorToBinding(tensor) {
    if (!tensor) {
      return null;
    }
    const tensorData = this.tensorMap.get(tensor.dataId);
    if ("texture" in tensorData.resourceInfo) {
      const info = tensorData.resourceInfo;
      if (info.texture instanceof GPUExternalTexture) {
        return info.texture;
      } else {
        return info.texture.createView();
      }
    }
    const bufferInfo = tensorData.resourceInfo;
    return { offset: 0, size: bufferInfo.size, buffer: bufferInfo.buffer };
  }
  async getQueryTime(query) {
    if (this.supportTimeQuery) {
      return this.getTimeFromQuerySet(query);
    } else {
      return 0;
    }
  }
  uploadToGPU(dataId) {
    const tensorData = this.tensorMap.get(dataId);
    if (tensorData.resourceInfo) {
      return;
    }
    const size = GPUBytesPerElement(tensorData.dtype) * util_exports.sizeFromShape(tensorData.shape);
    const buffer2 = this.bufferManager.acquireBuffer(size, this.defaultGpuBufferUsage());
    tensorData.resourceInfo = { size, usage: this.defaultGpuBufferUsage(), buffer: buffer2 };
    if (tensorData.values) {
      const stagingBuffer = this.bufferManager.acquireUploadBuffer(size, GPUBufferUsage.MAP_WRITE | GPUBufferUsage.COPY_SRC);
      const arrayBuffer = stagingBuffer.getMappedRange();
      if (tensorData.dtype === "int32" || tensorData.dtype === "bool") {
        new Int32Array(arrayBuffer).set(tensorData.values);
      } else {
        new Float32Array(arrayBuffer).set(tensorData.values);
      }
      stagingBuffer.unmap();
      this.ensureCommandEncoderReady();
      this.ensureComputePassEnded();
      this.currentCommandEncoder.copyBufferToBuffer(stagingBuffer, 0, buffer2, 0, size);
      const stagingInfo = {
        size,
        usage: GPUBufferUsage.MAP_WRITE | GPUBufferUsage.COPY_SRC,
        buffer: stagingBuffer
      };
      this.stagingPendingDisposal.push(stagingInfo);
    }
  }
  makeUniforms(programUniform) {
    let currentOffset = 0;
    let preLength = 0;
    const offsets = [];
    let maxAlignmentOfField = 1;
    programUniform.forEach((d) => {
      if (d.data.length === 0) {
        d.data = [1];
      }
      let baseAlignment;
      switch (d.data.length) {
        case 1:
          baseAlignment = 4;
          break;
        case 2:
          baseAlignment = 8;
          break;
        case 3:
          baseAlignment = 16;
          break;
        case 4:
          baseAlignment = 16;
          break;
        case 5:
          baseAlignment = 16;
          break;
        case 6:
          baseAlignment = 16;
          break;
        default:
          util_exports.assert(false, () => `Unsupported ${d.data.length}D shape`);
      }
      if (preLength === 5 || preLength === 6) {
        baseAlignment = 16;
      }
      if (baseAlignment > maxAlignmentOfField) {
        maxAlignmentOfField = baseAlignment;
      }
      currentOffset = Math.ceil(currentOffset / baseAlignment) * baseAlignment;
      preLength = d.data.length;
      offsets.push(currentOffset);
      currentOffset += d.data.length * 4;
    });
    currentOffset = Math.ceil(currentOffset / maxAlignmentOfField) * maxAlignmentOfField;
    const arrayBuffer = new ArrayBuffer(currentOffset);
    programUniform.forEach((d, i) => {
      const offset = offsets[i];
      if (d.type === "int32") {
        new Int32Array(arrayBuffer, offset, d.data.length).set(d.data);
      } else if (d.type === "uint32") {
        new Uint32Array(arrayBuffer, offset, d.data.length).set(d.data);
      } else {
        new Float32Array(arrayBuffer, offset, d.data.length).set(d.data);
      }
    });
    const uniformBuffer = this.bufferManager.acquireBuffer(currentOffset, GPUBufferUsage.COPY_DST | GPUBufferUsage.UNIFORM);
    this.queue.writeBuffer(uniformBuffer, 0, arrayBuffer, 0, currentOffset);
    const uniformInfo = {
      size: currentOffset,
      usage: GPUBufferUsage.COPY_DST | GPUBufferUsage.UNIFORM,
      buffer: uniformBuffer
    };
    this.uniformPendingDisposal.push(uniformInfo);
    return { offset: 0, size: currentOffset, buffer: uniformBuffer };
  }
  runWebGPUProgram(program, inputs, outputDtype, programDefinedUniform, output) {
    if (!output) {
      output = this.makeTensorInfo(program.outputShape, outputDtype);
    }
    if (util_exports.sizeFromShape(output.shape) === 0) {
      this.tensorMap.get(output.dataId).values = util_exports.getTypedArrayFromDType(output.dtype, 0);
      return output;
    }
    this.uploadToGPU(output.dataId);
    program.dispatch = reshapeDispatch(this.device, program);
    let programUniform = [];
    let bufferShapes = [];
    if (!program.isFromPixels) {
      programUniform.push({ type: "float32", data: [NaN] }, { type: "float32", data: [Infinity] });
      bufferShapes = inputs.concat(output).map((d) => d.shape);
      const uniformsType = "int32";
      bufferShapes.map((d) => {
        programUniform.push({ type: uniformsType, data: d });
      });
      const strides = util_exports.computeStrides(output.shape);
      programUniform.push({ type: uniformsType, data: strides });
      if (program.size) {
        const size = util_exports.sizeFromShape(program.outputShape);
        programUniform.push({ type: uniformsType, data: [program.isVec4 ? size / 4 : size] });
      }
    }
    const inputsData = inputs.map((input, i) => {
      if (input.dtype === "complex64") {
        throw new Error(`GPGPUProgram does not support complex64 input. For complex64 dtypes, please separate the program into real and imaginary parts.`);
      }
      this.uploadToGPU(input.dataId);
      return {
        // Returning dtype from tensorMap because it reflects dtype
        // of underlying buffer, rather than abstract dtype.
        dtype: this.tensorMap.get(input.dataId).dtype,
        shape: input.shape,
        name: program.variableNames[i]
      };
    });
    const key = makeShaderKey(program, bufferShapes, inputsData, output);
    let pipeline;
    if (key in this.pipelineCache) {
      pipeline = this.pipelineCache[key];
    } else {
      pipeline = compileProgram(this.device, program, inputsData, output);
      this.pipelineCache[key] = pipeline;
    }
    if (programDefinedUniform) {
      programUniform = [...programUniform, ...programDefinedUniform];
    }
    const bindings = [
      this.tensorToBinding(output),
      ...inputs.map((t) => this.tensorToBinding(t)),
      this.makeUniforms(programUniform)
    ];
    const bindGroup = this.device.createBindGroup({
      layout: pipeline.getBindGroupLayout(0),
      entries: bindings.map((b, i) => ({ binding: i, resource: b }))
    });
    this.ensureCommandEncoderReady();
    const pass = this.getComputePass();
    const shouldTimeProgram = this.activeTimers != null;
    if (shouldTimeProgram) {
      if (this.supportTimeQuery) {
        pass.writeTimestamp(this.querySet, 0);
      }
    }
    pass.setPipeline(pipeline);
    pass.setBindGroup(0, bindGroup);
    pass.dispatchWorkgroups(program.dispatch[0], program.dispatch[1], program.dispatch[2]);
    if (shouldTimeProgram) {
      if (this.supportTimeQuery) {
        pass.writeTimestamp(this.querySet, 1);
      }
    }
    this.dispatchNumberInEncoder++;
    inputs.forEach((input) => {
      this.commandQueueOwnedIds.add(input.dataId);
    });
    this.commandQueueOwnedIds.add(output.dataId);
    if (env().get("WEBGPU_DEFERRED_SUBMIT_BATCH_SIZE") <= this.dispatchNumberInEncoder) {
      this.submitQueue();
    }
    if (shouldTimeProgram) {
      this.activeTimers.push({
        name: program.constructor.name,
        query: this.getQueryTime(this.querySet)
      });
    }
    return output;
  }
  async getTimeFromQuerySet(querySet) {
    const queryBuffer = this.bufferManager.acquireBuffer(16, GPUBufferUsage.COPY_SRC | GPUBufferUsage.QUERY_RESOLVE);
    const dst = this.bufferManager.acquireBuffer(16, GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST);
    this.ensureCommandEncoderReady();
    this.ensureComputePassEnded();
    this.currentCommandEncoder.resolveQuerySet(querySet, 0, 2, queryBuffer, 0);
    this.currentCommandEncoder.copyBufferToBuffer(queryBuffer, 0, dst, 0, 16);
    this.submitQueue();
    await dst.mapAsync(GPUMapMode.READ);
    const arrayBuf = new BigUint64Array(dst.getMappedRange());
    const timeElapsedNanos = Number(arrayBuf[1] - arrayBuf[0]);
    dst.unmap();
    this.bufferManager.releaseBuffer(dst, 16, GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST);
    this.bufferManager.releaseBuffer(queryBuffer, 16, GPUBufferUsage.COPY_SRC | GPUBufferUsage.QUERY_RESOLVE);
    return timeElapsedNanos / 1e6;
  }
  shouldExecuteOnCPU(inputs, sizeThreshold = CPU_HANDOFF_SIZE_THRESHOLD) {
    return env().getBool("WEBGPU_CPU_FORWARD") && inputs.every((input) => this.tensorMap.get(input.dataId).resourceInfo == null && util_exports.sizeFromShape(input.shape) < sizeThreshold);
  }
  numDataIds() {
    return this.tensorMap.numDataIds() - this.tensorDataPendingDisposal.length;
  }
  dispose() {
    if (this.disposed) {
      return;
    }
    this.bufferManager.dispose();
    this.textureManager.dispose();
    this.disposed = true;
  }
};
WebGPUBackend.nextDataId = 0;

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/base.js
if (isWebGPUSupported()) {
  registerBackend(
    "webgpu",
    async () => {
      env().set("CHECK_COMPUTATION_FOR_ERRORS", false);
      const gpuDescriptor = {
        powerPreference: env().get("WEBGPU_USE_LOW_POWER_GPU") ? "low-power" : "high-performance"
      };
      const adapter = await navigator.gpu.requestAdapter(gpuDescriptor);
      const deviceDescriptor = {};
      if (adapter.features.has("timestamp-query-inside-passes")) {
        deviceDescriptor.requiredFeatures = // tslint:disable-next-line:no-any
        ["timestamp-query-inside-passes"];
      }
      const adapterLimits = adapter.limits;
      deviceDescriptor.requiredLimits = {
        "maxComputeWorkgroupStorageSize": adapterLimits.maxComputeWorkgroupStorageSize,
        "maxComputeWorkgroupsPerDimension": adapterLimits.maxComputeWorkgroupsPerDimension,
        "maxStorageBufferBindingSize": adapterLimits.maxStorageBufferBindingSize
      };
      const device = await adapter.requestDevice(deviceDescriptor);
      const adapterInfo = await adapter.requestAdapterInfo();
      return new WebGPUBackend(device, adapterInfo);
    },
    3
    /*priority*/
  );
}

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/binary_op_util.js
var BinaryOpType;
(function(BinaryOpType2) {
  BinaryOpType2[BinaryOpType2["ADD"] = 0] = "ADD";
  BinaryOpType2[BinaryOpType2["ATAN2"] = 1] = "ATAN2";
  BinaryOpType2[BinaryOpType2["COMPLEX_MULTIPLY_IMAG"] = 2] = "COMPLEX_MULTIPLY_IMAG";
  BinaryOpType2[BinaryOpType2["COMPLEX_MULTIPLY_REAL"] = 3] = "COMPLEX_MULTIPLY_REAL";
  BinaryOpType2[BinaryOpType2["DIV"] = 4] = "DIV";
  BinaryOpType2[BinaryOpType2["EQUAL"] = 5] = "EQUAL";
  BinaryOpType2[BinaryOpType2["GREATER"] = 6] = "GREATER";
  BinaryOpType2[BinaryOpType2["GREATER_EQUAL"] = 7] = "GREATER_EQUAL";
  BinaryOpType2[BinaryOpType2["INT_DIV"] = 8] = "INT_DIV";
  BinaryOpType2[BinaryOpType2["LESS"] = 9] = "LESS";
  BinaryOpType2[BinaryOpType2["LESS_EQUAL"] = 10] = "LESS_EQUAL";
  BinaryOpType2[BinaryOpType2["LOGICAL_AND"] = 11] = "LOGICAL_AND";
  BinaryOpType2[BinaryOpType2["LOGICAL_OR"] = 12] = "LOGICAL_OR";
  BinaryOpType2[BinaryOpType2["MAX"] = 13] = "MAX";
  BinaryOpType2[BinaryOpType2["MIN"] = 14] = "MIN";
  BinaryOpType2[BinaryOpType2["MOD"] = 15] = "MOD";
  BinaryOpType2[BinaryOpType2["MUL"] = 16] = "MUL";
  BinaryOpType2[BinaryOpType2["NOT_EQUAL"] = 17] = "NOT_EQUAL";
  BinaryOpType2[BinaryOpType2["POW"] = 18] = "POW";
  BinaryOpType2[BinaryOpType2["PRELU"] = 19] = "PRELU";
  BinaryOpType2[BinaryOpType2["SQUARED_DIFFERENCE"] = 20] = "SQUARED_DIFFERENCE";
  BinaryOpType2[BinaryOpType2["SUB"] = 21] = "SUB";
})(BinaryOpType || (BinaryOpType = {}));
var CHECK_NAN_SNIPPET = `
  if (isnan(a)) { return a; }
  if (isnan(b)) { return b; }
  `;
var CHECK_NAN_SNIPPET_VEC4 = `
  resultTemp = select(
      resultTemp, vec4<f32>(valueForNaN),
      vec4<bool>(isNaN) | isnanVec4(a) | isnanVec4(b));
  `;
var ADD = "return a + b;";
var COMPLEX_MULTIPLY_REAL = "return areal * breal - aimag * bimag;";
var COMPLEX_MULTIPLY_IMAG = "return areal * bimag + aimag * breal;";
var DIV = "return a / b;";
var EQUAL = "return f32(a == b);";
var EQUAL_VEC4 = "return vec4<f32>(a == b);";
var GREATER = "return f32(a > b);";
var GREATER_VEC4 = "return vec4<f32>(a > b);";
var GREATER_EQUAL = "return f32(a >= b);";
var GREATER_EQUAL_VEC4 = "return vec4<f32>(a >= b);";
var INT_DIV = `
  let s = sign(a) * sign(b);
  let ia = i32(round(a));
  let ib = i32(round(b));
  return f32(idiv(ia, ib, s));
`;
var INT_DIV_VEC4 = `
  let ia = vec4<i32>(round(a));
  let ib = vec4<i32>(round(b));
  let cond = ib != vec4<i32>(0);
  var resultTemp = vec4<i32>(0);
  let s = sign(a) * sign(b);

  // Windows (D3D) wants guaranteed non-zero int division at compile-time.
  if (cond[0]) {
    resultTemp[0] = idiv(ia[0], ib[0], s[0]);
  }
  if (cond[1]) {
    resultTemp[1] = idiv(ia[1], ib[1], s[1]);
  }
  if (cond[2]) {
    resultTemp[2] = idiv(ia[2], ib[2], s[2]);
  }
  if (cond[3]) {
    resultTemp[3] = idiv(ia[3], ib[3], s[3]);
  }
  return vec4<f32>(resultTemp);
`;
var LESS = "return f32(a < b);";
var LESS_VEC4 = "return vec4<f32>(a < b);";
var LESS_EQUAL = "return f32(a <= b);";
var LESS_EQUAL_VEC4 = "return vec4<f32>(a <= b);";
var LOGICAL_AND = "return f32(a >= 1.0 && b >= 1.0);";
var LOGICAL_AND_VEC4 = `return (vec4<f32>(a >= vec4<f32>(1.0)) *
  vec4<f32>(b >= vec4<f32>(1.0)));`;
var LOGICAL_OR = "return f32(a >= 1.0 || b >= 1.0);";
var LOGICAL_OR_VEC4 = `return min(vec4<f32>(a >= vec4<f32>(1.0)) +
  vec4<f32>(b >= vec4<f32>(1.0)), vec4<f32>(1.0));`;
var MOD = `
  ${CHECK_NAN_SNIPPET}
  if (b == 0.) {
    return uniforms.NAN;
  }
  var resultTemp = a % b;
  if ((a < 0. && b < 0.) || (a >= 0. && b > 0.)) {
    return resultTemp;
  } else {
    return (resultTemp + b) % b;
  }
`;
var MOD_VEC4 = `
  let isNaN = !vec4<bool>(b);
  let valueForNaN = uniforms.NAN;
  var resultTemp = vec4<f32>(a % b);
  ${CHECK_NAN_SNIPPET_VEC4}

  if (!((a[0] < 0. && b[0] < 0.) || (a[0] >= 0. && b[0] > 0.))) {
    resultTemp[0] = (resultTemp[0] + b[0]) % b[0];
  }
  if (!((a[1] < 0. && b[1] < 0.) || (a[1] >= 0. && b[1] > 0.))) {
    resultTemp[1] = (resultTemp[1] + b[1]) % b[1];
  }
  if (!((a[2] < 0. && b[2] < 0.) || (a[2] >= 0. && b[2] > 0.))) {
    resultTemp[2] = (resultTemp[2] + b[2]) % b[2];
  }
  if (!((a[3] < 0. && b[3] < 0.) || (a[3] >= 0. && b[3] > 0.))) {
    resultTemp[3] = (resultTemp[3] + b[3]) % b[3];
  }

  return resultTemp;
`;
var MUL = "return a * b;";
var NOT_EQUAL = `
  if (isnan(a) || isnan(b)) {
    return 1.0;
  }
  return f32(a != b);
`;
var NOT_EQUAL_VEC4 = `
  var resultTemp = vec4<f32>(a != b);
  let valueForNaN = 1.0;
  ${CHECK_NAN_SNIPPET_VEC4}

  return resultTemp;
`;
var POW = `
  if(a < 0.0 && floor(b) < b) {
    return uniforms.NAN;
  }
  if (b == 0.0) {
    return 1.0;
  }
  if (round(abs(b) % 2.0) != 1.0) {
    return pow(abs(a), b);
  }
  return sign(a) * pow(abs(a), b);
`;
var POW_VEC4 = `
  let isModRound1Bool = vec4<i32>(round(abs(b) % vec4<f32>(2.0))) == vec4<i32>(1);
  let isModRound1 = vec4<f32>(isModRound1Bool);
  let multiplier = sign(a) * isModRound1 + (vec4<f32>(1.0) - isModRound1);
  var resultTemp = multiplier * pow(abs(a), b);

  // Ensure that a^0 = 1, including 0^0 = 1 as this correspond to TF and JS
  let isExpZero = b == vec4<f32>(0.0);
  if (isExpZero.r) {
    resultTemp.r = 1.0;
  }
  if (isExpZero.g) {
    resultTemp.g = 1.0;
  }
  if (isExpZero.b) {
    resultTemp.b = 1.0;
  }
  if (isExpZero.a) {
    resultTemp.a = 1.0;
  }
  let isNaN = (a < vec4<f32>(0.0)) & (floor(b) < b);
  let valueForNaN = uniforms.NAN;
  ${CHECK_NAN_SNIPPET_VEC4}
  return resultTemp;
`;
var PRELU = `if (a < 0.0) { return b * a; }  return a;`;
var PRELU_VEC4 = `
  let aLessThanZero = vec4<f32>(a < vec4<f32>(0.0));
  return (aLessThanZero * (b * a)) + ((vec4<f32>(1.0) - aLessThanZero) * a);
`;
var SQUARED_DIFFERENCE = "return (a - b) * (a - b);";
var SUB = "return a - b;";
function getBinaryWithNanString(op, useVec4, valueForNaN = "uniforms.NAN") {
  const checkNanSnippet = useVec4 ? CHECK_NAN_SNIPPET_VEC4 : CHECK_NAN_SNIPPET;
  return useVec4 ? `
    let valueForNaN = ${valueForNaN};
    var resultTemp = vec4<f32>(${op}(a, b));
    ` + checkNanSnippet + `
    return resultTemp;
  ` : checkNanSnippet + `
    return ${op}(a, b);
  `;
}
function getBinaryOpString(type, useVec4) {
  switch (type) {
    case BinaryOpType.ADD:
      return ADD;
    case BinaryOpType.ATAN2:
      return getBinaryWithNanString("atan2", useVec4);
    case BinaryOpType.COMPLEX_MULTIPLY_IMAG:
      return COMPLEX_MULTIPLY_IMAG;
    case BinaryOpType.COMPLEX_MULTIPLY_REAL:
      return COMPLEX_MULTIPLY_REAL;
    case BinaryOpType.DIV:
      return DIV;
    case BinaryOpType.EQUAL:
      return useVec4 ? EQUAL_VEC4 : EQUAL;
    case BinaryOpType.GREATER:
      return useVec4 ? GREATER_VEC4 : GREATER;
    case BinaryOpType.GREATER_EQUAL:
      return useVec4 ? GREATER_EQUAL_VEC4 : GREATER_EQUAL;
    case BinaryOpType.INT_DIV:
      return useVec4 ? INT_DIV_VEC4 : INT_DIV;
    case BinaryOpType.LESS:
      return useVec4 ? LESS_VEC4 : LESS;
    case BinaryOpType.LESS_EQUAL:
      return useVec4 ? LESS_EQUAL_VEC4 : LESS_EQUAL;
    case BinaryOpType.LOGICAL_AND:
      return useVec4 ? LOGICAL_AND_VEC4 : LOGICAL_AND;
    case BinaryOpType.LOGICAL_OR:
      return useVec4 ? LOGICAL_OR_VEC4 : LOGICAL_OR;
    case BinaryOpType.MAX:
      return getBinaryWithNanString("max", useVec4);
    case BinaryOpType.MIN:
      return getBinaryWithNanString("min", useVec4);
    case BinaryOpType.MOD:
      return useVec4 ? MOD_VEC4 : MOD;
    case BinaryOpType.MUL:
      return MUL;
    case BinaryOpType.NOT_EQUAL:
      return useVec4 ? NOT_EQUAL_VEC4 : NOT_EQUAL;
    case BinaryOpType.POW:
      return useVec4 ? POW_VEC4 : POW;
    case BinaryOpType.PRELU:
      return useVec4 ? PRELU_VEC4 : PRELU;
    case BinaryOpType.SQUARED_DIFFERENCE:
      return SQUARED_DIFFERENCE;
    case BinaryOpType.SUB:
      return SUB;
    default:
      throw new Error(`BinaryType ${type} is not implemented!`);
  }
}

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/unary_op_util.js
var UnaryOpType;
(function(UnaryOpType2) {
  UnaryOpType2[UnaryOpType2["ABS"] = 0] = "ABS";
  UnaryOpType2[UnaryOpType2["ACOS"] = 1] = "ACOS";
  UnaryOpType2[UnaryOpType2["ACOSH"] = 2] = "ACOSH";
  UnaryOpType2[UnaryOpType2["ASIN"] = 3] = "ASIN";
  UnaryOpType2[UnaryOpType2["ASINH"] = 4] = "ASINH";
  UnaryOpType2[UnaryOpType2["ATAN"] = 5] = "ATAN";
  UnaryOpType2[UnaryOpType2["ATANH"] = 6] = "ATANH";
  UnaryOpType2[UnaryOpType2["CEIL"] = 7] = "CEIL";
  UnaryOpType2[UnaryOpType2["COS"] = 8] = "COS";
  UnaryOpType2[UnaryOpType2["COSH"] = 9] = "COSH";
  UnaryOpType2[UnaryOpType2["ELU"] = 10] = "ELU";
  UnaryOpType2[UnaryOpType2["ERF"] = 11] = "ERF";
  UnaryOpType2[UnaryOpType2["EXP"] = 12] = "EXP";
  UnaryOpType2[UnaryOpType2["EXPM1"] = 13] = "EXPM1";
  UnaryOpType2[UnaryOpType2["FLOOR"] = 14] = "FLOOR";
  UnaryOpType2[UnaryOpType2["IS_FINITE"] = 15] = "IS_FINITE";
  UnaryOpType2[UnaryOpType2["IS_INF"] = 16] = "IS_INF";
  UnaryOpType2[UnaryOpType2["IS_NAN"] = 17] = "IS_NAN";
  UnaryOpType2[UnaryOpType2["LINEAR"] = 18] = "LINEAR";
  UnaryOpType2[UnaryOpType2["LOG"] = 19] = "LOG";
  UnaryOpType2[UnaryOpType2["LOG1P"] = 20] = "LOG1P";
  UnaryOpType2[UnaryOpType2["LOGICAL_NOT"] = 21] = "LOGICAL_NOT";
  UnaryOpType2[UnaryOpType2["NEG"] = 22] = "NEG";
  UnaryOpType2[UnaryOpType2["RELU"] = 23] = "RELU";
  UnaryOpType2[UnaryOpType2["RELU6"] = 24] = "RELU6";
  UnaryOpType2[UnaryOpType2["LEAKYRELU"] = 25] = "LEAKYRELU";
  UnaryOpType2[UnaryOpType2["RECIPROCAL"] = 26] = "RECIPROCAL";
  UnaryOpType2[UnaryOpType2["ROUND"] = 27] = "ROUND";
  UnaryOpType2[UnaryOpType2["RSQRT"] = 28] = "RSQRT";
  UnaryOpType2[UnaryOpType2["SELU"] = 29] = "SELU";
  UnaryOpType2[UnaryOpType2["SIGMOID"] = 30] = "SIGMOID";
  UnaryOpType2[UnaryOpType2["SIGN"] = 31] = "SIGN";
  UnaryOpType2[UnaryOpType2["SIN"] = 32] = "SIN";
  UnaryOpType2[UnaryOpType2["SINH"] = 33] = "SINH";
  UnaryOpType2[UnaryOpType2["SOFTPLUS"] = 34] = "SOFTPLUS";
  UnaryOpType2[UnaryOpType2["SQRT"] = 35] = "SQRT";
  UnaryOpType2[UnaryOpType2["SQUARE"] = 36] = "SQUARE";
  UnaryOpType2[UnaryOpType2["STEP"] = 37] = "STEP";
  UnaryOpType2[UnaryOpType2["TAN"] = 38] = "TAN";
  UnaryOpType2[UnaryOpType2["TANH"] = 39] = "TANH";
  UnaryOpType2[UnaryOpType2["TO_INT"] = 40] = "TO_INT";
})(UnaryOpType || (UnaryOpType = {}));
var ABS = `return abs(a);`;
var ACOS = `
  if (abs(a) > 1.) {
    return uniforms.NAN;
  }
  return acos(a);
`;
var ACOSH = `
  if (a < 1.) {
    return uniforms.NAN;
  }
  return acosh(a);
`;
var ASIN = `
  if (abs(a) > 1.) {
    return uniforms.NAN;
  }
  return asin(a);
`;
var ASINH = `return asinh(a);`;
var ATAN = `
  if (isnan(a)) {
    return uniforms.NAN;
  }
  return atan(a);
`;
var ATANH = `
  if (abs(a) > 1.) {
    return uniforms.NAN;
  }
  if (a == 1.) {
    return uniforms.INFINITY;
  }
  if (a == -1.) {
    return -uniforms.INFINITY;
  }
  return atanh(a);
`;
var CEIL = `return ceil(a);`;
var COS = `return cos(a);`;
var COSH = `
  let e2x = exp(-a);
  return (e2x + 1.0 / e2x) / 2.0;
`;
var EXPM1 = `return exp(a) - 1.0;`;
var ELU = `if (a >= 0.0) { return a; }  return (exp(a) - 1.0);`;
var ELU_VEC4 = `
  var resFloat = exp(a) - vec4<f32>(1.0);
  if (a.r >= 0.0) {
    resFloat.r = a.r;
  }
  if (a.g >= 0.0) {
    resFloat.g = a.g;
  }
  if (a.b >= 0.0) {
    resFloat.b = a.b;
  }
  if (a.a >= 0.0) {
    resFloat.a = a.a;
  }
  return resFloat;
`;
var ERF = `
  // Error function is calculated approximately with elementary function.
  // See "Handbook of Mathematical Functions with Formulas,
  // Graphs, and Mathematical Tables", Abramowitz and Stegun.
  let p = ${backend_util_exports.ERF_P};
  let a1 = ${backend_util_exports.ERF_A1};
  let a2 = ${backend_util_exports.ERF_A2};
  let a3 = ${backend_util_exports.ERF_A3};
  let a4 = ${backend_util_exports.ERF_A4};
  let a5 = ${backend_util_exports.ERF_A5};

  let sign = sign(a);
  let absA = abs(a);
  let t = 1.0 / (1.0 + p * absA);
  return sign * (1.0 - (((((a5 * t + a4) * t) + a3) * t + a2) * t + a1) * t * exp(-absA * absA));
`;
var EXP = `return exp(a);`;
var FLOOR = `return floor(a);`;
var IS_FINITE = `return f32(!isnan(a) && !isinf(a));`;
var IS_INF = `return f32(isinf(a));`;
var IS_NAN = `return f32(isnan(a));`;
var LINEAR = `return a;`;
var LOG = `if (a < 0.0) { return uniforms.NAN; }
  return log(a);`;
var LOG1P = `
  if (isnan(a)) { return a; }
  return log(1.0 + a);
`;
var LOGICAL_NOT = `return f32(!(a >= 1.0));`;
var NEG = `return -a;`;
var LEAKYRELU = `if (a < 0.0) { return uniforms.alpha * a; } return a;`;
var LEAKYRELU_VEC4 = `
  let aLessThanZero = vec4<f32>(a < vec4<f32>(0.0));
  return (aLessThanZero * (uniforms.alpha * a)) + ((vec4<f32>(1.0) - aLessThanZero) * a);
`;
var RECIPROCAL = `return 1.0 / a;`;
var RELU = `return select(a, 0.0, a < 0.0);`;
var RELU6 = "return clamp(a, 0.0, 6.0);";
var RELU6_VEC4 = "return clamp(a, vec4<f32>(0.0, 0.0, 0.0, 0.0), vec4<f32>(6.0, 6.0, 6.0, 6.0));";
var RELU_VEC4 = `
  return select(a, vec4<f32>(0.0), a < vec4<f32>(0.0));
`;
var ROUND = `return round(a);`;
var RSQRT = `return inverseSqrt(a);`;
var SELU = `
  if (a >= 0.0) {
    return ${backend_util_exports.SELU_SCALE} * a;
  } else {
    return ${backend_util_exports.SELU_SCALEALPHA} * (exp(a) - 1.0);
  }
`;
var SIGMOID = `return 1.0 / (1.0 + exp(-1.0 * a));`;
var SIGN = `return sign(a);`;
var SIN = `return sin(a);`;
var SINH = `
  let e2x = exp(a);
  return (e2x - 1.0 / e2x) / 2.0;
`;
var SOFTPLUS = `
  let epsilon = 1.1920928955078125e-7;
  let threshold = log(epsilon) + 2.0;

  let too_large = a > -threshold;
  let too_small = a < threshold;
  let exp_a = exp(a);

  if (too_large) {
    return a;
  } else if (too_small) {
    return exp_a;
  } else {
    return log(exp_a + 1.0);
  }
`;
var SQRT = `return sqrt(a);`;
var SQUARE = `return a * a;`;
var STEP = `
  if (isnan(a)) {
    return a;
  }

  return select(uniforms.stepAlpha, 1.0, a > 0.0);
`;
var TAN = `return tan(a);`;
var TANH = `
  let e2x = exp(-2.0 * abs(a));
  return sign(a) * (1.0 - e2x) / (1.0 + e2x);
`;
var TO_INT = `return f32(i32((a)));`;
function getUnaryOpString(type, useVec4) {
  switch (type) {
    case UnaryOpType.ABS:
      return ABS;
    case UnaryOpType.ACOS:
      return ACOS;
    case UnaryOpType.ACOSH:
      return ACOSH;
    case UnaryOpType.ASIN:
      return ASIN;
    case UnaryOpType.ASINH:
      return ASINH;
    case UnaryOpType.ATAN:
      return ATAN;
    case UnaryOpType.ATANH:
      return ATANH;
    case UnaryOpType.COS:
      return COS;
    case UnaryOpType.COSH:
      return COSH;
    case UnaryOpType.CEIL:
      return CEIL;
    case UnaryOpType.ELU:
      return useVec4 ? ELU_VEC4 : ELU;
    case UnaryOpType.ERF:
      return ERF;
    case UnaryOpType.EXP:
      return EXP;
    case UnaryOpType.EXPM1:
      return EXPM1;
    case UnaryOpType.FLOOR:
      return FLOOR;
    case UnaryOpType.IS_FINITE:
      return IS_FINITE;
    case UnaryOpType.IS_INF:
      return IS_INF;
    case UnaryOpType.IS_NAN:
      return IS_NAN;
    case UnaryOpType.LINEAR:
      return LINEAR;
    case UnaryOpType.LOG:
      return LOG;
    case UnaryOpType.LOG1P:
      return LOG1P;
    case UnaryOpType.LOGICAL_NOT:
      return LOGICAL_NOT;
    case UnaryOpType.NEG:
      return NEG;
    case UnaryOpType.LEAKYRELU:
      return useVec4 ? LEAKYRELU_VEC4 : LEAKYRELU;
    case UnaryOpType.RECIPROCAL:
      return RECIPROCAL;
    case UnaryOpType.RELU:
      return useVec4 ? RELU_VEC4 : RELU;
    case UnaryOpType.RELU6:
      return useVec4 ? RELU6_VEC4 : RELU6;
    case UnaryOpType.ROUND:
      return ROUND;
    case UnaryOpType.RSQRT:
      return RSQRT;
    case UnaryOpType.SELU:
      return SELU;
    case UnaryOpType.SIGMOID:
      return SIGMOID;
    case UnaryOpType.SIGN:
      return SIGN;
    case UnaryOpType.SIN:
      return SIN;
    case UnaryOpType.SINH:
      return SINH;
    case UnaryOpType.SOFTPLUS:
      return SOFTPLUS;
    case UnaryOpType.SQRT:
      return SQRT;
    case UnaryOpType.SQUARE:
      return SQUARE;
    case UnaryOpType.STEP:
      return STEP;
    case UnaryOpType.TAN:
      return TAN;
    case UnaryOpType.TANH:
      return TANH;
    case UnaryOpType.TO_INT:
      return TO_INT;
    default:
      throw new Error(`BinaryType ${type} is not implemented!`);
  }
}

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/activation_util.js
var typeSnippet = (component) => {
  switch (component) {
    case 1:
      return "f32";
    case 2:
      return "vec2<f32>";
    case 3:
      return "vec3<f32>";
    case 4:
      return "vec4<f32>";
    default:
      throw new Error(`${component}-component is not supported.`);
  }
};
function activationFnSnippet(activation, hasPreluActivationWeights = false, packed = false, coordsLength = 3) {
  if (activation === null) {
    return "";
  }
  let activationOpSnippet = "";
  if (activation === "linear") {
    activationOpSnippet = getUnaryOpString(UnaryOpType.LINEAR);
  } else if (activation === "relu") {
    activationOpSnippet = getUnaryOpString(UnaryOpType.RELU, packed);
  } else if (activation === "elu") {
    activationOpSnippet = getUnaryOpString(UnaryOpType.ELU, packed);
  } else if (activation === "relu6") {
    activationOpSnippet = getUnaryOpString(UnaryOpType.RELU6, packed);
  } else if (activation === "prelu") {
    activationOpSnippet = getBinaryOpString(BinaryOpType.PRELU, packed);
  } else if (activation === "sigmoid") {
    activationOpSnippet = getUnaryOpString(UnaryOpType.SIGMOID, packed);
  } else if (activation === "leakyrelu") {
    activationOpSnippet = getUnaryOpString(UnaryOpType.LEAKYRELU, packed);
  } else {
    throw new Error(`Activation ${activation} has not been implemented for the WebGPU backend.`);
  }
  const elementSize = packed ? 4 : 1;
  const dataType = typeSnippet(elementSize);
  let activationFnSnippet2 = "";
  if (hasPreluActivationWeights) {
    activationFnSnippet2 = `
      fn activation(a : ${dataType}, coords : vec${coordsLength}<i32>) -> ${dataType} {
        let b = getPreluActivationWeightsByOutputCoords(coords);
        ${activationOpSnippet}
      }`;
  } else {
    activationFnSnippet2 = `
      fn activation(a : ${dataType}, coords : vec${coordsLength}<i32>) -> ${dataType} {
        ${activationOpSnippet}
      }`;
  }
  return activationFnSnippet2;
}
function biasActivationSnippet(hasBias, activation) {
  return `
      ${hasBias ? "value = value + getBiasByOutputCoords(coords);" : ""}
      ${activation ? "value = activation(value, coords);" : ""}
      `;
}

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/matmul_packed_webgpu.js
function matMulReadFnSource(transposeA, transposeB, fitAOuter = false, fitBOuter = false, fitInner = false, component = 1) {
  util_exports.assert(transposeA && component === 1 || !transposeA, () => `transposeA ${transposeA} is not compatible with component size ${component}`);
  const sampleA = `
      ${transposeA ? `value = getA(batch, col, row);` : `value = getA(batch, row, col);`}

    `;
  const sampleB = transposeB ? `value = getB(batch, col, row);` : `value = getB(batch, row, col);`;
  return `
  fn mm_readA(batch: i32, row: i32, colIn: i32) -> ${typeSnippet(component)} {
    var value = ${typeSnippet(component)}(0.0);
    let col = colIn * ${component};
    ${fitAOuter && fitInner ? sampleA : `
    ${transposeA ? `if(row < uniforms.dimAOuter && col < uniforms.dimInner)` : `if(row < uniforms.aShape[1] && col < uniforms.aShape[2])`}
    {
      ${sampleA}
    }
    `}
    return value;
  }

  fn mm_readB(batch: i32, row: i32, colIn: i32) -> ${typeSnippet(component)} {
    let col = colIn * ${component};
    var value = ${typeSnippet(component)}(0.0);
    ${sampleB}
    return value;
  }
  `;
}
function matMulReadWriteFnSource(hasBias, activation, transposeA, transposeB, fitAOuter = false, fitBOuter = false, fitInner = false, component = 1) {
  return `
  ${matMulReadFnSource(transposeA, transposeB, fitAOuter, fitBOuter, fitInner, component)}
  fn mm_write(batch: i32, row: i32, colIn: i32, valueIn: ${typeSnippet(component)}) {
    let col = colIn * ${component};
    ${fitAOuter && fitBOuter ? "" : "if (row < uniforms.dimAOuter && col < uniforms.dimBOuter)"}
    {
      var value = valueIn;
      let coords = vec3<i32>(batch, row, col);
      ${biasActivationSnippet(hasBias, activation)}
      setOutputAtCoords(coords[0], coords[1], coords[2], value);
    }
  }
  `;
}
var writeDataToSubAVec4Snippet = (transpose2, innerElementSize) => {
  if (transpose2) {
    return `
        mm_Asub[inputRow][inputCol] = mm_readA(batchA,
          kStart + inputRow,
          globalRowStart / ${innerElementSize} + inputCol);
        `;
  } else {
    return `
        mm_Asub[inputRow][inputCol] = mm_readA(batchA,
          globalRow + innerRow,
          kStart / ${innerElementSize} + inputCol);
        `;
  }
};
var calculateResultSnippet = (transposeA, innerElementSize, rowPerThread) => {
  if (transposeA) {
    return `
        let ACached0 = mm_Asub[k * ${innerElementSize}][localRow];
        let ACached1 = mm_Asub[k * ${innerElementSize} + 1][localRow];
        let ACached2 = mm_Asub[k * ${innerElementSize} + 2][localRow];
        ${innerElementSize === 3 ? "" : `let ACached3 = mm_Asub[k * ${innerElementSize} + 3][localRow];`}
        for (var i = 0; i < ${rowPerThread}; i++) {
          acc[i] = BCached0 * ACached0[i] + acc[i];
          acc[i] = BCached1 * ACached1[i] + acc[i];
          acc[i] = BCached2 * ACached2[i] + acc[i];
          ${innerElementSize === 3 ? "" : "acc[i] = BCached3 * ACached3[i] + acc[i];"}
        }`;
  } else {
    return `
        for (var i = 0; i < ${rowPerThread}; i++) {
          let ACached = mm_Asub[tileRow + i][k];
          acc[i] = BCached0 * ACached.x + acc[i];
          acc[i] = BCached1 * ACached.y + acc[i];
          acc[i] = BCached2 * ACached.z + acc[i];
          ${innerElementSize === 3 ? "" : "acc[i] = BCached3 * ACached.w + acc[i];"}
        }`;
  }
};
function makeMatMulPackedVec4Source(workPerThread, workgroupSize, transposeA = false, tileInner = 32, splitK = false, splitedDimInner = 32, isVectorA = false, broadcastBatch = false) {
  const tileAOuter = workgroupSize[1] * workPerThread[1];
  const tileBOuter = workgroupSize[0] * workPerThread[0];
  const tileAWidth = transposeA ? tileAOuter : tileInner;
  const tileAHight = transposeA ? tileInner : tileAOuter;
  const innerElementSize = tileAWidth / workgroupSize[0];
  const rowPerThreadB = tileInner / workgroupSize[1];
  const rowPerThread = workPerThread[1];
  util_exports.assert((transposeA && innerElementSize === 4 && workPerThread[1] === 4 || !transposeA && (innerElementSize === 3 || innerElementSize === 4)) && tileAWidth % workgroupSize[0] === 0 && tileInner % workgroupSize[1] === 0 && workPerThread[0] === 4, () => `If transposeA ${transposeA} is true, innerElementSize ${innerElementSize} and workPerThread[1] ${workPerThread[1]} must be 4.
          Otherwise, innerElementSize ${innerElementSize} must be 3 or 4.
      tileAWidth ${tileAWidth} must be divisible by workgroupSize[0]${workgroupSize[0]}. tileInner ${tileInner} must be divisible by workgroupSize[1] ${workgroupSize[1]}. colPerThread ${workPerThread[0]} must be 4.`);
  return `
  var<workgroup> mm_Asub : array<array<vec${innerElementSize}<f32>, ${tileAWidth / innerElementSize}>, ${tileAHight}>;
  var<workgroup> mm_Bsub : array<array<vec4<f32>, ${tileBOuter / workPerThread[0]}>, ${tileInner}>;

  ${getMainHeaderString()} {
    let localRow = i32(localId.y);
    let tileRow = ${isVectorA ? "0" : `localRow * ${rowPerThread}`};
    let tileCol = i32(localId.x);

    let globalRow = ${isVectorA ? "0" : `i32(globalId.y) * ${rowPerThread}`};
    let globalCol = i32(globalId.x);
    let batch = ${splitK ? "0" : "i32(globalId.z)"};
    let batchA = ${splitK || !broadcastBatch ? "batch" : "batch % uniforms.aShape[0]"};
    let batchB = ${splitK || !broadcastBatch ? "batch" : "batch % uniforms.bShape[0]"};
    let globalRowStart = i32(workgroupId.y) * ${tileAOuter};

    let numTiles = ${splitK ? `${Math.ceil(splitedDimInner / tileInner)}` : `(uniforms.dimInner - 1) / ${tileInner} + 1`};
    var kStart = ${splitK ? `i32(globalId.z) * ${splitedDimInner}` : "0"};

    var acc: array<vec4<f32>, ${rowPerThread}>;

    // Loop over shared dimension.
    let tileRowB = localRow * ${rowPerThreadB};
    for (var t = 0; t < numTiles; t++) {
        // Load one tile of A into local memory.
        for (var innerRow = 0; innerRow < ${rowPerThread}; innerRow++) {
            let inputRow = tileRow + innerRow;
            let inputCol = tileCol;
            ${writeDataToSubAVec4Snippet(transposeA, innerElementSize)}
        }

        // Load one tile of B into local memory.
        for (var innerRow = 0; innerRow < ${rowPerThreadB}; innerRow++) {
            let inputRow = tileRowB + innerRow;
            let inputCol = tileCol;
            mm_Bsub[inputRow][inputCol] = mm_readB(batchB, kStart + inputRow, globalCol);
        }
        kStart = kStart + ${tileInner};
        workgroupBarrier();

        // Compute acc values for a single thread.
        for (var k = 0; k < ${tileInner / innerElementSize}; k++) {
            let BCached0 = mm_Bsub[k * ${innerElementSize}][tileCol];
            let BCached1 = mm_Bsub[k * ${innerElementSize} + 1][tileCol];
            let BCached2 = mm_Bsub[k * ${innerElementSize} + 2][tileCol];
            ${innerElementSize === 3 ? "" : `let BCached3 = mm_Bsub[k * ${innerElementSize} + 3][tileCol];`}

            ${calculateResultSnippet(transposeA, innerElementSize, rowPerThread)}
        }

        workgroupBarrier();
    }

    for (var innerRow = 0; innerRow < ${rowPerThread}; innerRow++) {
        mm_write(batch, globalRow + innerRow, globalCol, acc[innerRow]);
    }
  }`;
}
var writeDataToSubASnippet = (transpose2) => {
  if (transpose2) {
    return `
        mm_Asub[inputRow][inputCol] = mm_readA(batchA,
          kStart + inputRow,
          globalRowStart + inputCol);
        `;
  } else {
    return `
        mm_Asub[inputRow][inputCol] = mm_readA(batchA,
          globalRowStart + inputRow,
          kStart + inputCol);
        `;
  }
};
var readDataFromSubASnippet = (transposeA) => {
  return transposeA ? "let ACached = mm_Asub[k][tileRow + innerRow];" : "let ACached = mm_Asub[tileRow + innerRow][k];";
};
function makeMatMulPackedSource(workPerThread, workgroupSize, transposeA = false, tileInner = 32, splitK = false, splitedDimInner = 32, sequentialAccessByThreads = false, broadcastBatch = false) {
  const tileAOuter = workPerThread[1] * workgroupSize[1];
  const tileBOuter = workPerThread[0] * workgroupSize[0];
  const tileAWidth = transposeA ? tileAOuter : tileInner;
  const tileAHight = transposeA ? tileInner : tileAOuter;
  util_exports.assert(tileAHight % workgroupSize[1] === 0 && tileAWidth % workgroupSize[0] === 0 && tileInner % workgroupSize[1] === 0, () => `tileAHight ${tileAHight} must be divisible by workgroupSize[1]${workgroupSize[1]}, tileAWidth ${tileAWidth} must be divisible by workgroupSize[0]${workgroupSize[0]}, tileInner ${tileInner} must be divisible by workgroupSize[1]${workgroupSize[1]}`);
  const rowPerThreadA = tileAHight / workgroupSize[1];
  const colPerThreadA = tileAWidth / workgroupSize[0];
  const rowPerThreadB = tileInner / workgroupSize[1];
  const rowPerThread = workPerThread[1];
  const colPerThread = workPerThread[0];
  const matmulSnippet = sequentialAccessByThreads ? `
      let localRow = i32(localId.y);
      let localCol = i32(localId.x);
      let globalRowStart = i32(workgroupId.y) * ${tileAOuter};
      let globalColStart = i32(workgroupId.x) * ${tileBOuter};

      // Loop over shared dimension.
      for (var t = 0; t < numTiles; t++) {
        // Load one tile of A into local memory.
        for (var inputRow = localRow; inputRow < ${tileAHight}; inputRow = inputRow + ${workgroupSize[1]}) {
          for (var inputCol = localCol; inputCol < ${tileAWidth}; inputCol = inputCol + ${workgroupSize[0]}) {
            ${writeDataToSubASnippet(transposeA)}
          }
        }
        // Load one tile of B into local memory.
        for (var inputRow = localRow; inputRow < ${tileInner}; inputRow = inputRow + ${workgroupSize[1]}) {
              for (var inputCol = localCol; inputCol < ${tileBOuter}; inputCol = inputCol + ${workgroupSize[0]}) {
            mm_Bsub[inputRow][inputCol] = mm_readB(batchB,
              kStart + inputRow,
              globalColStart + inputCol);
          }
        }
        kStart = kStart + ${tileInner};
        workgroupBarrier();

        // Compute acc values for a single thread.
        var BCached : array<f32, ${colPerThread}>;
        for (var k = 0; k < ${tileInner}; k++) {
          for (var inner = 0; inner < ${colPerThread}; inner++) {
            BCached[inner] = mm_Bsub[k][localCol + inner * ${workgroupSize[0]}];
          }
          for (var innerRow = 0; innerRow < ${rowPerThread}; innerRow++) {
            let ACached = ${transposeA ? `mm_Asub[k][localRow + innerRow * ${workgroupSize[1]}];` : `mm_Asub[localRow + innerRow * ${workgroupSize[1]}][k];`}
            for (var innerCol = 0; innerCol < ${colPerThread}; innerCol++) {
              acc[innerRow][innerCol] = acc[innerRow][innerCol] +
                  ACached * BCached[innerCol];
            }
          }
        }
        workgroupBarrier();
      }
      for (var innerRow = 0; innerRow < ${rowPerThread}; innerRow++) {
        let gRow = globalRowStart + localRow + innerRow * ${workgroupSize[1]};
        for (var innerCol = 0; innerCol < ${colPerThread}; innerCol++) {
          let gCol = globalColStart + localCol + innerCol * ${workgroupSize[0]};
          mm_write(batch, gRow, gCol, acc[innerRow][innerCol]);
        }
      }
      ` : `
  let tileRow = i32(localId.y) * ${rowPerThread};
  let tileCol = i32(localId.x) * ${colPerThread};

  let globalRow = i32(globalId.y) * ${rowPerThread};
  let globalCol = i32(globalId.x) * ${colPerThread};
  let globalRowStart = i32(workgroupId.y) * ${tileAOuter};

  let tileRowA = i32(localId.y) * ${rowPerThreadA};
  let tileColA = i32(localId.x) * ${colPerThreadA};
  let tileRowB = i32(localId.y) * ${rowPerThreadB};
  // Loop over shared dimension.
  for (var t = 0; t < numTiles; t++) {
    // Load one tile of A into local memory.
    for (var innerRow = 0; innerRow < ${rowPerThreadA}; innerRow++) {
      for (var innerCol = 0; innerCol < ${colPerThreadA}; innerCol++) {
        let inputRow = tileRowA + innerRow;
        let inputCol = tileColA + innerCol;
        ${writeDataToSubASnippet(transposeA)}
      }
    }

    // Load one tile of B into local memory.
    for (var innerRow = 0; innerRow < ${rowPerThreadB}; innerRow++) {
      for (var innerCol = 0; innerCol < ${colPerThread}; innerCol++) {
        let inputRow = tileRowB + innerRow;
        let inputCol = tileCol + innerCol;
        mm_Bsub[inputRow][inputCol] = mm_readB(batchB,
          kStart + inputRow,
          globalCol + innerCol);
      }
    }
    kStart = kStart + ${tileInner};
    workgroupBarrier();

    // Compute acc values for a single thread.
    var BCached : array<f32, ${colPerThread}>;
    for (var k = 0; k < ${tileInner}; k++) {
      for (var inner = 0; inner < ${colPerThread}; inner++) {
        BCached[inner] = mm_Bsub[k][tileCol + inner];
      }

      for (var innerRow = 0; innerRow < ${rowPerThread}; innerRow++) {
        ${readDataFromSubASnippet(transposeA)}
        for (var innerCol = 0; innerCol < ${colPerThread}; innerCol++) {
          acc[innerRow][innerCol] = acc[innerRow][innerCol] + ACached * BCached[innerCol];
        }
      }
    }

    workgroupBarrier();
  }

  for (var innerRow = 0; innerRow < ${rowPerThread}; innerRow++) {
    for (var innerCol = 0; innerCol < ${colPerThread}; innerCol++) {
      mm_write(batch, globalRow + innerRow, globalCol + innerCol,
          acc[innerRow][innerCol]);
    }
  }
  `;
  return `
    var<workgroup> mm_Asub : array<array<f32, ${tileAWidth}>, ${tileAHight}>;
    var<workgroup> mm_Bsub : array<array<f32, ${tileBOuter}>, ${tileInner}>;

    ${getMainHeaderString()} {
      let batch = ${splitK ? "0" : "i32(globalId.z)"};
      let batchA = ${splitK || !broadcastBatch ? "batch" : "batch % uniforms.aShape[0]"};
      let batchB = ${splitK || !broadcastBatch ? "batch" : "batch % uniforms.bShape[0]"};
      let numTiles = ${splitK ? `${Math.ceil(splitedDimInner / tileInner)}` : `(uniforms.dimInner - 1) / ${tileInner} + 1`};
      var kStart = ${splitK ? `i32(globalId.z) * ${splitedDimInner}` : "0"};

      var acc : array<array<f32, ${colPerThread}>, ${rowPerThread}>;

      // Without this initialization strange values show up in acc.
      for (var innerRow = 0; innerRow < ${rowPerThread}; innerRow++) {
        for (var innerCol = 0; innerCol < ${colPerThread}; innerCol++) {
          acc[innerRow][innerCol] = 0.0;
        }
      }
      ${matmulSnippet}
    }
  `;
}
var readVectorASnippet = (transpose2) => {
  return transpose2 ? `
      mm_readA(batchA, colA, globalRow),
      mm_readA(batchA, colA + 1, globalRow),
      mm_readA(batchA, colA + 2, globalRow),
      mm_readA(batchA, colA + 3, globalRow)
  ` : `
      mm_readA(batchA, globalRow, colA),
      mm_readA(batchA, globalRow, colA + 1),
      mm_readA(batchA, globalRow, colA + 2),
      mm_readA(batchA, globalRow, colA + 3)
  `;
};
function makeVectorMatrixProductSource(workgroupSize, transposeA = false) {
  util_exports.assert(workgroupSize[1] === 1 && workgroupSize[2] === 1, () => `A linear work group size is required. But got ${workgroupSize}.`);
  const tileSize = workgroupSize[0] * 4;
  return `
    var<workgroup> mm_Asub : array<vec4<f32>, ${workgroupSize[0]}>;

    ${getMainHeaderString()} {
      let tileCol = i32(localId.x);
      let globalCol = i32(globalId.x);
      let globalRow = i32(globalId.y);

      let numTiles = (uniforms.dimInner - 1) / ${tileSize} + 1;
      let batch = i32(globalId.z);
      let batchA = batch % uniforms.aShape[0];
      let batchB = batch % uniforms.bShape[0];
      // Without this initialization strange values show up in acc.
      var acc = 0.0;

      // Loop over shared dimension.
      for (var t = 0; t < numTiles; t++) {
        // Load one tile of A into local memory.
        let colA = t * ${tileSize} + tileCol * 4;
        mm_Asub[tileCol] = vec4<f32>(${readVectorASnippet(transposeA)});
        workgroupBarrier();

        // Compute acc values for a single thread.
        for (var k = 0; k < ${tileSize / 4}; k++) {
          let rowB = t * ${tileSize} + k * 4;
          let BCached = vec4<f32>(mm_readB(batchB, rowB, globalCol),
                              mm_readB(batchB, rowB + 1, globalCol),
                              mm_readB(batchB, rowB + 2, globalCol),
                              mm_readB(batchB, rowB + 3, globalCol));

          let ACached = mm_Asub[k];
          acc = acc + dot(ACached, BCached);
        }

        workgroupBarrier();
      }

      mm_write(batch, globalRow, globalCol, acc);
    }
  `;
}
var MatMulPackedProgram = class {
  constructor(aShape, outputShape, transposeA = false, transposeB = false, bias = null, activation = null, preluActivationWeights = null, sequentialAccessByThreads = false) {
    this.variableNames = ["A", "B"];
    this.uniforms = `dimAOuter : i32, dimBOuter : i32, dimInner : i32,`;
    this.outputShape = outputShape;
    this.dispatchLayout = { x: [2], y: [1], z: [0] };
    const dimInner = transposeA ? aShape[1] : aShape[2];
    this.isVec4 = (dimInner % 4 === 0 && !transposeA || outputShape[1] % 4 === 0 && transposeA) && outputShape[2] % 4 === 0 && !transposeB;
    this.isVectorA = outputShape[1] === 1 && !transposeA;
    if (!this.isVec4 && this.isVectorA) {
      this.elementsPerThread = [1, 1, 1];
      this.workgroupSize = [32, 1, 1];
    } else {
      const workgroupInfo = computeWorkgroupInfoForMatMul(outputShape[1], dimInner, outputShape[2], transposeA);
      this.workgroupSize = workgroupInfo.workgroupSize;
      this.elementsPerThread = workgroupInfo.elementsPerThread;
    }
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize, this.elementsPerThread);
    const addBias = bias != null;
    const hasPreluActivationWeights = preluActivationWeights != null;
    if (addBias) {
      this.variableNames.push("bias");
    }
    if (hasPreluActivationWeights) {
      this.variableNames.push("preluActivationWeights");
    }
    this.sequentialAccessByThreads = sequentialAccessByThreads;
    this.transposeA = transposeA;
    this.transposeB = transposeB;
    this.addBias = addBias;
    this.activation = activation;
    this.hasPreluActivationWeights = hasPreluActivationWeights;
    [this.fitAOuter, this.fitBOuter, this.fitInner] = this.getShapeFit(outputShape[1], outputShape[2], dimInner);
    this.shaderKey = `matMulPacked_${this.elementsPerThread}_${transposeA}_${transposeB}_${this.activation}_${this.fitAOuter}_${this.fitBOuter}_${this.fitInner}_${this.isVec4}_${this.isVectorA}_${this.sequentialAccessByThreads}`;
  }
  getShapeFit(dimAOuter, dimBOuter, dimInner) {
    const tileAOuter = this.workgroupSize[1] * this.elementsPerThread[1];
    const tileBOuter = this.workgroupSize[0] * this.elementsPerThread[0];
    if (!this.isVec4 && this.isVectorA) {
      this.tileInner = this.workgroupSize[0] * 4;
    } else {
      this.tileInner = tileBOuter;
    }
    const fitAOuter = dimAOuter % tileAOuter === 0;
    const fitBOuter = dimBOuter % tileBOuter === 0;
    const fitInner = dimInner % this.tileInner === 0;
    return [fitAOuter, fitBOuter, fitInner];
  }
  getUserCode() {
    const userCode = `
      ${activationFnSnippet(this.activation, this.hasPreluActivationWeights, this.isVec4)}
      ${matMulReadWriteFnSource(this.addBias, this.activation, false, this.transposeB, this.fitAOuter, this.fitBOuter, this.fitInner, this.isVec4 ? 4 : 1)}
      ${this.isVec4 ? makeMatMulPackedVec4Source(this.elementsPerThread, this.workgroupSize, this.transposeA, this.tileInner, false, null, this.isVectorA, true) : this.isVectorA ? makeVectorMatrixProductSource(this.workgroupSize, this.transposeA) : makeMatMulPackedSource(this.elementsPerThread, this.workgroupSize, this.transposeA, this.tileInner, false, null, this.sequentialAccessByThreads, true)}
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/matmul_reduce_webgpu.js
function makeMatMulReduceSource(workgroupSizeX) {
  return `
    var<workgroup> sumValues : array<f32, ${workgroupSizeX}>;
    ${getMainHeaderString()} {
      let coords = getOutputCoords();
      let batch = coords[0];
      let batchA = batch % uniforms.aShape[0];
      let batchB = batch % uniforms.bShape[0];
      let row = coords[1];
      let col = coords[2];
      var sum = 0.0;
      let Length = uniforms.dimInner;
      for (var k = i32(localId.x); k < Length; k = k + ${workgroupSizeX}) {
        let dataA = mm_readA(batchA, row, k);
        let dataB = mm_readB(batchB, k, col);
        sum = sum + dataA * dataB;
      }
      sumValues[localId.x] = sum;
      workgroupBarrier();

      for(var currentSize = ${workgroupSizeX / 2}u; currentSize > 1u;
          currentSize = currentSize / 2u) {
        if (localId.x < currentSize)
        {
          sumValues[localId.x] = sumValues[localId.x] + sumValues[localId.x + currentSize];
        }
        workgroupBarrier();
      }

      if (localId.x == 0u) {
        sum = sumValues[0] + sumValues[1];
        mm_write(batch, row, col, sum);
      }
    }
  `;
}
var MatMulReduceProgram = class {
  constructor(outputShape, transposeA = false, transposeB = false, bias = null, activation = null, preluActivationWeights = null) {
    this.variableNames = ["A", "B"];
    this.uniforms = `dimAOuter : i32, dimBOuter : i32, dimInner : i32,`;
    this.workgroupSize = [256, 1, 1];
    this.outputShape = outputShape;
    this.dispatchLayout = { x: [], y: [1, 2], z: [0] };
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    const addBias = bias != null;
    const hasPreluActivationWeights = preluActivationWeights != null;
    if (addBias) {
      this.variableNames.push("bias");
    }
    if (hasPreluActivationWeights) {
      this.variableNames.push("preluActivationWeights");
    }
    this.transposeA = transposeA;
    this.transposeB = transposeB;
    this.addBias = addBias;
    this.activation = activation;
    this.hasPreluActivationWeights = hasPreluActivationWeights;
    this.shaderKey = `matMulReduce_${this.activation}_${transposeA}_${transposeB}`;
  }
  getUserCode() {
    const userCode = `
      ${activationFnSnippet(this.activation, this.hasPreluActivationWeights)}
      ${matMulReadWriteFnSource(this.addBias, this.activation, this.transposeA, this.transposeB)}
      ${makeMatMulReduceSource(this.workgroupSize[0])}
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/matmul_small_output_size_webgpu.js
function makeMatMulSmallOutputSizeSource(workgroupSize) {
  const tileAOuter = workgroupSize[1];
  const tileBOuter = workgroupSize[0];
  const tileInner = tileAOuter > tileBOuter ? tileAOuter : tileBOuter;
  return `
  var<workgroup> mm_Asub : array<array<f32, ${tileInner}>, ${tileAOuter}>;
  var<workgroup> mm_Bsub : array<array<f32, ${tileBOuter}>, ${tileInner}>;

  // If the output size is small for matrix multiplication, avoid to use vec4
  // and handle some elements per thread to optimally utilize the ALU.
  // Read data from global memory to registers firstly, then store them into
  // shared memory, so it is instruction-Level parallelism for arithmetic
  // operations and others handle IO operations between barrier api, makes ALU
  // and load/store units work simultaneously, could improves the performance.
  ${getMainHeaderString()} {
    let tileRow = i32(localId.y);
    let tileCol = i32(localId.x);
    let globalRow = i32(globalId.y);
    let globalCol = i32(globalId.x);
    let batch = i32(globalId.z);
    let batchA = batch % uniforms.aShape[0];
    let batchB = batch % uniforms.bShape[0];

    // uniforms.dimInner should be greater than 0.
    let numTiles = (uniforms.dimInner - 1) / ${tileInner} + 1;
    var acc = 0.0;

    var globalColA = tileCol;
    var globalRowB = 0;
    var regA = mm_readA(batchA, globalRow, globalColA);
    var regB0 = mm_readB(batchB, globalRowB + 2 * tileRow, globalCol);
    var regB1 = mm_readB(batchB, globalRowB + 2 * tileRow + 1, globalCol);
    globalColA = globalColA + ${tileInner};
    globalRowB = globalRowB + ${tileInner};

    for (var t = 0; t < numTiles; t = t + 1) {
      mm_Asub[tileRow][tileCol] = regA;
      mm_Bsub[2 * tileRow][tileCol] = regB0;
      mm_Bsub[2 * tileRow + 1][tileCol] = regB1;

      workgroupBarrier();

      regA = mm_readA(batchA, globalRow, globalColA);
      regB0 = mm_readB(batchB, globalRowB + 2 * tileRow, globalCol);
      regB1 = mm_readB(batchB, globalRowB + 2 * tileRow + 1, globalCol);
      globalColA = globalColA + ${tileInner};
      globalRowB = globalRowB + ${tileInner};

      for (var k = 0; k < ${tileInner}; k = k + 1) {
        acc = acc + mm_Asub[tileRow][k] * mm_Bsub[k][tileCol];
      }
      workgroupBarrier();
    }

    mm_write(batch, globalRow, globalCol, acc);
  }
  `;
}
var MatMulSmallOutputSizeProgram = class {
  constructor(aShape, bShape, outputShape, transposeA = false, transposeB = false, bias = null, activation = null, preluActivationWeights = null) {
    this.variableNames = ["A", "B"];
    this.uniforms = `dimAOuter : i32, dimBOuter : i32, dimInner : i32,`;
    this.workgroupSize = [16, 8, 1];
    this.outputShape = outputShape;
    this.dispatchLayout = { x: [2], y: [1], z: [0] };
    this.dispatch = [
      Math.ceil(outputShape[2] / this.workgroupSize[0]),
      Math.ceil(outputShape[1] / this.workgroupSize[1]),
      outputShape[0]
    ];
    const addBias = bias != null;
    if (addBias) {
      this.variableNames.push("bias");
    }
    const hasPreluActivationWeights = preluActivationWeights != null;
    if (hasPreluActivationWeights) {
      this.variableNames.push("preluActivationWeights");
    }
    this.transposeA = transposeA;
    this.transposeB = transposeB;
    this.addBias = addBias;
    this.activation = activation;
    this.hasPreluActivationWeights = hasPreluActivationWeights;
    this.shaderKey = `matMulSmallOutputSize_${this.activation}_${transposeA}_${transposeB}`;
  }
  getUserCode() {
    const userCode = `
      ${activationFnSnippet(this.activation, this.hasPreluActivationWeights)}
      ${matMulReadWriteFnSource(this.addBias, this.activation, this.transposeA, this.transposeB)}
      ${makeMatMulSmallOutputSizeSource(this.workgroupSize)}
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/matmul_splitK_webgpu.js
var MatMulSplitKProgram = class {
  constructor(outputShape, dimInner, transposeA = false, transposeB = false) {
    this.variableNames = ["A", "B"];
    this.uniforms = `dimAOuter : i32, dimBOuter : i32, dimInner : i32,`;
    this.workgroupSize = [8, 8, 1];
    this.atomic = true;
    this.isVec4 = false;
    this.splitedDimInner = 128;
    util_exports.assert(outputShape[0] === 1, () => "MatMulSplitKProgram only supports batch = 1.");
    this.outputShape = outputShape;
    this.dispatchLayout = { x: [2], y: [1], z: [0, 3] };
    this.isVec4 = (transposeA && this.outputShape[1] % 4 === 0 || !transposeA && dimInner % 4 === 0) && this.outputShape[2] % 4 === 0;
    this.elementsPerThread = [4, 4, this.splitedDimInner];
    if (!this.isVec4) {
      if (this.outputShape[1] < 16) {
        this.elementsPerThread[1] = 1;
      }
      if (this.outputShape[2] < 16) {
        this.elementsPerThread[0] = 1;
      }
    }
    this.dispatch = computeDispatch(this.dispatchLayout, [
      this.outputShape[0],
      this.outputShape[1],
      this.outputShape[2],
      dimInner
    ], this.workgroupSize, this.elementsPerThread);
    this.transposeA = transposeA;
    this.transposeB = transposeB;
    this.shaderKey = `matMulSplitK_${transposeA}_${transposeB}_${this.elementsPerThread}_${this.isVec4}`;
  }
  getUserCode() {
    const component = this.isVec4 ? 4 : 1;
    const userCode = `
      ${matMulReadFnSource(false, this.transposeB, false, false, false, component)}
      fn mm_write(batch: i32, row : i32, colIn : i32, value : ${typeSnippet(component)}) {
        let col = colIn * ${component};
        if (row < uniforms.dimAOuter && col < uniforms.dimBOuter) {
          let coords = vec3<i32>(batch, row, col);
          let flatIndex = getOutputIndexFromCoords(coords);
          // The problem is that we should initialize output to zero before using.
          // Otherwise, the original value will be added to the result.
          for (var i = 0; i < ${component}; i = i + 1) {
            ${atomicAddSnippet("&result[flatIndex + i]", `${component > 1 ? "value[i]" : "value"}`, "float32")}
          }
        }
      }
      ${this.isVec4 ? makeMatMulPackedVec4Source(this.elementsPerThread, this.workgroupSize, this.transposeA, 32, true, this.splitedDimInner) : makeMatMulPackedSource(this.elementsPerThread, this.workgroupSize, this.transposeA, 32, true, this.splitedDimInner)}
    `;
    return userCode;
  }
};
var BiasActivationProgram = class {
  constructor(outputShape, bias = null, activation = null, preluActivationWeights = null) {
    this.uniforms = "";
    this.variableNames = ["x"];
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = outputShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.addBias = bias != null;
    this.hasPreluActivationWeights = preluActivationWeights != null;
    this.activation = activation;
    if (this.addBias) {
      this.variableNames.push("bias");
    }
    if (this.hasPreluActivationWeights) {
      this.variableNames.push("preluActivationWeights");
    }
    this.shaderKey = `biasActivation_${activation}`;
  }
  getUserCode() {
    return `
    ${activationFnSnippet(this.activation, this.hasPreluActivationWeights)}
    ${getMainHeaderString("index")} {
      if (index < uniforms.size) {
        let coords = getCoordsFromIndex(index);
        var value = getXByOutputIndex(index);
        ${biasActivationSnippet(this.addBias, this.activation)}
        setOutputAtIndex(index, value);
      }
    }
    `;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/fill_webgpu.js
var FillProgram = class {
  constructor(shape) {
    this.variableNames = [];
    this.outputShape = [];
    this.uniforms = "value : f32,";
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = shape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.shaderKey = "fill";
  }
  getUserCode() {
    const userCode = `
    ${getMainHeaderString("index")} {
      if (index < uniforms.size) {
        setOutputAtIndex(index, uniforms.value);
      }
    }
  `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Fill.js
function fill(args) {
  const { backend, attrs } = args;
  const { shape, value } = attrs;
  let { dtype } = attrs;
  dtype = dtype || util_exports.inferDtype(value);
  if (dtype === "string") {
    const values = util_exports.getArrayFromDType(dtype, util_exports.sizeFromShape(shape));
    values.fill(value);
    return backend.makeTensorInfo(shape, dtype, values);
  } else {
    const program = new FillProgram(shape);
    const uniformData = [{ type: "float32", data: [value] }];
    return backend.runWebGPUProgram(program, [], dtype, uniformData);
  }
}
var fillConfig = {
  kernelName: Fill,
  backendName: "webgpu",
  kernelFunc: fill
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Reshape.js
function reshape(args) {
  const { inputs, attrs } = args;
  const { x } = inputs;
  const { shape } = attrs;
  const xSize = util_exports.sizeFromShape(x.shape);
  const $shape = util_exports.inferFromImplicitShape(shape, xSize);
  const $xSize = util_exports.sizeFromShape($shape);
  util_exports.assert(xSize === $xSize, () => `The new shape (${$shape}) has ${$xSize} elements and the old shape (${x.shape}) has ${xSize} elements. The new shape and old shape must have the same number of elements.`);
  args.backend.incRef(x.dataId);
  return { dataId: x.dataId, shape: $shape, dtype: x.dtype };
}
var reshapeConfig = {
  kernelName: Reshape,
  backendName: "webgpu",
  kernelFunc: reshape
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/BatchMatMul_impl.js
function batchMatMulImpl({ a, b, transposeA, transposeB, backend, bias = null, preluActivationWeights = null, leakyreluAlpha = 0, activation = null }) {
  const aRank = a.shape.length;
  const bRank = b.shape.length;
  const innerShapeA = transposeA ? a.shape[aRank - 2] : a.shape[aRank - 1];
  const innerShapeB = transposeB ? b.shape[bRank - 1] : b.shape[bRank - 2];
  const outerShapeA = transposeA ? a.shape[aRank - 1] : a.shape[aRank - 2];
  const outerShapeB = transposeB ? b.shape[bRank - 2] : b.shape[bRank - 1];
  const outerDimsA = a.shape.slice(0, -2);
  const outerDimsB = b.shape.slice(0, -2);
  const batchDimA = util_exports.sizeFromShape(outerDimsA);
  const batchDimB = util_exports.sizeFromShape(outerDimsB);
  const outShapeOuterDims = broadcast_util_exports.assertAndGetBroadcastShape(a.shape.slice(0, -2), b.shape.slice(0, -2));
  const outShape = outShapeOuterDims.concat([outerShapeA, outerShapeB]);
  util_exports.assert(innerShapeA === innerShapeB, () => `Error in matMul: inner shapes (${innerShapeA}) and (${innerShapeB}) of Tensors with shapes ${a.shape} and ${b.shape} and transposeA=${transposeA} and transposeB=${transposeB} must match.`);
  const a3dShape = transposeA ? [batchDimA, innerShapeA, outerShapeA] : [batchDimA, outerShapeA, innerShapeA];
  const b3dShape = transposeB ? [batchDimB, outerShapeB, innerShapeB] : [batchDimB, innerShapeB, outerShapeB];
  const a3d = reshape({ inputs: { x: a }, backend, attrs: { shape: a3dShape } });
  const b3d = reshape({ inputs: { x: b }, backend, attrs: { shape: b3dShape } });
  const intermediates = [a3d, b3d];
  const batchDim = Math.max(batchDimA, batchDimB);
  const inputs = [a3d, b3d];
  const dimensions = [
    { type: "int32", data: [outerShapeA] },
    { type: "int32", data: [outerShapeB] },
    { type: "int32", data: [innerShapeA] }
  ];
  let program;
  let out;
  const outputShape = [batchDim, outerShapeA, outerShapeB];
  let matmulProgramType = env().get("WEBGPU_MATMUL_PROGRAM_TYPE");
  if (matmulProgramType < 0) {
    const thresholdFlagValue = env().getNumber("WEBGPU_THRESHOLD_TO_INCREASE_WORKGROUPS_FOR_MATMUL");
    const thresholdToIncreaseWorkgroups = thresholdFlagValue > 0 ? thresholdFlagValue : backend.thresholdToIncreaseWorkgroups;
    const workgroupsBy32x32 = batchDim * Math.ceil(outerShapeA / 32) * Math.ceil(outerShapeB / 32);
    const hasFewWorkgroups = workgroupsBy32x32 <= thresholdToIncreaseWorkgroups || outerShapeA <= 8 && workgroupsBy32x32 <= thresholdToIncreaseWorkgroups * 2;
    if (hasFewWorkgroups) {
      if (batchDim * outerShapeA * outerShapeB <= 128) {
        matmulProgramType = MatMulProgramType.MatMulReduceProgram;
      } else if (batchDim === 1 && innerShapeB >= 2e3) {
        matmulProgramType = MatMulProgramType.MatMulSplitKProgram;
      } else {
        matmulProgramType = MatMulProgramType.MatMulSmallOutputSizeProgram;
      }
    } else {
      matmulProgramType = MatMulProgramType.MatMulPackedProgram;
    }
  }
  switch (matmulProgramType) {
    case MatMulProgramType.MatMulReduceProgram:
      program = new MatMulReduceProgram(outputShape, transposeA, transposeB, bias, activation, preluActivationWeights);
      break;
    case MatMulProgramType.MatMulSplitKProgram: {
      out = fill({ backend, attrs: { shape: outputShape, value: 0, dtype: a.dtype } });
      program = new MatMulSplitKProgram(outputShape, innerShapeB, transposeA, transposeB);
      if (bias || activation) {
        out = backend.runWebGPUProgram(program, inputs, a.dtype, dimensions, out);
        const biasActivationProgram = new BiasActivationProgram(out.shape, bias, activation, preluActivationWeights);
        let uniformData = null;
        const activationInputs = [out];
        if (bias) {
          activationInputs.push(bias);
        }
        if (preluActivationWeights) {
          activationInputs.push(preluActivationWeights);
        }
        if (activation === "leakyrelu") {
          uniformData = [{ type: "float32", data: [leakyreluAlpha] }];
          biasActivationProgram.uniforms += " alpha : f32,";
        }
        const outActivated = backend.runWebGPUProgram(biasActivationProgram, activationInputs, out.dtype, uniformData);
        intermediates.push(out);
        const outReshaped2 = reshape({ inputs: { x: outActivated }, backend, attrs: { shape: outShape } });
        intermediates.push(outActivated);
        for (const i of intermediates) {
          backend.disposeData(i.dataId);
        }
        return outReshaped2;
      }
      break;
    }
    case MatMulProgramType.MatMulSmallOutputSizeProgram:
      program = new MatMulSmallOutputSizeProgram(a3dShape, b3dShape, outputShape, transposeA, transposeB, bias, activation, preluActivationWeights);
      break;
    case MatMulProgramType.MatMulPackedProgram:
      const sequentialAccessByThreads = backend.adapterInfo.isIntel();
      program = new MatMulPackedProgram(a3dShape, outputShape, transposeA, transposeB, bias, activation, preluActivationWeights, sequentialAccessByThreads);
      break;
    default:
      throw new Error(`Unsupported MatMulProgramType ${matmulProgramType}.`);
  }
  if (bias) {
    inputs.push(bias);
  }
  if (preluActivationWeights) {
    inputs.push(preluActivationWeights);
  }
  if (activation === "leakyrelu") {
    dimensions.push({ type: "float32", data: [leakyreluAlpha] });
    program.uniforms += " alpha : f32,";
  }
  out = backend.runWebGPUProgram(program, inputs, a.dtype, dimensions, out);
  const outReshaped = reshape({ inputs: { x: out }, backend, attrs: { shape: outShape } });
  intermediates.push(out);
  for (const i of intermediates) {
    backend.disposeData(i.dataId);
  }
  return outReshaped;
}

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/_FusedMatMul.js
function _fusedMatMul(args) {
  const { inputs, backend, attrs } = args;
  const { a, b, bias, preluActivationWeights } = inputs;
  const { transposeA, transposeB, activation, leakyreluAlpha } = attrs;
  return batchMatMulImpl({
    a,
    b,
    transposeA,
    transposeB,
    backend,
    bias,
    preluActivationWeights,
    leakyreluAlpha,
    activation
  });
}
var _fusedMatMulConfig = {
  kernelName: _FusedMatMul,
  backendName: "webgpu",
  kernelFunc: _fusedMatMul
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/binary_op_complex_webgpu.js
var BinaryOpComplexProgram = class {
  constructor(op, aShape, bShape) {
    this.variableNames = ["AReal", "AImag", "BReal", "BImag"];
    this.workgroupSize = [128, 1, 1];
    this.size = true;
    this.outputShape = backend_util_exports.assertAndGetBroadcastShape(aShape, bShape);
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.shaderKey = `binaryOpComplex_${op}`;
    this.op = op;
  }
  getUserCode() {
    const opStr = getBinaryOpString(this.op, false);
    const userCode = `
      fn binaryOpComplex(
          areal : f32, aimag : f32, breal : f32, bimag : f32) -> f32 {
        ${opStr}
      }

      ${getMainHeaderString("index")} {
        if(index < uniforms.size) {
          let areal = getARealByOutputIndex(index);
          let aimag = getAImagByOutputIndex(index);
          let breal = getBRealByOutputIndex(index);
          let bimag = getBImagByOutputIndex(index);
          setOutputAtIndex(index, binaryOpComplex(areal, aimag, breal, bimag));
        }
      }
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/binary_op_webgpu.js
var BinaryOpProgram = class {
  constructor(op, aShape, bShape) {
    this.size = true;
    this.variableNames = ["A", "B"];
    this.outputShape = backend_util_exports.assertAndGetBroadcastShape(aShape, bShape);
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.op = op;
    this.useSharedMemoryWithA = aShape.length <= 1 && bShape.length > 1 && aShape[0] < 128;
    this.useSharedMemoryWithB = bShape.length <= 1 && aShape.length > 1 && bShape[0] < 128;
    if (this.useSharedMemoryWithA || this.useSharedMemoryWithB) {
      this.isVec4 = false;
      this.lastDimensionSize = this.useSharedMemoryWithB ? bShape[0] : aShape[0];
      this.shaderKey = `binary_${this.type}_${op}_${this.lastDimensionSize}_${this.useSharedMemoryWithB}`;
      this.type = "shared";
      this.workgroupSize = [256, 1, 1];
      this.workPerThread = 1;
    } else {
      if (util_exports.arraysEqual(aShape, bShape) && util_exports.sizeFromShape(aShape) % 4 === 0) {
        this.isVec4 = true;
        this.type = "vec4";
        this.workPerThread = 4;
      } else {
        this.isVec4 = false;
        this.type = "plain";
        this.workPerThread = 1;
      }
      this.shaderKey = `binary_${this.type}_${op}`;
      this.workgroupSize = [128, 1, 1];
    }
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize, [this.workPerThread, 1, 1]);
  }
  getUserCode() {
    let userCode;
    const dType = this.isVec4 ? "vec4<f32>" : "f32";
    const opFnStr = `
    fn binaryOperation(a : ${dType}, b : ${dType}) -> ${dType} {
      let isNaN = false;
      {
        ${getBinaryOpString(this.op, this.isVec4)}
      }
    };
    `;
    if (this.type === "shared") {
      const sharedIndexSnippet = this.lastDimensionSize > 1 ? `coords[${this.outputShape.length - 1}]` : "0";
      const accessDataSnippet = this.useSharedMemoryWithB ? `let a = getAByOutputIndex(index);
          let b = sharedBuf[${sharedIndexSnippet}];` : `let a = sharedBuf[${sharedIndexSnippet}];
          let b = getBByOutputIndex(index);`;
      userCode = `
        ${opFnStr}
        var<workgroup> sharedBuf : array<f32, ${this.lastDimensionSize}>;
        ${getMainHeaderString("index")} {
          // Fill in the shared memory buffer.
          let localIndex = i32(localId.x);
          if(localIndex < ${this.lastDimensionSize}) {
            sharedBuf[localIndex] = f32(${this.useSharedMemoryWithB ? "B" : "A"}[localIndex]);
          }
          workgroupBarrier();

          if(index < uniforms.size) {
            let coords = getCoordsFromIndex(index);
            ${accessDataSnippet}
            setOutputAtIndex(index, binaryOperation(a, b));
          }
        }
        `;
    } else {
      userCode = `
       ${opFnStr}
       ${getMainHeaderString("index")} {
         if (index < uniforms.size) {
           let a = getAByOutputIndex(index);
           let b = getBByOutputIndex(index);
           setOutputAtIndex(index, binaryOperation(a, b));
         }
       }
       `;
    }
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Identity.js
function identity(args) {
  const { inputs } = args;
  const { x } = inputs;
  args.backend.incRef(x.dataId);
  return { dataId: x.dataId, shape: x.shape, dtype: x.dtype };
}
var identityConfig = {
  kernelName: Identity,
  backendName: "webgpu",
  kernelFunc: identity
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Complex.js
function complex(args) {
  const { inputs, backend } = args;
  const { real: real2, imag: imag2 } = inputs;
  const complexInfo = backend.makeTensorInfo(real2.shape, "complex64");
  const complex2 = backend.tensorMap.get(complexInfo.dataId);
  const realTensorInfo = identity({ inputs: { x: real2 }, backend });
  const imagTensorInfo = identity({ inputs: { x: imag2 }, backend });
  complex2.complexTensorInfos = { real: realTensorInfo, imag: imagTensorInfo };
  return complexInfo;
}
var complexConfig = {
  kernelName: Complex,
  backendName: "webgpu",
  kernelFunc: complex
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/unary_op_webgpu.js
var UnaryOpProgram = class {
  constructor(outputShape, op, uniforms = "") {
    this.variableNames = ["A"];
    this.size = true;
    const workgroupSizeX = 128;
    this.workgroupSize = [workgroupSizeX, 1, 1];
    this.outputShape = outputShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.op = op;
    if (uniforms !== "") {
      this.uniforms = uniforms;
    }
    this.shaderKey = `unary_${op}`;
  }
  getUserCode() {
    return `
      fn unaryOperation(a : f32) -> f32 {
        ${getUnaryOpString(this.op, false)}
      }
      ${getMainHeaderString("index")} {
        if (index < uniforms.size) {
          let a = getAByOutputIndex(index);
          setOutputAtIndex(index, unaryOperation(a));
        }
      }
      `;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernel_utils/kernel_funcs_utils.js
function unaryKernelFunc({ opType, cpuKernelImpl, dtype }) {
  return ({ inputs, backend }) => {
    const { x } = inputs;
    const webgpuBackend = backend;
    const $dtype = dtype || x.dtype;
    if (webgpuBackend.shouldExecuteOnCPU([x]) && cpuKernelImpl != null) {
      const xData = webgpuBackend.tensorMap.get(x.dataId);
      const outValues = cpuKernelImpl(xData.values, $dtype);
      return webgpuBackend.makeTensorInfo(x.shape, $dtype, outValues);
    }
    const program = new UnaryOpProgram(x.shape, opType);
    return webgpuBackend.runWebGPUProgram(program, [x], $dtype);
  };
}
function binaryKernelFunc({ opType, cpuKernelImpl, supportsComplex = false, dtype }) {
  return ({ inputs, backend }) => {
    const { a, b } = inputs;
    const webgpuBackend = backend;
    if (supportsComplex && a.dtype === "complex64") {
      const aData = webgpuBackend.tensorMap.get(a.dataId);
      const bData = webgpuBackend.tensorMap.get(b.dataId);
      let real2, imag2;
      if (opType !== BinaryOpType.MUL) {
        [real2, imag2] = [
          [aData.complexTensorInfos.real, bData.complexTensorInfos.real],
          [aData.complexTensorInfos.imag, bData.complexTensorInfos.imag]
        ].map((complexParts) => {
          const [aPart, bPart] = complexParts;
          const aHandle = {
            dataId: aPart.dataId,
            dtype: aPart.dtype,
            shape: a.shape
          };
          const bHandle = {
            dataId: bPart.dataId,
            dtype: bPart.dtype,
            shape: b.shape
          };
          const program2 = new BinaryOpProgram(opType, a.shape, b.shape);
          return webgpuBackend.runWebGPUProgram(program2, [aHandle, bHandle], upcastType(aPart.dtype, bPart.dtype));
        });
      } else {
        const realProgram = new BinaryOpComplexProgram(BinaryOpType.COMPLEX_MULTIPLY_REAL, a.shape, b.shape);
        const imagProgram = new BinaryOpComplexProgram(BinaryOpType.COMPLEX_MULTIPLY_IMAG, a.shape, b.shape);
        const inputs2 = [
          {
            dataId: aData.complexTensorInfos.real.dataId,
            dtype: aData.complexTensorInfos.real.dtype,
            shape: a.shape
          },
          {
            dataId: aData.complexTensorInfos.imag.dataId,
            dtype: aData.complexTensorInfos.imag.dtype,
            shape: a.shape
          },
          {
            dataId: bData.complexTensorInfos.real.dataId,
            dtype: bData.complexTensorInfos.real.dtype,
            shape: b.shape
          },
          {
            dataId: bData.complexTensorInfos.imag.dataId,
            dtype: bData.complexTensorInfos.imag.dtype,
            shape: b.shape
          }
        ];
        real2 = webgpuBackend.runWebGPUProgram(realProgram, inputs2, "float32");
        imag2 = webgpuBackend.runWebGPUProgram(imagProgram, inputs2, "float32");
      }
      const complexOutput = complex({ inputs: { real: real2, imag: imag2 }, backend: webgpuBackend });
      webgpuBackend.disposeData(real2.dataId);
      webgpuBackend.disposeData(imag2.dataId);
      return complexOutput;
    }
    const $dtype = dtype || upcastType(a.dtype, b.dtype);
    if ((a.dtype === "string" || b.dtype === "string" || webgpuBackend.shouldExecuteOnCPU([a, b])) && cpuKernelImpl != null) {
      const aData = webgpuBackend.tensorMap.get(a.dataId).values;
      const bData = webgpuBackend.tensorMap.get(b.dataId).values;
      const decodedAVals = a.dtype === "string" ? (
        // tslint:disable-next-line: no-any
        backend_util_exports.fromUint8ToStringArray(aData)
      ) : aData;
      const decodedBVals = a.dtype === "string" ? (
        // tslint:disable-next-line: no-any
        backend_util_exports.fromUint8ToStringArray(bData)
      ) : bData;
      const [outValues, outShape] = cpuKernelImpl(a.shape, b.shape, decodedAVals, decodedBVals, $dtype);
      return webgpuBackend.makeTensorInfo(outShape, $dtype, outValues);
    }
    const program = new BinaryOpProgram(opType, a.shape, b.shape);
    return webgpuBackend.runWebGPUProgram(program, [a, b], $dtype);
  };
}

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernel_utils/shared.js
var { addImpl: addImplCPU, castImpl: castImplCPU, ceilImpl: ceilImplCPU, concatImpl: concatImplCPU, equalImpl: equalImplCPU, expImpl: expImplCPU, expm1Impl: expm1ImplCPU, floorImpl: floorImplCPU, gatherNdImpl: gatherNdImplCPU, gatherV2Impl: gatherV2ImplCPU, greaterEqualImpl: greaterEqualImplCPU, greaterImpl: greaterImplCPU, lessEqualImpl: lessEqualImplCPU, lessImpl: lessImplCPU, logImpl: logImplCPU, maxImpl: maxImplCPU, maximumImpl: maximumImplCPU, minimumImpl: minimumImplCPU, multiplyImpl: multiplyImplCPU, negImpl: negImplCPU, notEqualImpl: notEqualImplCPU, prodImpl: prodImplCPU, rangeImpl: rangeImplCPU, rsqrtImpl: rsqrtImplCPU, scatterImpl: scatterImplCPU, simpleAbsImpl: simpleAbsImplCPU, sliceImpl: sliceImplCPU, stridedSliceImpl: stridedSliceImplCPU, stringNGramsImpl: stringNGramsImplCPU, subImpl: subImplCPU, tileImpl: tileImplCPU, topKImpl: topKImplCPU, transposeImpl: transposeImplCPU, uniqueImpl: uniqueImplCPU } = shared_exports;

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Abs.js
var abs = unaryKernelFunc({ opType: UnaryOpType.ABS, cpuKernelImpl: simpleAbsImplCPU });
var absConfig = {
  kernelName: Abs,
  backendName: "webgpu",
  kernelFunc: abs
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Acos.js
var acos = unaryKernelFunc({ opType: UnaryOpType.ACOS });
var acosConfig = {
  kernelName: Acos,
  backendName: "webgpu",
  kernelFunc: acos
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Acosh.js
var acosh = unaryKernelFunc({ opType: UnaryOpType.ACOSH });
var acoshConfig = {
  kernelName: Acosh,
  backendName: "webgpu",
  kernelFunc: acosh
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Add.js
var addKernelFunc = binaryKernelFunc({ opType: BinaryOpType.ADD, cpuKernelImpl: addImplCPU, supportsComplex: true });
var addConfig = {
  kernelName: Add,
  backendName: "webgpu",
  kernelFunc: addKernelFunc
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/addn_packed_webgpu.js
var AddNPackedProgram = class {
  constructor(shapes) {
    this.workPerThread = 1;
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = shapes[0];
    this.variableNames = shapes.map((_, i) => `T${i}`);
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize, [this.workPerThread, 1, 1]);
    this.shaderKey = "addN";
  }
  getUserCode() {
    const snippets = [];
    this.variableNames.forEach((variable) => {
      snippets.push(`let v${variable} = get${variable}ByOutputCoords(coords);`);
    });
    const operation = this.variableNames.map((variable) => {
      return `v${variable}`;
    }).join(" + ");
    const userCode = `
      ${getMainHeaderString("index")} {
        for (var i = 0; i < ${this.workPerThread}; i = i + 1) {
          let flatIndex = index * ${this.workPerThread} + i;
          if (flatIndex < uniforms.size) {
            let coords = getCoordsFromIndex(flatIndex);
            ${snippets.join("\n        ")}
            setOutputAtIndex(flatIndex, ${operation});
          }
        }
      }
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/AddN.js
function addN(args) {
  const { inputs, backend } = args;
  const tensors = inputs;
  if (tensors.length === 1) {
    return identity({ inputs: { x: tensors[0] }, backend });
  }
  const dtype = tensors.map((t) => t.dtype).reduce((d1, d2) => upcastType(d1, d2));
  const shapes = tensors.map((t) => t.shape);
  const program = new AddNPackedProgram(shapes);
  return backend.runWebGPUProgram(program, tensors, dtype);
}
var addNConfig = {
  kernelName: AddN,
  backendName: "webgpu",
  kernelFunc: addN
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/transpose_shared_webgpu.js
var TransposeSharedProgram = class {
  constructor(aShape, newDim) {
    this.variableNames = ["A"];
    this.workgroupSize = [16, 16, 1];
    const outputShape = new Array(aShape.length);
    for (let i = 0; i < outputShape.length; i++) {
      outputShape[i] = aShape[newDim[i]];
    }
    this.outputShape = outputShape;
    this.dispatchLayout = { x: [0], y: [1] };
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize, [1, 1, 1]);
    this.shaderKey = "transposeShared";
  }
  getUserCode() {
    util_exports.assert(this.workgroupSize[0] === this.workgroupSize[1], () => `Must be a square tile, current tile shape is ${this.workgroupSize[0]} x ${this.workgroupSize[1]}`);
    const tileSize = this.workgroupSize[0];
    const userCode = `
      var<workgroup> tile : array<array<f32, ${this.workgroupSize[0] + 1}>, ${this.workgroupSize[0]}>;
      ${getMainHeaderString()} {
        var x = i32(workgroupId.x) * ${tileSize} + i32(localId.x);
        var y = i32(workgroupId.y) * ${tileSize} + i32(localId.y);
        let width = uniforms.outShape[0];
        let height = uniforms.outShape[1];
        if (x < width && y < height) {
          tile[localId.y][localId.x] = f32(A[y * width + x]);
        }
        workgroupBarrier();

        x = i32(workgroupId.y) * ${tileSize} + i32(localId.x);
        y = i32(workgroupId.x) * ${tileSize} + i32(localId.y);
        if (x < height && y < width) {
          setOutputAtIndex((y * height + x), tile[localId.x]
            [localId.y]);
        }
      }
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/transpose_webgpu.js
var TransposeProgram = class {
  constructor(aShape, newDim) {
    this.variableNames = ["A"];
    this.workPerThread = 1;
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    const outputShape = new Array(aShape.length);
    for (let i = 0; i < outputShape.length; i++) {
      outputShape[i] = aShape[newDim[i]];
    }
    this.outputShape = outputShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize, [this.workPerThread, 1, 1]);
    this.newDim = newDim;
    this.shaderKey = `transpose_${newDim}`;
  }
  getUserCode() {
    const dtype = getCoordsDataType(this.outputShape.length);
    const switched = getSwitchedCoords(this.newDim);
    const userCode = `
      ${getMainHeaderString("index")} {
        for(var i = 0; i < ${this.workPerThread}; i = i + 1) {
          let flatIndex = index * ${this.workPerThread} + i;
          if(flatIndex < uniforms.size) {
            let resRC = getCoordsFromIndex(flatIndex);
            setOutputAtIndex(flatIndex, A[getIndexFromCoords${this.outputShape.length}D(
              ${dtype}(${switched}), uniforms.aShape)]);
          }
        }
      }
    `;
    return userCode;
  }
};
function getSwitchedCoords(newDim) {
  const rank = newDim.length;
  if (rank > 6) {
    throw Error(`Transpose for rank ${rank} is not yet supported`);
  }
  const switchedCoords = new Array(rank);
  for (let i = 0; i < newDim.length; i++) {
    switchedCoords[newDim[i]] = `resRC.${getCoordsXYZ(i)}`;
  }
  return switchedCoords.join();
}

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Transpose.js
function transpose(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { perm } = attrs;
  const webgpuBackend = backend;
  const xRank = x.shape.length;
  const newShape = new Array(xRank);
  for (let i = 0; i < newShape.length; i++) {
    newShape[i] = x.shape[perm[i]];
  }
  if (backend.shouldExecuteOnCPU([x])) {
    const xData = webgpuBackend.tensorMap.get(x.dataId);
    const values = xData.values;
    const outValues = transposeImplCPU(values, x.shape, x.dtype, perm, newShape);
    return backend.makeTensorInfo(newShape, x.dtype, outValues);
  }
  if (x.shape.length === 2 && util_exports.arraysEqual(perm, [1, 0])) {
    const program2 = new TransposeSharedProgram(x.shape, perm);
    return webgpuBackend.runWebGPUProgram(program2, [x], x.dtype);
  }
  const program = new TransposeProgram(x.shape, perm);
  return webgpuBackend.runWebGPUProgram(program, [x], x.dtype);
}
var transposeConfig = {
  kernelName: Transpose,
  backendName: "webgpu",
  kernelFunc: transpose
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/reduce_webgpu.js
var ReduceProgram = class {
  constructor(reduceInfo, reduceType) {
    this.workgroupSize = [64, 1, 1];
    this.variableNames = ["x"];
    this.uniforms = "reduceSize : i32,";
    this.size = true;
    this.inputShape = [reduceInfo.batchSize, reduceInfo.inSize];
    const [outputShape] = backend_util_exports.computeOutAndReduceShapes(this.inputShape, [1]);
    this.outputShape = outputShape.length === 0 ? [1] : outputShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, [1, 1, 1]);
    this.reduceType = reduceType;
    this.shaderKey = `reduce_${reduceType}`;
  }
  getUserCode() {
    let reduceOp = ``;
    let initValue = "0.0";
    const workgroupSizeX = this.workgroupSize[0];
    if (this.reduceType === "min" || this.reduceType === "max") {
      reduceOp = `
         if (isnan(candidate)) {
          bestValue = uniforms.NAN;
         } else if (!isnan(bestValue) && candidate ${this.reduceType === "min" ? "<" : ">"} bestValue)
           {  bestValue = candidate; }`;
      initValue = "f32(x[offset])";
    } else if (this.reduceType === "sum" || this.reduceType === "mean") {
      reduceOp = " bestValue = bestValue + candidate; ";
    } else if (this.reduceType === "prod") {
      reduceOp = " bestValue = bestValue * candidate; ";
      initValue = "1.0";
    } else if (this.reduceType === "all") {
      reduceOp = " bestValue = f32(bestValue >= 1.0 && candidate >= 1.0); ";
      initValue = "1.0";
    } else if (this.reduceType === "any") {
      reduceOp = " bestValue = f32(bestValue >= 1.0 || candidate >= 1.0); ";
      initValue = "0.0";
    }
    const outputSnippet = this.reduceType === "mean" ? (
      // tslint:disable-next-line:max-line-length
      `setOutputAtIndex(outputIndex, bestValue / f32(uniforms.reduceSize));`
    ) : `setOutputAtIndex(outputIndex, bestValue);`;
    const sharedMemorySnippet = `
         var<workgroup> xBestValues : array<f32, ${workgroupSizeX}>;
       `;
    const userCode = `
       fn DIV_CEIL(a : u32, b : u32) -> u32 {
        return ((a - 1u) / b + 1u);
       }

       ${sharedMemorySnippet}
       fn getOffset(outputIndex : i32) -> i32 {
         let outputCoords = getCoordsFromIndex(outputIndex);
         let offset = ${this.outputShape.length === 1 ? "outputCoords" : "outputCoords[0]"} * uniforms.reduceSize;
          return offset;
       }
       ${getMainHeaderString("index")} {
         let outputIndex = index / ${workgroupSizeX};
         let offset = getOffset(outputIndex);
         var bestValue = ${initValue};
         let Length = uniforms.reduceSize;
         let WorkPerThread = DIV_CEIL(u32(Length), ${workgroupSizeX}u);
         for (var k = i32(localId.x); k < Length && outputIndex < uniforms.size;
             k = k + ${workgroupSizeX}) {
           let candidate = f32(x[offset + k]);
           ${reduceOp}
         }
         xBestValues[localId.x] = bestValue;
         workgroupBarrier();

         var reduceSize = min(u32(Length), ${workgroupSizeX}u);
         for (var currentSize = reduceSize / 2u; reduceSize > 1u;
             currentSize = reduceSize / 2u) {
           let interval = DIV_CEIL(reduceSize, 2u);
           if (localId.x < currentSize) {
            let candidate = xBestValues[localId.x + interval];
            ${reduceOp}
            xBestValues[localId.x] = bestValue;
           }
           reduceSize = interval;
           workgroupBarrier();
         }

         if (localId.x == 0u && outputIndex < uniforms.size) {
          ${outputSnippet}
        }
       }
     `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernel_utils/reduce.js
function reduce(x, axis, keepDims, reduceType, backend) {
  const xRank = x.shape.length;
  const toDispose = [];
  const origAxes = util_exports.parseAxisParam(axis, x.shape);
  let axes = origAxes;
  const permutedAxes = backend_util_exports.getAxesPermutation(axes, xRank);
  let input = x;
  if (permutedAxes != null) {
    input = transpose({ inputs: { x }, attrs: { perm: permutedAxes }, backend });
    axes = backend_util_exports.getInnerMostAxes(axes.length, xRank);
    toDispose.push(input);
  }
  backend_util_exports.assertAxesAreInnerMostDims(reduceType, axes, xRank);
  const [reduceOutShape, reduceShape] = backend_util_exports.computeOutAndReduceShapes(input.shape, axes);
  let resOutShape = reduceOutShape;
  if (keepDims) {
    resOutShape = backend_util_exports.expandShapeToKeepDim(reduceOutShape, origAxes);
  }
  let res;
  if ((reduceType === "max" || reduceType === "prod") && backend.shouldExecuteOnCPU([input])) {
    const xVals = backend.tensorMap.get(input.dataId).values;
    switch (reduceType) {
      case "max":
        const outValues = maxImplCPU(xVals, util_exports.sizeFromShape(reduceShape), resOutShape, x.dtype);
        res = backend.makeTensorInfo(resOutShape, x.dtype, outValues);
        break;
      case "prod":
        const { outVals, outShape, outDtype } = prodImplCPU(input.shape, input.dtype, xVals, axes);
        res = backend.makeTensorInfo(outShape, outDtype, outVals);
        break;
      default:
        throw new Error(`${reduceType} CPU implementation is not yet supported.`);
    }
  } else {
    const inSize = util_exports.sizeFromShape(reduceShape);
    const xSize = util_exports.sizeFromShape(input.shape);
    const batchSize = xSize / inSize;
    const reduceInfo = { windowSize: inSize, inSize, batchSize, outSize: 1 };
    const dtype = reduceType === "mean" ? "float32" : sumOutType(x.dtype);
    const uniformData = [
      { type: "int32", data: [inSize] }
    ];
    const program = new ReduceProgram(reduceInfo, reduceType);
    const reduced = backend.runWebGPUProgram(program, [input], dtype, uniformData);
    toDispose.push(reduced);
    res = reshape({ inputs: { x: reduced }, attrs: { shape: resOutShape }, backend });
  }
  toDispose.forEach((t) => backend.disposeData(t.dataId));
  return res;
}

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/All.js
function all(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { keepDims, axis } = attrs;
  return reduce(x, axis, keepDims, "all", backend);
}
var allConfig = {
  kernelName: All,
  backendName: "webgpu",
  kernelFunc: all
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Any.js
function any(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { keepDims, axis } = attrs;
  return reduce(x, axis, keepDims, "any", backend);
}
var anyConfig = {
  kernelName: Any,
  backendName: "webgpu",
  kernelFunc: any
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/argminmax_webgpu.js
var ArgMinMaxProgram = class {
  constructor(inputShape, axis, reduceType) {
    this.workgroupSize = [64, 1, 1];
    this.variableNames = ["x"];
    this.uniforms = "infinityValue : f32,";
    this.size = true;
    const axes = [axis];
    this.op = reduceType === "min" ? "<" : ">";
    const [outputShape, reduceShape] = backend_util_exports.computeOutAndReduceShapes(inputShape, axes);
    this.outputShape = outputShape.length === 0 ? [1] : outputShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    if (util_exports.sizeFromShape(reduceShape) < 32 || util_exports.sizeFromShape(outputShape) > 1e3) {
      this.type = "plain";
      this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    } else {
      this.type = "shared";
      this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, [1, 1, 1]);
    }
    this.inputShape = inputShape;
    this.shaderKey = `argMinMax_${this.op}_${this.type}`;
  }
  getUserCode() {
    const workgroupSizeX = this.workgroupSize[0];
    const getInputShapeLastDim = () => {
      if (this.inputShape.length === 1) {
        return "uniforms.xShape";
      } else {
        return `uniforms.xShape.${getCoordsXYZ(this.inputShape.length - 1)}`;
      }
    };
    const splitOutputCoords = () => {
      let snippet = "";
      if (this.outputShape.length === 1) {
        if (this.inputShape.length !== 1) {
          snippet += "outputCoords,";
        }
      } else {
        for (let i = 0; i < this.outputShape.length; i++) {
          snippet += `outputCoords.${getCoordsXYZ(i)},`;
        }
      }
      return snippet;
    };
    if (this.type === "shared") {
      const sharedMemorySnippet = `
      var<workgroup> xBestIndices : array<i32, ${workgroupSizeX}>;
      var<workgroup> xBestValues : array<f32, ${workgroupSizeX}>;
    `;
      const userCode = `
      fn DIV_CEIL(a : u32, b : u32) -> u32 {
        return ((a - 1u) / b + 1u);
      }

      ${sharedMemorySnippet}

      ${getMainHeaderString("index")} {
        let outputIndex = index / ${workgroupSizeX};
        let reduceLength = ${getInputShapeLastDim()};

        var bestIndex = i32(localId.x);
        var bestValue = uniforms.infinityValue;
        let outputCoords = getCoordsFromIndex(outputIndex);
        for (var k = i32(localId.x); k < reduceLength && outputIndex < uniforms.size;
            k = k + ${workgroupSizeX}) {
          let candidate = getX(${splitOutputCoords()} k);
          if (!isnan(candidate) && candidate ${this.op} bestValue) {
            bestValue = candidate;
            bestIndex = k;
          }
        }
        xBestValues[localId.x] = bestValue;
        xBestIndices[localId.x] = bestIndex;
        workgroupBarrier();

        var reduceSize = min(u32(reduceLength), ${workgroupSizeX}u);
        for (var currentSize = reduceSize / 2u; reduceSize > 1u;
            currentSize = reduceSize / 2u) {
          let interval = DIV_CEIL(reduceSize, 2u);
          if (localId.x < currentSize) {
            let candidate = xBestValues[localId.x + interval];
            if (candidate ${this.op} bestValue) {
              bestValue = candidate;
              xBestValues[localId.x] = bestValue;
              xBestIndices[localId.x] = xBestIndices[localId.x + interval];
            }
          }
          reduceSize = interval;
          workgroupBarrier();
        }

        if (localId.x == 0u && outputIndex < uniforms.size) {
          setOutputAtIndexI32(outputIndex, xBestIndices[localId.x]);
        }
      }
    `;
      return userCode;
    } else {
      const userCode = `
      ${getMainHeaderString("index")} {
        if (index < uniforms.size) {
          let outputCoords = getCoordsFromIndex(index);
          var bestIndex = 0;
          var bestValue = getX(${splitOutputCoords()} 0);
          let reduceLength = ${getInputShapeLastDim()};
          for (var i = 1; i < reduceLength; i++) {
            let candidate = getX(${splitOutputCoords()} i);
            if (candidate ${this.op} bestValue) {
              bestValue = candidate;
              bestIndex = i;
            }
          }
          setOutputAtIndexI32(index, bestIndex);
        }
      }
      `;
      return userCode;
    }
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/ArgMax.js
function argMax(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { axis } = attrs;
  let axes = util_exports.parseAxisParam(axis, x.shape);
  const permutedAxes = backend_util_exports.getAxesPermutation(axes, x.shape.length);
  let $x = x;
  const intermediateTensorInfos = [];
  if (permutedAxes != null) {
    $x = transpose({ inputs: { x }, backend, attrs: { perm: permutedAxes } });
    intermediateTensorInfos.push($x);
    axes = backend_util_exports.getInnerMostAxes(axes.length, $x.shape.length);
  }
  backend_util_exports.assertAxesAreInnerMostDims("argMax", [axes[0]], $x.shape.length);
  const program = new ArgMinMaxProgram($x.shape, axes[0], "max");
  const uniformData = [{ type: "float32", data: [Number.NEGATIVE_INFINITY] }];
  const out = backend.runWebGPUProgram(program, [$x], "int32", uniformData);
  intermediateTensorInfos.forEach((t) => backend.disposeData(t.dataId));
  return out;
}
var argMaxConfig = {
  kernelName: ArgMax,
  backendName: "webgpu",
  kernelFunc: argMax
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/ArgMin.js
function argMin(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { axis } = attrs;
  let axes = util_exports.parseAxisParam(axis, x.shape);
  const permutedAxes = backend_util_exports.getAxesPermutation(axes, x.shape.length);
  let $x = x;
  const intermediateTensorInfos = [];
  if (permutedAxes != null) {
    $x = transpose({ inputs: { x }, backend, attrs: { perm: permutedAxes } });
    intermediateTensorInfos.push($x);
    axes = backend_util_exports.getInnerMostAxes(axes.length, $x.shape.length);
  }
  backend_util_exports.assertAxesAreInnerMostDims("argMin", [axes[0]], $x.shape.length);
  const program = new ArgMinMaxProgram($x.shape, axes[0], "min");
  const uniformData = [{ type: "float32", data: [Number.POSITIVE_INFINITY] }];
  const out = backend.runWebGPUProgram(program, [$x], "int32", uniformData);
  intermediateTensorInfos.forEach((t) => backend.disposeData(t.dataId));
  return out;
}
var argMinConfig = {
  kernelName: ArgMin,
  backendName: "webgpu",
  kernelFunc: argMin
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Asin.js
var asin = unaryKernelFunc({ opType: UnaryOpType.ASIN });
var asinConfig = {
  kernelName: Asin,
  backendName: "webgpu",
  kernelFunc: asin
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Asinh.js
var asinh = unaryKernelFunc({ opType: UnaryOpType.ASINH });
var asinhConfig = {
  kernelName: Asinh,
  backendName: "webgpu",
  kernelFunc: asinh
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Atan.js
var atan = unaryKernelFunc({ opType: UnaryOpType.ATAN });
var atanConfig = {
  kernelName: Atan,
  backendName: "webgpu",
  kernelFunc: atan
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Atan2.js
var atan2 = binaryKernelFunc({ opType: BinaryOpType.ATAN2 });
var atan2Config = {
  kernelName: Atan2,
  backendName: "webgpu",
  kernelFunc: atan2
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Atanh.js
var atanh = unaryKernelFunc({ opType: UnaryOpType.ATANH });
var atanhConfig = {
  kernelName: Atanh,
  backendName: "webgpu",
  kernelFunc: atanh
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/pool2d_webgpu.js
var Pool2DProgram = class {
  constructor(convInfo, poolType) {
    this.variableNames = ["x"];
    this.uniforms = `stride : vec2<i32>, pad : vec2<i32>, dilation : vec2<i32>, convDims : vec2<i32>, filterDims : vec2<i32>,`;
    this.workgroupSize = [128, 1, 1];
    this.size = true;
    this.outputShape = convInfo.outShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.shaderKey = `pool2D_${poolType}`;
    this.poolType = poolType;
  }
  getUserCode() {
    let updateSnippet = `resultValue = max(value, resultValue);`;
    if (this.poolType === "avg") {
      updateSnippet = `resultValue = resultValue + value; count = count + 1.0;`;
    }
    let returnValue = `resultValue`;
    if (this.poolType === "avg") {
      returnValue = `resultValue / max(count, 1.0)`;
    }
    const userCode = `
      ${getMainHeaderString("index")} {
      if (index < uniforms.size) {
        let coords = getCoordsFromIndex(index);
          let batch = coords[0];
          let xRCCorner = vec2<i32>(coords.yz) * uniforms.stride - uniforms.pad;
          let xRCorner = xRCCorner.x;
          let xCCorner = xRCCorner.y;

          var resultValue = ${this.poolType === "avg" ? "0.0" : "-1.0 / pow(10.0, -20.0)"};
          var count = 0.0;

          for (var wR = 0; wR < uniforms.filterDims.x; wR = wR + uniforms.dilation.x) {
            let xR = xRCorner + wR;

            if (xR < 0 || xR >= uniforms.convDims.x) {
              continue;
            }

            for (var wC = 0; wC < uniforms.filterDims.y; wC = wC + uniforms.dilation.y) {
              let xC = xCCorner + wC;
              if (xC < 0 || xC >= uniforms.convDims.y) {
                continue;
              }

              let value = getX(batch, xR, xC, coords[3]);
              ${updateSnippet}
            }
          }

          setOutputAtIndex(index, ${returnValue});
        }
      }
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/pool_filtersizeone_webgpu.js
var PoolWithFilterSizeEqualsOneProgram = class {
  constructor(convInfo) {
    this.variableNames = ["x"];
    this.uniforms = `stride : vec2<i32>,`;
    this.workgroupSize = [256, 1, 1];
    this.size = true;
    this.outputShape = convInfo.outShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.shaderKey = "poolWithFilterSizeEqualsOne";
  }
  getUserCode() {
    const userCode = `
      ${getMainHeaderString("index")} {
        if (index < uniforms.size) {
          let coords = getCoordsFromIndex(index);
          let batch = coords[0];
          let d = coords[3];

          let xRCCorner = coords.yz * uniforms.stride;
          let xRCorner = xRCCorner.x;
          let xCCorner = xRCCorner.y;

          let value = getX(batch, xRCorner, xCCorner, d);
          setOutputAtIndex(index, value);
        }
      }
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Max.js
function max(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { reductionIndices, keepDims } = attrs;
  return reduce(x, reductionIndices, keepDims, "max", backend);
}
var maxConfig = {
  kernelName: Max,
  backendName: "webgpu",
  kernelFunc: max
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Mean.js
function mean(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { keepDims, axis } = attrs;
  return reduce(x, axis, keepDims, "mean", backend);
}
var meanConfig = {
  kernelName: Mean,
  backendName: "webgpu",
  kernelFunc: mean
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Pool_impl.js
function poolImpl(x, convInfo, poolType, backend) {
  if (convInfo.filterWidth === 1 && convInfo.filterHeight === 1 && util_exports.arraysEqual(convInfo.inShape, convInfo.outShape)) {
    return identity({ inputs: { x }, backend });
  }
  if (convInfo.filterWidth === convInfo.inWidth && convInfo.filterHeight === convInfo.inHeight && convInfo.batchSize === 1 && convInfo.padInfo.type === "VALID") {
    const length = x.shape.length;
    const reshapeX = reshape({
      inputs: { x },
      backend,
      attrs: {
        shape: [
          x.shape[length - 3] * x.shape[length - 2],
          x.shape[length - 1]
          /* channel */
        ]
      }
    });
    let reduceX;
    if (poolType === "avg") {
      reduceX = mean({ inputs: { x: reshapeX }, backend, attrs: { axis: 0, keepDims: false } });
    } else {
      util_exports.assert(poolType === "max", () => `Invalid pool type ${poolType}`);
      reduceX = max({
        inputs: { x: reshapeX },
        backend,
        attrs: { reductionIndices: 0, keepDims: false }
      });
    }
    const result = reshape({ inputs: { x: reduceX }, backend, attrs: { shape: convInfo.outShape } });
    backend.disposeData(reshapeX.dataId);
    backend.disposeData(reduceX.dataId);
    return result;
  }
  let program;
  const dimensions = [{ type: "int32", data: [convInfo.strideHeight, convInfo.strideWidth] }];
  if (convInfo.filterHeight === 1 && convInfo.filterWidth === 1) {
    program = new PoolWithFilterSizeEqualsOneProgram(convInfo);
  } else {
    if (poolType === "avg") {
      program = new Pool2DProgram(convInfo, "avg");
    } else {
      util_exports.assert(poolType === "max", () => `Invalid pool type ${poolType}`);
      program = new Pool2DProgram(convInfo, "max");
    }
    dimensions.push({ type: "int32", data: [convInfo.padInfo.top, convInfo.padInfo.left] }, {
      type: "int32",
      data: [convInfo.dilationHeight, convInfo.dilationWidth]
    }, { type: "int32", data: [convInfo.inHeight, convInfo.inWidth] }, {
      type: "int32",
      data: [convInfo.effectiveFilterHeight, convInfo.effectiveFilterWidth]
    });
  }
  return backend.runWebGPUProgram(program, [x], x.dtype, dimensions);
}

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/AvgPool.js
function avgPool(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { filterSize, strides, pad, dimRoundingMode } = attrs;
  const dilations = 1;
  const convInfo = backend_util_exports.computePool2DInfo(x.shape, filterSize, strides, dilations, pad, dimRoundingMode);
  return poolImpl(x, convInfo, "avg", backend);
}
var avgPoolConfig = {
  kernelName: AvgPool,
  backendName: "webgpu",
  kernelFunc: avgPool
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/avg_pool2d_backprop_webgpu.js
var AvgPool2DBackpropProgram = class {
  constructor(convInfo) {
    this.variableNames = ["dy"];
    this.uniforms = `stride : vec2<i32>, pads : vec2<i32>, dilation : vec2<i32>, filterDims : vec2<i32>,
       outHeight : i32, outWidth : i32, avgMultiplier : f32,`;
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = convInfo.inShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.shaderKey = `avg_pool2d_backprop`;
  }
  getUserCode() {
    const userCode = `
      ${getMainHeaderString("index")} {
      if (index < uniforms.size) {
        let coords = getCoordsFromIndex(index);
        let batch = coords[0];
        let d = coords[3];

        let dyRCCorner = vec2<i32>(coords.yz) - uniforms.pads;
        let dyRCorner = dyRCCorner.x;
        let dyCCorner = dyRCCorner.y;

        // Convolve dy(?, ?, d) with pos mask(:, :, d) to get dx(xR, xC, d).
        // ? = to be determined. : = across all values in that axis.
        var dotProd = 0.0;
        for (var wR = 0; wR < uniforms.filterDims[0]; wR = wR + uniforms.dilation[0]) {
          let dyR = f32(dyRCorner + wR) / f32(uniforms.stride[0]);

          if (dyR < 0.0 || dyR >= f32(uniforms.outHeight) || fract(dyR) > 0.0) {
            continue;
          }
          let idyR = i32(dyR);

          for (var wC = 0; wC < uniforms.filterDims[1]; wC = wC + uniforms.dilation[1]) {
            let dyC = f32(dyCCorner + wC) / f32(uniforms.stride[1]);

            if (dyC < 0.0 || dyC >= f32(uniforms.outWidth) || fract(dyC) > 0.0) {
              continue;
            }
            let idyC = i32(dyC);

            let dyValue = getDy(batch, idyR, idyC, d);

            dotProd = dotProd + dyValue * uniforms.avgMultiplier;
          }
        }
        setOutputAtIndex(index, dotProd);
      }
    }
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/AvgPoolGrad.js
function avgPoolGrad(args) {
  const { inputs, backend, attrs } = args;
  const { dy, input } = inputs;
  const x = input;
  assertNotComplex([dy, input], "avgPoolGrad");
  const { filterSize, strides, pad } = attrs;
  const convInfo = backend_util_exports.computePool2DInfo(x.shape, filterSize, strides, 1, pad);
  const program = new AvgPool2DBackpropProgram(convInfo);
  const avgMultiplier = 1 / (convInfo.filterHeight * convInfo.filterWidth);
  const uniformData = [
    { type: "int32", data: [convInfo.strideHeight, convInfo.strideWidth] },
    {
      type: "int32",
      data: [
        convInfo.effectiveFilterHeight - 1 - convInfo.padInfo.top,
        convInfo.effectiveFilterWidth - 1 - convInfo.padInfo.left
      ]
    },
    { type: "int32", data: [convInfo.dilationHeight, convInfo.dilationWidth] },
    {
      type: "int32",
      data: [convInfo.effectiveFilterHeight, convInfo.effectiveFilterWidth]
    },
    { type: "int32", data: [convInfo.outHeight] },
    { type: "int32", data: [convInfo.outWidth] },
    { type: "float32", data: [avgMultiplier] }
  ];
  return backend.runWebGPUProgram(program, [dy], x.dtype, uniformData);
}
var avgPoolGradConfig = {
  kernelName: AvgPoolGrad,
  backendName: "webgpu",
  kernelFunc: avgPoolGrad
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/BatchMatMul.js
function batchMatMul(args) {
  const { inputs, backend, attrs } = args;
  const { a, b } = inputs;
  const { transposeA, transposeB } = attrs;
  return batchMatMulImpl({ a, b, transposeA, transposeB, backend });
}
var batchMatMulConfig = {
  kernelName: BatchMatMul,
  backendName: "webgpu",
  kernelFunc: batchMatMul
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/slice_webgpu.js
var SliceProgram = class {
  constructor(start, destSize) {
    this.variableNames = ["source"];
    this.workPerThread = 1;
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = destSize;
    this.rank = destSize.length;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize, [this.workPerThread, 1, 1]);
    this.start = start;
    this.uniforms = `start : ${getCoordsDataType(start.length)}, `;
    this.shaderKey = "slice";
  }
  getUserCode() {
    const dtype = getCoordsDataType(this.rank);
    const sourceCoords = getCoords(this.rank);
    let coordSum;
    if (this.start.length === 1) {
      coordSum = this.outputShape.map((_, i) => {
        return `sourceLoc = uniforms.start + coords;`;
      });
    } else {
      coordSum = this.outputShape.map((_, i) => {
        return `sourceLoc.${coords[i]} = uniforms.start.${getCoordsXYZ(i)} + coords.${coords[i]};`;
      });
    }
    const userCode = `
      ${getMainHeaderString("index")} {
        if (index < uniforms.size) {
          var sourceLoc : ${dtype};
          let coords = getCoordsFromIndex(index);
          ${coordSum.join("\n")}
          setOutputAtIndex(index, getSource(${sourceCoords}));
        }
      }
    `;
    return userCode;
  }
};
var coords = ["x", "y", "z", "w", "u", "v"];
function getCoords(rank) {
  if (rank === 1) {
    return "sourceLoc";
  } else if (rank <= 6) {
    return coords.slice(0, rank).map((coord) => `sourceLoc.${coord}`).join(",");
  } else {
    throw Error(`Slicing for rank ${rank} is not yet supported`);
  }
}

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Slice.js
function slice(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { begin, size } = attrs;
  const [$begin, $size] = slice_util_exports.parseSliceParams(x, begin, size);
  slice_util_exports.assertParamsValid(x, $begin, $size);
  if (backend.shouldExecuteOnCPU([x]) || x.dtype === "string") {
    const xBufferInfo = backend.tensorMap.get(x.dataId);
    const outValues = sliceImplCPU(xBufferInfo.values, $begin, $size, x.shape, x.dtype);
    return backend.makeTensorInfo($size, x.dtype, outValues);
  }
  if (util_exports.sizeFromShape($size) === 0) {
    return backend.makeTensorInfo($size, x.dtype, []);
  }
  const program = new SliceProgram($begin, $size);
  const uniformData = [{ type: "int32", data: $begin }];
  return backend.runWebGPUProgram(program, [x], x.dtype, uniformData);
}
var sliceConfig = {
  kernelName: Slice,
  backendName: "webgpu",
  kernelFunc: slice
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/BatchToSpaceND.js
var batchToSpaceND = (args) => {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { blockShape, crops } = attrs;
  util_exports.assert(x.shape.length <= 4, () => "batchToSpaceND for rank > 4 with a WebGPU backend not implemented yet");
  const prod2 = blockShape.reduce((a, b) => a * b);
  const reshaped = backend_util_exports.getReshaped(x.shape, blockShape, prod2);
  const permuted = backend_util_exports.getPermuted(reshaped.length, blockShape.length);
  const reshapedPermuted = backend_util_exports.getReshapedPermuted(x.shape, blockShape, prod2);
  const sliceBeginCoords = backend_util_exports.getSliceBeginCoords(crops, blockShape.length);
  const sliceSize = backend_util_exports.getSliceSize(reshapedPermuted, crops, blockShape.length);
  const toDispose = [];
  const reshapedIntermediate = reshape({ inputs: { x }, backend, attrs: { shape: reshaped } });
  const transposedIntermediate = transpose({ inputs: { x: reshapedIntermediate }, backend, attrs: { perm: permuted } });
  const reshapedIntermediate2 = reshape({
    inputs: { x: transposedIntermediate },
    backend,
    attrs: { shape: reshapedPermuted }
  });
  const sliced = slice({
    inputs: { x: reshapedIntermediate2 },
    backend,
    attrs: { begin: sliceBeginCoords, size: sliceSize }
  });
  toDispose.push(reshapedIntermediate);
  toDispose.push(transposedIntermediate);
  toDispose.push(reshapedIntermediate2);
  toDispose.forEach((t) => backend.disposeData(t.dataId));
  return sliced;
};
var batchToSpaceNDConfig = {
  kernelName: BatchToSpaceND,
  backendName: "webgpu",
  kernelFunc: batchToSpaceND
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/bincount_webgpu.js
var writeSnippet = `
  fn bincount_write(index: i32, value: f32) {
    ${atomicAddSnippet("&result[index]", "value", "float32")}
  }
`;
var binaryWriteSnippet = `
  fn bincount_write(index: i32, value: f32) {
    atomicStore(&result[index], bitcast<i32>(value));
  }
`;
var BincountProgram = class {
  constructor(shape, hasWeights, binaryOutput = false) {
    this.outputShape = [];
    this.variableNames = ["x"];
    this.uniforms = "binCountSize : i32,";
    this.workgroupSize = [64, 1, 1];
    this.atomic = true;
    this.hasWeights = true;
    this.binaryOutput = false;
    this.outputShape = shape;
    this.rank = shape.length;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.binaryOutput = binaryOutput;
    if (binaryOutput) {
      this.atomic = false;
    }
    this.hasWeights = hasWeights;
    if (this.hasWeights) {
      this.variableNames.push("w");
    }
    this.shaderKey = `bincount_${this.hasWeights}_${this.binaryOutput}_${this.rank}`;
  }
  getUserCode() {
    const userCode = `
    ${this.binaryOutput ? binaryWriteSnippet : writeSnippet}
  ${getMainHeaderString("index")} {
    ${this.rank === 1 ? `if (index < uniforms.xShape) {
      let indexVal = i32(getX(index));
      if (indexVal < uniforms.binCountSize) {
        let value = ${this.binaryOutput ? 1 : this.hasWeights ? "getW(index)" : "1."};
        bincount_write(indexVal, value);
      }
    }` : `let coord = getCoordsFromIndex(index);
    if (coordsInBounds2D(coord, uniforms.xShape)) {
      let indexVal = i32(getX(coord[0], coord[1]));
      if (indexVal < uniforms.binCountSize) {
        let value = ${this.binaryOutput ? 1 : this.hasWeights ? "getW(coord[0], coord[1])" : "1."};
        bincount_write(coord.x * uniforms.binCountSize + indexVal, value);
      }
    }`}
  }
  `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Bincount.js
function bincount(args) {
  const { inputs, backend, attrs } = args;
  const { x, weights } = inputs;
  const { size } = attrs;
  const xSize = util_exports.sizeFromShape(x.shape);
  const weightsSize = util_exports.sizeFromShape(weights.shape);
  const hasWeights = weightsSize > 0;
  const outputSize = [size];
  const dtype = weights.dtype;
  const output = fill({ backend, attrs: { shape: outputSize, value: 0, dtype } });
  const program = new BincountProgram([xSize], hasWeights);
  const uniformData = [{ type: "int32", data: [size] }];
  const bincountInputs = hasWeights ? [x, weights] : [x];
  const res = backend.runWebGPUProgram(program, bincountInputs, dtype, uniformData, output);
  return res;
}
var bincountConfig = {
  kernelName: Bincount,
  backendName: "webgpu",
  kernelFunc: bincount
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/NotEqual.js
var notEqual = binaryKernelFunc({
  opType: BinaryOpType.NOT_EQUAL,
  dtype: "bool",
  cpuKernelImpl: notEqualImplCPU
});
var notEqualConfig = {
  kernelName: NotEqual,
  backendName: "webgpu",
  kernelFunc: notEqual
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Real.js
function real(args) {
  const { inputs, backend } = args;
  const { input } = inputs;
  const inputData = backend.tensorMap.get(input.dataId);
  return identity({ inputs: { x: inputData.complexTensorInfos.real }, backend });
}
var realConfig = {
  kernelName: Real,
  backendName: "webgpu",
  kernelFunc: real
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernel_utils/int.js
function int(input, backend) {
  const program = new UnaryOpProgram(input.shape, UnaryOpType.TO_INT);
  const output = backend.runWebGPUProgram(program, [input], "int32");
  return { dataId: output.dataId, shape: output.shape, dtype: output.dtype };
}

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Cast.js
function cast(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { dtype } = attrs;
  if (dtype === "complex64") {
    if (x.dtype === "complex64") {
      return identity({ inputs: { x }, backend });
    }
    const zerosTensor = zeros(x.shape);
    const floatX = cast({ inputs: { x }, backend, attrs: { dtype: "float32" } });
    const result = complex({ inputs: { real: floatX, imag: zerosTensor }, backend });
    zerosTensor.dispose();
    backend.disposeData(floatX.dataId);
    return result;
  }
  if (x.dtype === "complex64") {
    const realPart = real({ inputs: { input: x }, backend });
    const result = cast({ inputs: { x: realPart }, backend, attrs: { dtype } });
    backend.disposeData(realPart.dataId);
    return result;
  }
  if (!util_exports.hasEncodingLoss(x.dtype, dtype)) {
    const result = identity({ inputs: { x }, backend });
    return { dataId: result.dataId, shape: result.shape, dtype };
  }
  if (backend.shouldExecuteOnCPU([x])) {
    const values = backend.tensorMap.get(x.dataId).values;
    const [resultShape, resultType, resultData] = castImplCPU(values, x.shape, x.dtype, dtype);
    return backend.makeTensorInfo(resultShape, resultType, resultData);
  }
  if (dtype === "int32") {
    return int(x, backend);
  }
  if (dtype === "bool") {
    const zerosTensorInfo = backend.makeTensorInfo([], "bool", util_exports.getTypedArrayFromDType("bool", 1));
    const binaryInputs = { a: x, b: zerosTensorInfo };
    const result = notEqual({ inputs: binaryInputs, backend });
    backend.disposeData(zerosTensorInfo.dataId);
    return result;
  }
  throw new Error(`Error in Cast: failed to cast ${x.dtype} to ${dtype}`);
}
var castConfig = {
  kernelName: Cast,
  backendName: "webgpu",
  kernelFunc: cast
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Ceil.js
var ceil = unaryKernelFunc({ opType: UnaryOpType.CEIL, cpuKernelImpl: ceilImplCPU });
var ceilConfig = {
  kernelName: Ceil,
  backendName: "webgpu",
  kernelFunc: ceil
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/clip_vec4_webgpu.js
var ClipVec4Program = class {
  constructor(outputShape) {
    this.variableNames = ["A"];
    this.uniforms = "minVal : f32, maxVal : f32,";
    this.workPerThread = 4;
    this.workgroupSize = [64, 1, 1];
    this.isVec4 = true;
    this.size = true;
    this.outputShape = outputShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize, [this.workPerThread, 1, 1]);
    this.shaderKey = "clipVec4";
  }
  getUserCode() {
    const userCode = `
      ${getMainHeaderString("index")} {
        if(index < uniforms.size) {
          let value = getAByOutputIndex(index);
          var clampedValue = clamp(
              value, vec4<f32>(uniforms.minVal), vec4<f32>(uniforms.maxVal));
          clampedValue = select(clampedValue, value, isnanVec4(value));
          setOutputAtIndex(index, clampedValue);
        }
      }
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/clip_webgpu.js
var ClipProgram = class {
  constructor(outputShape) {
    this.variableNames = ["A"];
    this.uniforms = "minVal : f32, maxVal : f32,";
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = outputShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.shaderKey = "clip";
  }
  getUserCode() {
    const userCode = `
      ${getMainHeaderString("index")} {
        if(index < uniforms.size) {
          let value = getAByOutputIndex(index);
          if (isnan(value)) {
            setOutputAtIndex(index, value);
            return;
          }
          setOutputAtIndex(index, clamp(value, uniforms.minVal, uniforms.maxVal));
        }
      }
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/ClipByValue.js
function clipByValue(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { clipValueMin, clipValueMax } = attrs;
  let program;
  const uniformData = [
    { type: "float32", data: [clipValueMin] },
    { type: "float32", data: [clipValueMax] }
  ];
  if (util_exports.sizeFromShape(x.shape) % 4 === 0) {
    program = new ClipVec4Program(x.shape);
  } else {
    program = new ClipProgram(x.shape);
  }
  return backend.runWebGPUProgram(program, [x], x.dtype, uniformData);
}
var clipByValueConfig = {
  kernelName: ClipByValue,
  backendName: "webgpu",
  kernelFunc: clipByValue
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/concat_webgpu.js
var ConcatProgram = class {
  constructor(shapes) {
    this.uniforms = "";
    this.workPerThread = 1;
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = backend_util_exports.computeOutShape(
      shapes,
      1
      /* axis */
    );
    this.variableNames = shapes.map((_, i) => `T${i}`);
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize, [this.workPerThread, 1, 1]);
    this.offsetLength = shapes.length - 1;
    for (let i = 0; i < this.offsetLength; i++) {
      this.uniforms += `offset${i} : i32,`;
    }
    this.shaderKey = "concat";
  }
  getUserCode() {
    const snippets = [];
    if (this.offsetLength > 0) {
      snippets.push(`if (yC < uniforms.offset0){ setOutputAtCoords(coords.x, coords.y, getT0(yR, yC)); }`);
      for (let i = 1; i < this.offsetLength; i++) {
        snippets.push(`else if (yC < uniforms.offset${[i]}){ setOutputAtCoords(coords.x, coords.y, getT${i}(yR, yC - uniforms.offset${i - 1})); }`);
      }
      const lastIndex = this.offsetLength;
      const lastShiftIndex = this.offsetLength - 1;
      snippets.push(`else { setOutputAtCoords(coords.x, coords.y, getT${lastIndex}(yR, yC - uniforms.offset${lastShiftIndex})); }`);
    } else {
      snippets.push(`setOutputAtCoords(coords.x, coords.y, getT0(yR, yC));`);
    }
    const userCode = `
      ${getMainHeaderString("index")} {
        for(var i = 0; i < ${this.workPerThread}; i = i + 1) {
          let flatIndex = index * ${this.workPerThread} + i;
          if(flatIndex < uniforms.size) {
            let coords = getCoordsFromIndex(flatIndex);
            let yR = coords.x;
            let yC = coords.y;

            ${snippets.join("\n        ")}
          }
        }
      }
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Imag.js
function imag(args) {
  const { inputs, backend } = args;
  const { input } = inputs;
  const inputData = backend.tensorMap.get(input.dataId);
  return identity({ inputs: { x: inputData.complexTensorInfos.imag }, backend });
}
var imagConfig = {
  kernelName: Imag,
  backendName: "webgpu",
  kernelFunc: imag
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Concat_impl.js
function concatImpl(inputs, axis, backend) {
  const dtype = inputs[0].dtype;
  if (dtype === "complex64") {
    const reals = inputs.map((t) => real({ inputs: { input: t }, backend }));
    const imags = inputs.map((t) => imag({ inputs: { input: t }, backend }));
    const realConcated = concatImpl(reals, axis, backend);
    const imagConcated = concatImpl(imags, axis, backend);
    const result = complex({ inputs: { real: realConcated, imag: imagConcated }, backend });
    reals.forEach((r) => backend.disposeData(r.dataId));
    imags.forEach((i) => backend.disposeData(i.dataId));
    backend.disposeData(realConcated.dataId);
    backend.disposeData(imagConcated.dataId);
    return result;
  }
  let runOnCpu = backend.shouldExecuteOnCPU(inputs);
  if (dtype === "string") {
    runOnCpu = true;
  }
  if (runOnCpu) {
    const tensors2D2 = inputs.map((t) => {
      const innerSize = util_exports.sizeFromShape(t.shape.slice(axis));
      const shape = [-1, innerSize];
      return reshape({ inputs: { x: t }, backend, attrs: { shape } });
    });
    const inputsValShapes = tensors2D2.map((t) => {
      return { vals: backend.readSync(t.dataId), shape: t.shape };
    });
    const outShape2 = backend_util_exports.computeOutShape(
      tensors2D2.map((t) => t.shape),
      1
      /* axis */
    );
    const simplyConcat = tensors2D2[0].shape[0] === 1;
    const outVals = concatImplCPU(inputsValShapes, outShape2, dtype, simplyConcat);
    const finalOutShape = backend_util_exports.computeOutShape(inputs.map((t) => t.shape), axis);
    const outInfo = backend.makeTensorInfo(finalOutShape, dtype, outVals);
    tensors2D2.forEach((t) => backend.disposeData(t.dataId));
    return outInfo;
  }
  const maxInputNum = backend.device.limits.maxStorageBuffersPerShaderStage - 1;
  if (inputs.length > maxInputNum) {
    const reducedInputs = [];
    for (let i = 0; i < inputs.length; i += maxInputNum) {
      const subArray = inputs.slice(i, i + maxInputNum);
      reducedInputs.push(concatImpl(subArray, axis, backend));
    }
    const result = concatImpl(reducedInputs, axis, backend);
    for (const i of reducedInputs) {
      backend.disposeData(i.dataId);
    }
    return result;
  }
  const { tensors2D, outShape } = computeTensors2D(inputs, axis, backend);
  const shapes = tensors2D.map((t) => t.shape);
  const program = new ConcatProgram(shapes);
  const uniformData = [];
  const offsets = new Array(shapes.length - 1);
  if (offsets.length > 0) {
    offsets[0] = shapes[0][1];
    uniformData.push({ type: "int32", data: [offsets[0]] });
    for (let i = 1; i < offsets.length; i++) {
      offsets[i] = offsets[i - 1] + shapes[i][1];
      uniformData.push({ type: "int32", data: [offsets[i]] });
    }
  }
  const res = backend.runWebGPUProgram(program, tensors2D, tensors2D[0].dtype, uniformData);
  tensors2D.forEach((r) => backend.disposeData(r.dataId));
  const reshapedResult = reshape({ inputs: { x: res }, backend, attrs: { shape: outShape } });
  backend.disposeData(res.dataId);
  return reshapedResult;
}
function computeTensors2D(inputs, axis, backend) {
  const outShape = backend_util_exports.computeOutShape(inputs.map((t) => t.shape), axis);
  const tensors2D = inputs.map((t) => reshape({
    inputs: { x: t },
    backend,
    attrs: {
      shape: [
        util_exports.sizeFromShape(t.shape.slice(0, axis)),
        util_exports.sizeFromShape(t.shape.slice(axis))
      ]
    }
  }));
  return { tensors2D, outShape };
}

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Concat.js
function concat(args) {
  const { inputs, backend, attrs } = args;
  const { axis } = attrs;
  const $axis = util_exports.parseAxisParam(axis, inputs[0].shape)[0];
  const shapes = inputs.map((t) => t.shape);
  backend_util_exports.assertParamsConsistent(shapes, $axis);
  const outShape = backend_util_exports.computeOutShape(inputs.map((t) => t.shape), $axis);
  if (util_exports.sizeFromShape(outShape) === 0) {
    return backend.makeTensorInfo(outShape, inputs[0].dtype, []);
  }
  const $inputs = inputs.filter((t) => util_exports.sizeFromShape(t.shape) > 0);
  if ($inputs.length === 1) {
    return identity({ inputs: { x: $inputs[0] }, backend });
  }
  return concatImpl($inputs, $axis, backend);
}
var concatConfig = {
  kernelName: Concat,
  backendName: "webgpu",
  kernelFunc: concat
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/conv2d_mm_webgpu.js
function conv2dCommonSnippet(isChannelsLast, fitAOuter, fitBOuter, fitInner, addBias = false, activation = null, hasPreluActivationWeights = false, innerElementSizeX = 4, innerElementSizeW = 4, innerElementSize = 4) {
  const getXSnippet = (innerElementSize2) => {
    switch (innerElementSize2) {
      case 1:
        return "resData = x[xIndex];";
      case 3:
        return "resData = vec3<f32>(x[xIndex], x[xIndex + 1], x[xIndex + 2]);";
      case 4:
        return "resData = x[xIndex / 4];";
      default:
        throw new Error(`innerElementSize ${innerElementSize2} is not supported.`);
    }
  };
  const getWSnippet = (innerElementSize2) => {
    switch (innerElementSize2) {
      case 1:
        return "return W[row * uniforms.wShape[3] + colIn];";
      case 4:
        return "return W[row * uniforms.wShape[3] / 4 + colIn];";
      default:
        throw new Error(`innerElementSize ${innerElementSize2} is not supported.`);
    }
  };
  const coordASnippet = isChannelsLast ? `
      let coord = vec4<i32>(batch, xRow, xCol, xCh);
      ` : `
      let coord = vec4<i32>(batch, xCh, xRow, xCol);
      `;
  const coordResSnippet = isChannelsLast ? `
      let coords = vec4<i32>(
        batch,
        row / outWidth,
        row % outWidth,
        col);
      ` : `
      let coords = vec4<i32>(
        batch,
        row,
        col / outWidth,
        col % outWidth);
      `;
  const xHight = isChannelsLast ? "uniforms.xShape[1]" : "uniforms.xShape[2]";
  const xWidth = isChannelsLast ? "uniforms.xShape[2]" : "uniforms.xShape[3]";
  const row = isChannelsLast ? "row" : "col";
  const col = isChannelsLast ? "col" : "row";
  const readXSnippet = `
      let inChannels = uniforms.wShape[2];
      let outWidth = ${isChannelsLast ? "uniforms.outShape[2]" : "uniforms.outShape[3]"};
      let outRow = ${row} / outWidth;
      let outCol = ${row} % outWidth;

      let WRow = ${col} / (uniforms.filterDims[1] * inChannels);
      let WCol = ${col} / inChannels % uniforms.filterDims[1];
      let xRow = outRow * uniforms.stride[0] + uniforms.dilation[0] * WRow - uniforms.pad[0];
      let xCol = outCol * uniforms.stride[1] + uniforms.dilation[1] * WCol - uniforms.pad[1];
      let xCh = ${col} % inChannels;
      var resData = ${typeSnippet(innerElementSizeX)}(0.0);
      // The bounds checking is always needed since we use it to pad zero for
      // the 'same' padding type.
      if (xRow >= 0 && xRow < ${xHight} && xCol >= 0 && xCol < ${xWidth}) {
        ${coordASnippet}
        let xIndex = getIndexFromCoords4D(coord, uniforms.xShape);
        ${getXSnippet(innerElementSizeX)}
      }
      return resData;`;
  const sampleX = isChannelsLast ? fitAOuter && fitInner ? `
      let col = colIn * ${innerElementSizeX};
      ${readXSnippet}` : `
      let col = colIn * ${innerElementSizeX};
      if (row < uniforms.dimAOuter && col < uniforms.dimInner) {
        ${readXSnippet}
      }
      return ${typeSnippet(innerElementSizeX)}(0.0);` : fitInner && fitBOuter ? `
      let col = colIn * ${innerElementSizeX};
      ${readXSnippet}` : `
      let col = colIn * ${innerElementSizeX};
      if (row < uniforms.dimInner && col < uniforms.dimBOuter) {
        ${readXSnippet}
      }
      return ${typeSnippet(innerElementSizeX)}(0.0);`;
  const sampleW = `${getWSnippet(innerElementSizeW)}`;
  const resType = typeSnippet(innerElementSize);
  const aType = isChannelsLast ? typeSnippet(innerElementSizeX) : typeSnippet(innerElementSizeW);
  const bType = isChannelsLast ? typeSnippet(innerElementSizeW) : typeSnippet(innerElementSizeX);
  const userCode = `
      ${activationFnSnippet(activation, hasPreluActivationWeights, innerElementSize === 4, 4)}
      fn mm_readA(batch: i32, row : i32, colIn : i32) -> ${aType} {
        ${isChannelsLast ? sampleX : sampleW}
      }

      fn mm_readB(batch: i32, row : i32, colIn : i32) -> ${bType} {
        ${isChannelsLast ? sampleW : sampleX}
      }

      fn mm_write(batch: i32, row : i32, colIn : i32, valueIn : ${resType}) {
        let col = colIn * ${innerElementSize};
        if (row < uniforms.dimAOuter && col < uniforms.dimBOuter)
        {
        var value = valueIn;
        let outWidth = ${isChannelsLast ? "uniforms.outShape[2]" : "uniforms.outShape[3]"};
        ${coordResSnippet}
        ${biasActivationSnippet(addBias, activation)}
        setOutputAtCoords(coords[0], coords[1], coords[2], coords[3], value);
        }
      }`;
  return userCode;
}
var Conv2DMMProgram = class {
  constructor(convInfo, dimAOuter, dimBOuter, dimInner, addBias = false, activation = null, hasPreluActivationWeights = false, sequentialAccessByThreads = false) {
    this.variableNames = ["x", "W"];
    this.uniforms = `filterDims : vec2<i32>, pad : vec2<i32>, stride : vec2<i32>, dilation : vec2<i32>, dimAOuter : i32, dimBOuter : i32, dimInner : i32,`;
    this.outputShape = convInfo.outShape;
    this.isChannelsLast = convInfo.dataFormat === "channelsLast";
    this.isVec4 = ((convInfo.inChannels % 4 === 0 || convInfo.inChannels % 3 === 0) && this.isChannelsLast || convInfo.outWidth % 4 === 0 && !this.isChannelsLast) && convInfo.outChannels % 4 === 0;
    this.dispatchLayout = this.isChannelsLast ? { x: [3], y: [1, 2], z: [0] } : { x: [2, 3], y: [1], z: [0] };
    this.workgroupSize = computeWorkgroupSizeForConv2d(this.dispatchLayout, this.outputShape, this.isVec4);
    this.elementsPerThread = computeWorkPerThreadForConv2d(this.dispatchLayout, this.outputShape, this.isVec4);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize, this.elementsPerThread);
    if (this.isVec4) {
      if (this.isChannelsLast && convInfo.inChannels % 4 !== 0) {
        this.innerElementSize = 3;
        this.variableTypes = ["f32", "vec4<f32>"];
      } else {
        this.innerElementSize = 4;
        this.variableTypes = ["vec4<f32>", "vec4<f32>"];
      }
      if (addBias) {
        this.variableNames.push("bias");
        this.variableTypes.push("vec4<f32>");
      }
      if (hasPreluActivationWeights) {
        this.variableNames.push("preluActivationWeights");
        this.variableTypes.push("vec4<f32>");
      }
    } else {
      this.innerElementSize = this.elementsPerThread[0];
      if (addBias) {
        this.variableNames.push("bias");
      }
      if (hasPreluActivationWeights) {
        this.variableNames.push("preluActivationWeights");
      }
    }
    this.sequentialAccessByThreads = sequentialAccessByThreads;
    this.addBias = addBias;
    this.activation = activation;
    this.hasPreluActivationWeights = hasPreluActivationWeights;
    this.tileAOuter = this.workgroupSize[1] * this.elementsPerThread[1];
    this.tileBOuter = this.workgroupSize[0] * this.elementsPerThread[0];
    this.tileInner = Math.max(this.workgroupSize[0] * this.innerElementSize, this.workgroupSize[1]);
    this.fitAOuter = dimAOuter % this.tileAOuter === 0;
    this.fitBOuter = dimBOuter % this.tileBOuter === 0;
    this.fitInner = dimInner % this.tileInner === 0;
    this.shaderKey = `conv2DMM_${this.elementsPerThread}_${this.activation}}_${this.fitAOuter}_${this.fitBOuter}_${this.fitInner}_${this.isVec4}_${this.innerElementSize}_${this.isChannelsLast}_${this.sequentialAccessByThreads}`;
  }
  getUserCode() {
    const matMulSource = this.isVec4 ? makeMatMulPackedVec4Source(this.elementsPerThread, this.workgroupSize, !this.isChannelsLast, this.tileInner) : makeMatMulPackedSource(this.elementsPerThread, this.workgroupSize, !this.isChannelsLast, this.tileInner, false, null, this.sequentialAccessByThreads);
    const elementsSize = this.isVec4 ? [this.innerElementSize, 4, 4] : [1, 1, 1];
    const userCode = `
    ${conv2dCommonSnippet(this.isChannelsLast, this.fitAOuter, this.fitBOuter, this.fitInner, this.addBias, this.activation, this.hasPreluActivationWeights, elementsSize[0], elementsSize[1], elementsSize[2])}
    ${matMulSource}
  `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/conv2d_naive_webgpu.js
var Conv2DNaiveProgram = class {
  constructor(convInfo, addBias = false, activation = null, hasPreluActivationWeights = false) {
    this.variableNames = ["x", "W"];
    this.uniforms = "filterDims: vec2<i32>, pad: vec2<i32>, stride: vec2<i32>, dilation: vec2<i32>,";
    this.workgroupSize = [4, 4, 8];
    this.outputShape = convInfo.outShape;
    this.isChannelsLast = convInfo.dataFormat === "channelsLast";
    this.dispatchLayout = this.isChannelsLast ? { x: [2], y: [1], z: [0, 3] } : { x: [3], y: [2], z: [0, 1] };
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.addBias = addBias;
    this.activation = activation;
    this.hasPreluActivationWeights = hasPreluActivationWeights;
    if (addBias) {
      this.variableNames.push("bias");
    }
    if (hasPreluActivationWeights) {
      this.variableNames.push("preluActivationWeights");
    }
    this.shaderKey = `conv2dnaive_${this.activation}_${this.isChannelsLast}`;
  }
  getUserCode() {
    const userCode = `
       ${activationFnSnippet(this.activation, this.hasPreluActivationWeights, false, 4)}
       fn readInp(batch : i32, row : i32, col : i32, chan : i32) -> f32{
         let coords = vec4<i32>(batch, row, col, chan);
         if (coordsInBounds4D(coords, uniforms.xShape)) {
           return  getX(batch, row, col, chan);
         } else {
          return 0.0;
         }
       }
       fn readFilt(row : i32, col : i32, xChannel : i32, outChannel : i32) -> f32{
         let coords = vec4<i32>(row, col, xChannel, outChannel);
         if(coordsInBounds4D(coords, uniforms.wShape)) {
           return getW(row, col, xChannel, outChannel);
          } else {
            return 0.0;
          }
       }
       fn writeResult(batch : i32, row : i32, col : i32, chan : i32, valueIn : f32) {
         let coords = ${this.isChannelsLast ? `vec4<i32>(batch, row, col, chan);` : `vec4<i32>(batch, chan, row, col);`}
         if (coordsInBounds4D(coords, uniforms.outShape)) {
           var value = valueIn;
           ${biasActivationSnippet(this.addBias, this.activation)}
           setOutputAtCoords(coords.x, coords.y, coords.z, coords.w, value);
         }
       }
       ${getMainHeaderString("index")} {
         let coords = getOutputCoords();
         let batch = coords[0];
         let outChannel = ${this.isChannelsLast ? `coords[3];` : `coords[1];`}
         let outRow = ${this.isChannelsLast ? `coords[1];` : `coords[2];`}
         let outCol = ${this.isChannelsLast ? `coords[2];` : `coords[3];`}
         var acc : f32 = 0.0;
         for (var row = 0; row < uniforms.filterDims[0]; row = row + 1) {
           for (var col = 0; col < uniforms.filterDims[1]; col = col + 1) {
             let xRow = outRow * uniforms.stride[0] + uniforms.dilation[0] * row - uniforms.pad[0];
             let xCol = outCol * uniforms.stride[1] + uniforms.dilation[1] * col - uniforms.pad[1];
             for (var xChannel = 0; xChannel < ${this.isChannelsLast ? `uniforms.xShape[3];` : `uniforms.xShape[1];`} xChannel = xChannel + 1) {
               ${this.isChannelsLast ? `let v = readInp(batch, xRow, xCol, xChannel);` : `let v = readInp(batch, xChannel, xRow, xCol);`}
               let f = readFilt(row, col, xChannel, outChannel);
               acc = acc + v * f;
             }
           }
         }
         writeResult(batch, outRow, outCol, outChannel, acc);
       }
     `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/im2col_webgpu.js
var Im2ColProgram = class {
  constructor(outputShape, isChannelsLast) {
    this.variableNames = ["x"];
    this.uniforms = `pad : vec2<i32>, stride : vec2<i32>, dilation : vec2<i32>, outWidth : i32, itemsPerBlockRow : i32,
       inChannels : i32,`;
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = outputShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.isChannelsLast = isChannelsLast;
    this.shaderKey = `im2col_${this.isChannelsLast}`;
  }
  getUserCode() {
    const rowDim = this.isChannelsLast ? 1 : 2;
    const colDim = this.isChannelsLast ? 2 : 3;
    const row = this.isChannelsLast ? "coords[1]" : "coords[2]";
    const col = this.isChannelsLast ? "coords[2]" : "coords[1]";
    const getXSnippet = this.isChannelsLast ? "getX(batch, xRow, xCol, ch)" : "getX(batch, ch, xRow, xCol)";
    const userCode = `
    ${getMainHeaderString("index")} {
      let coords = getCoordsFromIndex(index);
      if(index < uniforms.size) {
        let batch = coords[0];
        let row = ${row};
        let col = ${col};
        let offsetY = (row / uniforms.outWidth) * uniforms.stride[0] - uniforms.pad[0];
        let xRow = offsetY + uniforms.dilation[0] * (col / uniforms.itemsPerBlockRow);
        var value = 0.0;
        if(xRow < uniforms.xShape[${rowDim}] && xRow >= 0) {
          let offsetX = (row % uniforms.outWidth) * uniforms.stride[1] -
              uniforms.pad[1];
          let xCol = offsetX + uniforms.dilation[1] * ((col %
              uniforms.itemsPerBlockRow) / uniforms.inChannels);
          let ch = col % uniforms.inChannels;
          if(xCol < uniforms.xShape[${colDim}] && xCol >= 0) {
            value = ${getXSnippet};
          }
        }
        setOutputAtIndex(index, value);
      }
    }
   `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Conv2D_impl.js
function getShapeForBatchMatMul(shape, isChannelsLast) {
  const length = shape.length;
  if (length >= 3) {
    return isChannelsLast ? [
      ...shape.slice(0, -3),
      shape[length - 3] * shape[length - 2],
      shape[length - 1]
      /* channel */
    ] : [
      ...shape.slice(0, -3),
      shape[length - 3],
      shape[length - 2] * shape[length - 1]
      /* height * width */
    ];
  } else if (!isChannelsLast && length === 1 && shape[0] > 1) {
    return [shape[0], 1];
  } else {
    return null;
  }
}
function conv2dByMatMul({ x, filter, convInfo, backend, bias = null, preluActivationWeights = null, leakyreluAlpha = 0, activation = null }) {
  const isChannelsLast = convInfo.dataFormat === "channelsLast";
  const transposeA = isChannelsLast ? false : true;
  const transposeB = false;
  const sameSize = isChannelsLast && convInfo.filterHeight === convInfo.inHeight && convInfo.filterWidth === convInfo.inWidth && convInfo.padInfo.type === "VALID";
  const intermediates = [];
  let xReshaped;
  let filterReshaped;
  if (sameSize) {
    const sharedDim = convInfo.inHeight * convInfo.inWidth * convInfo.inChannels;
    xReshaped = reshape({
      inputs: { x },
      backend,
      attrs: { shape: [1, convInfo.batchSize, sharedDim] }
    });
    filterReshaped = reshape({
      inputs: { x: filter },
      backend,
      attrs: { shape: [1, sharedDim, convInfo.outChannels] }
    });
  } else {
    xReshaped = reshape({
      inputs: { x },
      backend,
      attrs: {
        shape: isChannelsLast ? [
          convInfo.batchSize,
          convInfo.inHeight * convInfo.inWidth,
          convInfo.inChannels
        ] : [
          convInfo.batchSize,
          convInfo.inChannels,
          convInfo.inHeight * convInfo.inWidth
        ]
      }
    });
    filterReshaped = reshape({
      inputs: { x: filter },
      backend,
      attrs: { shape: [1, convInfo.inChannels, convInfo.outChannels] }
    });
  }
  intermediates.push(xReshaped);
  intermediates.push(filterReshaped);
  if (preluActivationWeights != null) {
    const targetShape = getShapeForBatchMatMul(preluActivationWeights.shape, isChannelsLast);
    if (targetShape != null) {
      preluActivationWeights = reshape({
        inputs: { x: preluActivationWeights },
        backend,
        attrs: { shape: targetShape }
      });
      intermediates.push(preluActivationWeights);
    }
  }
  if (bias != null) {
    const targetShape = getShapeForBatchMatMul(bias.shape, isChannelsLast);
    if (targetShape != null) {
      bias = reshape({ inputs: { x: bias }, backend, attrs: { shape: targetShape } });
      intermediates.push(bias);
    }
  }
  const result = batchMatMulImpl({
    a: isChannelsLast ? xReshaped : filterReshaped,
    b: isChannelsLast ? filterReshaped : xReshaped,
    transposeA,
    transposeB,
    backend,
    bias,
    activation,
    preluActivationWeights,
    leakyreluAlpha
  });
  const out = reshape({ inputs: { x: result }, backend, attrs: { shape: convInfo.outShape } });
  intermediates.push(result);
  for (const i of intermediates) {
    backend.disposeData(i.dataId);
  }
  return out;
}
function conv2dWithIm2Col({ x, filter, convInfo, backend, bias = null, preluActivationWeights = null, leakyreluAlpha = 0, activation = null }) {
  const { filterWidth, filterHeight, inChannels, strideWidth, strideHeight, padInfo, outWidth, outHeight, dilationWidth, dilationHeight, dataFormat } = convInfo;
  const isChannelsLast = dataFormat === "channelsLast";
  const sharedDim = filterWidth * filterHeight * inChannels;
  const numCols = outHeight * outWidth;
  const x2ColShape = isChannelsLast ? [convInfo.batchSize, numCols, sharedDim] : [convInfo.batchSize, sharedDim, numCols];
  const im2ColProgram = new Im2ColProgram(x2ColShape, isChannelsLast);
  const dimensions = [
    { type: "int32", data: [padInfo.top, padInfo.left] },
    { type: "int32", data: [strideHeight, strideWidth] },
    { type: "int32", data: [dilationHeight, dilationWidth] },
    { type: "int32", data: [outWidth] },
    { type: "int32", data: [inChannels * filterWidth] },
    { type: "int32", data: [inChannels] }
  ];
  const x2Col = backend.runWebGPUProgram(im2ColProgram, [x], x.dtype, dimensions);
  const intermediates = [];
  intermediates.push(x2Col);
  const filterReshaped = reshape({ inputs: { x: filter }, backend, attrs: { shape: [1, sharedDim, -1] } });
  intermediates.push(filterReshaped);
  if (preluActivationWeights != null) {
    const targetShape = getShapeForBatchMatMul(preluActivationWeights.shape, isChannelsLast);
    if (targetShape != null) {
      preluActivationWeights = reshape({
        inputs: { x: preluActivationWeights },
        backend,
        attrs: { shape: targetShape }
      });
      intermediates.push(preluActivationWeights);
    }
  }
  if (bias != null) {
    const targetShape = getShapeForBatchMatMul(bias.shape, isChannelsLast);
    if (targetShape != null) {
      bias = reshape({ inputs: { x: bias }, backend, attrs: { shape: targetShape } });
      intermediates.push(bias);
    }
  }
  const transposeA = isChannelsLast ? false : true;
  const transposeB = false;
  const result = batchMatMulImpl({
    a: isChannelsLast ? x2Col : filterReshaped,
    b: isChannelsLast ? filterReshaped : x2Col,
    transposeA,
    transposeB,
    backend,
    bias,
    activation,
    preluActivationWeights,
    leakyreluAlpha
  });
  const out = reshape({ inputs: { x: result }, backend, attrs: { shape: convInfo.outShape } });
  intermediates.push(result);
  for (const i of intermediates) {
    backend.disposeData(i.dataId);
  }
  return out;
}
function conv2DImpl({ x, filter, convInfo, backend, bias = null, preluActivationWeights = null, leakyreluAlpha = 0, activation = null }) {
  const hasBias = bias != null;
  const hasPreluActivationWeights = preluActivationWeights != null;
  const isChannelsLast = convInfo.dataFormat === "channelsLast";
  const sameSize = isChannelsLast && convInfo.filterHeight === convInfo.inHeight && convInfo.filterWidth === convInfo.inWidth && convInfo.padInfo.type === "VALID";
  const useNaiveConv2d = env().getBool("WEBGPU_USE_NAIVE_CONV2D_DEBUG");
  if (!useNaiveConv2d && (sameSize || convInfo.filterHeight === 1 && convInfo.filterWidth === 1 && convInfo.dilationHeight === 1 && convInfo.dilationWidth === 1 && convInfo.strideHeight === 1 && convInfo.strideWidth === 1 && (convInfo.padInfo.type === "SAME" || convInfo.padInfo.type === "VALID"))) {
    return conv2dByMatMul({
      x,
      filter,
      convInfo,
      backend,
      bias,
      activation,
      preluActivationWeights,
      leakyreluAlpha
    });
  }
  const thresholdFlagValue = env().getNumber("WEBGPU_THRESHOLD_TO_INCREASE_WORKGROUPS_FOR_MATMUL");
  const thresholdToIncreaseWorkgroups = thresholdFlagValue > 0 ? thresholdFlagValue : backend.thresholdToIncreaseWorkgroups;
  const workgroupsBy32x32 = convInfo.batchSize * Math.ceil(convInfo.outHeight * convInfo.outWidth / 32) * Math.ceil(convInfo.outChannels / 32);
  if (env().getBool("WEBGPU_CONV_SEPARATE_IM2COL_SHADER") || workgroupsBy32x32 <= thresholdToIncreaseWorkgroups) {
    return conv2dWithIm2Col({
      x,
      filter,
      convInfo,
      backend,
      bias,
      preluActivationWeights,
      leakyreluAlpha,
      activation
    });
  }
  let program;
  const padInfo = [convInfo.padInfo.top, convInfo.padInfo.left];
  const dimensions = [
    { type: "int32", data: [convInfo.filterHeight, convInfo.filterWidth] },
    { type: "int32", data: [...padInfo] },
    { type: "int32", data: [convInfo.strideHeight, convInfo.strideWidth] },
    { type: "int32", data: [convInfo.dilationHeight, convInfo.dilationWidth] }
  ];
  if (useNaiveConv2d) {
    program = new Conv2DNaiveProgram(convInfo, hasBias, activation, hasPreluActivationWeights);
  } else {
    const dimAOuter = isChannelsLast ? convInfo.outHeight * convInfo.outWidth : convInfo.outChannels;
    const dimBOuter = isChannelsLast ? convInfo.outChannels : convInfo.outHeight * convInfo.outWidth;
    const dimInner = convInfo.filterHeight * convInfo.filterWidth * convInfo.inChannels;
    dimensions.push({ type: "int32", data: [dimAOuter] }, { type: "int32", data: [dimBOuter] }, { type: "int32", data: [dimInner] });
    const sequentialAccessByThreads = backend.adapterInfo.isIntel();
    program = new Conv2DMMProgram(convInfo, dimAOuter, dimBOuter, dimInner, hasBias, activation, hasPreluActivationWeights, sequentialAccessByThreads);
  }
  const intermediates = [];
  const inputVar = [x, filter];
  if (hasBias) {
    if (!isChannelsLast && bias.shape.length === 1) {
      bias = reshape({ inputs: { x: bias }, backend, attrs: { shape: [bias.shape[0], 1, 1] } });
      intermediates.push(bias);
    }
    inputVar.push(bias);
  }
  if (hasPreluActivationWeights) {
    if (!isChannelsLast && preluActivationWeights.shape.length === 1) {
      preluActivationWeights = reshape({
        inputs: { x: preluActivationWeights },
        backend,
        attrs: { shape: [preluActivationWeights.shape[0], 1, 1] }
      });
      intermediates.push(preluActivationWeights);
    }
    inputVar.push(preluActivationWeights);
  }
  if (activation === "leakyrelu") {
    dimensions.push({ type: "float32", data: [leakyreluAlpha] });
    program.uniforms += " alpha : f32,";
  }
  const out = backend.runWebGPUProgram(program, inputVar, x.dtype, dimensions);
  for (const i of intermediates) {
    backend.disposeData(i.dataId);
  }
  return out;
}

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Conv2D.js
function conv2d(args) {
  const { inputs, attrs, backend } = args;
  const { x, filter } = inputs;
  const { strides, pad, dataFormat, dilations, dimRoundingMode } = attrs;
  const $dataFormat = backend_util_exports.convertConv2DDataFormat(dataFormat);
  const convInfo = backend_util_exports.computeConv2DInfo(x.shape, filter.shape, strides, dilations, pad, dimRoundingMode, false, $dataFormat);
  return conv2DImpl({ x, filter, convInfo, backend });
}
var conv2DConfig = {
  kernelName: Conv2D,
  backendName: "webgpu",
  kernelFunc: conv2d
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/conv_backprop_webgpu.js
var Conv2DDerInputProgram = class {
  constructor(convInfo) {
    this.variableNames = ["dy", "W"];
    this.uniforms = "filterDims : vec2<i32>, pads : vec2<i32>, stride : vec2<i32>, outBackprop : vec4<i32>,";
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = convInfo.inShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.isChannelsLast = convInfo.dataFormat === "channelsLast";
    this.shaderKey = `conv2DDerInput_${this.isChannelsLast}`;
  }
  getUserCode() {
    const rowDim = this.isChannelsLast ? 1 : 2;
    const colDim = this.isChannelsLast ? 2 : 3;
    const channelDim = this.isChannelsLast ? 3 : 1;
    return `
    ${getMainHeaderString("index")} {
      if(index < uniforms.size) {
        let coords = getCoordsFromIndex(index);
        let batch = coords[0];
        let d1 = coords[${channelDim}];

        let dyCorner = vec2<i32>(coords[${rowDim}], coords[${colDim}]) - uniforms.pads;
        let dyRCorner = dyCorner.x;
        let dyCCorner = dyCorner.y;

        // Convolve dy(?, ?, d2) with w(:, :, d1, d2) to compute dx(xR, xC, d1).
        // ? = to be determined. : = across all values in that axis.
        var dotProd = 0.0;
        for (var wR = 0; wR < uniforms.filterDims.x; wR = wR + 1) {
          let dyR = (f32(dyRCorner) + f32(wR)) / f32(uniforms.stride.x);
          let wRPerm = uniforms.filterDims.x - 1 - wR;
          if (dyR < 0.0 || dyR >= f32(uniforms.outBackprop[1]) || fract(dyR) > 0.0 ||
              wRPerm < 0) {
            continue;
          }
          let idyR = i32(dyR);

          for (var wC = 0; wC < uniforms.filterDims.y; wC = wC + 1) {
            let dyC = (f32(dyCCorner) + f32(wC)) / f32(uniforms.stride.y);
            let wCPerm = uniforms.filterDims.y - 1 - wC;
            if (dyC < 0.0 || dyC >= f32(uniforms.outBackprop[2]) ||
                fract(dyC) > 0.0 || wCPerm < 0) {
              continue;
            }
            let idyC = i32(dyC);

            for (var d2 = 0; d2 < uniforms.outBackprop[3]; d2 = d2 + 1) {
              if (${this.isChannelsLast}) {
                let xValue = getDy(batch, idyR, idyC, d2);
                let wValue = getW(wRPerm, wCPerm, d1, d2);
                dotProd = dotProd + xValue * wValue;
              } else {
                let xValue = getDy(batch, d2, idyR, idyC);
                let wValue = getW(wRPerm, wCPerm, d1, d2);
                dotProd = dotProd + xValue * wValue;
              }

            }
          }
        }
        setOutputAtIndex(index, dotProd);
      }
    }
  `;
  }
};
var Conv2DDerFilterProgram = class {
  constructor(convInfo) {
    this.variableNames = ["x", "dy"];
    this.uniforms = "pad : vec2<i32>, stride : vec2<i32>, batchSize : i32, outHeight : i32, outWidth : i32, inHeight : i32, inWidth : i32,";
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = convInfo.filterShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.isChannelsLast = convInfo.dataFormat === "channelsLast";
    this.shaderKey = `conv2DDerFilter_${this.isChannelsLast}`;
  }
  getUserCode() {
    return `
    ${getMainHeaderString("index")} {
      if(index < uniforms.size) {
        let coords = getCoordsFromIndex(index);
        let wR = coords[0];
        let wC = coords[1];
        let d1 = coords[2];
        let d2 = coords[3];

        // Convolve x(?, ?, d1) with dy(:, :, d2) to get dw(wR, wC, d1, d2).
        // ? = to be determined. : = across all values in that axis.
        var dotProd = 0.0;
        for (var b = 0; b < uniforms.batchSize; b = b + 1) {
          for (var yR = 0; yR < uniforms.outHeight; yR = yR + 1) {
            let xR = wR + yR * uniforms.stride[0] - uniforms.pad[0];
            if (xR < 0 || xR >= uniforms.inHeight) {
              continue;
            }

            for (var yC = 0; yC < uniforms.outWidth; yC = yC + 1) {
              let xC = wC + yC * uniforms.stride[1] - uniforms.pad[1];

              if (xC < 0 || xC >= uniforms.inWidth) {
                continue;
              }

              if (${this.isChannelsLast}) {
                let dyValue = getDy(b, yR, yC, d2);
                let xValue = getX(b, xR, xC, d1);
                dotProd = dotProd + xValue * dyValue;
              } else {
                let dyValue = getDy(b, d2, yR, yC);
                let xValue = getX(b, d1, xR, xC);
                dotProd = dotProd + xValue * dyValue;
              }
            }
          }
        }
        setOutputAtIndex(index, dotProd);
      }
    }
  `;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Conv2DBackpropFilter.js
function conv2DBackpropFilter(args) {
  const { inputs, backend, attrs } = args;
  const { x, dy } = inputs;
  const { strides, pad, dataFormat, dimRoundingMode, filterShape } = attrs;
  const $dataFormat = backend_util_exports.convertConv2DDataFormat(dataFormat);
  const convInfo = backend_util_exports.computeConv2DInfo(x.shape, filterShape, strides, 1, pad, dimRoundingMode, false, $dataFormat);
  const program = new Conv2DDerFilterProgram(convInfo);
  const uniformData = [
    { type: "int32", data: [convInfo.padInfo.top, convInfo.padInfo.left] },
    { type: "int32", data: [convInfo.strideHeight, convInfo.strideWidth] },
    { type: "int32", data: [convInfo.batchSize] },
    { type: "int32", data: [convInfo.outHeight] },
    { type: "int32", data: [convInfo.outWidth] },
    { type: "int32", data: [convInfo.inHeight] },
    { type: "int32", data: [convInfo.inWidth] }
  ];
  return backend.runWebGPUProgram(program, [x, dy], x.dtype, uniformData);
}
var conv2DBackpropFilterConfig = {
  kernelName: Conv2DBackpropFilter,
  backendName: "webgpu",
  kernelFunc: conv2DBackpropFilter
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/conv_backprop_mm_webgpu.js
function conv2dTransposeCommonSnippet(innerElementSize = 4) {
  const getWSnippet = (innerElementSize2) => {
    switch (innerElementSize2) {
      case 1:
        return "return W[getIndexFromCoords4D(coord, uniforms.wShape)];";
      case 4:
        return `
            let coord1 = vec4<i32>(coordX, coordY, col + 1, rowInner);
            let coord2 = vec4<i32>(coordX, coordY, col + 2, rowInner);
            let coord3 = vec4<i32>(coordX, coordY, col + 3, rowInner);
            let v0 = W[getIndexFromCoords4D(coord, uniforms.wShape)];
            let v1 = W[getIndexFromCoords4D(coord1, uniforms.wShape)];
            let v2 = W[getIndexFromCoords4D(coord2, uniforms.wShape)];
            let v3 = W[getIndexFromCoords4D(coord3, uniforms.wShape)];
            return vec4<f32>(v0, v1, v2, v3);
            `;
      default:
        throw new Error(`innerElementSize ${innerElementSize2} is not supported.`);
    }
  };
  const readASnippet = `
      let outRow = row / uniforms.outShape[2];
      let outCol = row % uniforms.outShape[2];

      let WRow = col / (uniforms.filterDims[1] * uniforms.outBackprop[3]);
      let WCol = col / uniforms.outBackprop[3] % uniforms.filterDims[1];
      let xR = f32(outRow - uniforms.pads[0] + WRow) / f32(uniforms.stride[0]);
      let xC = f32(outCol - uniforms.pads[1] + WCol) / f32(uniforms.stride[1]);
      if (xR < 0.0 || xR >= f32(uniforms.outBackprop[1]) || fract(xR) > 0.0) {
        return ${typeSnippet(innerElementSize)}(0.0);
      }
      if (xC < 0.0 || xC >= f32(uniforms.outBackprop[2]) || fract(xC) > 0.0) {
        return ${typeSnippet(innerElementSize)}(0.0);
      }
      let coord = vec4<i32>(
          batch,
          i32(xR),
          i32(xC),
          col % uniforms.outBackprop[3]);
      return x[getIndexFromCoords4D(coord, uniforms.xShape)/${innerElementSize}];`;
  const sampleA = `if (row < uniforms.dimAOuter && col < uniforms.dimInner) {
        ${readASnippet}
      }
      return ${typeSnippet(innerElementSize)}(0.0);`;
  const userCode = `
  fn mm_readA(batch: i32, row : i32, colIn : i32) -> ${typeSnippet(innerElementSize)} {
    let col = colIn * ${innerElementSize};
    ${sampleA}
  }

  fn mm_readB(batch: i32, row : i32, colIn : i32) -> ${typeSnippet(innerElementSize)} {
    let col = colIn * ${innerElementSize};
    let coordX = uniforms.filterDims.x - 1 -
        row / (uniforms.filterDims[1] * uniforms.outBackprop[3]);
    let coordY = uniforms.filterDims.y - 1 -
        (row / uniforms.outBackprop[3]) % uniforms.filterDims[1];
    if (row < uniforms.dimInner && col < uniforms.dimBOuter &&
        coordX >= 0 && coordY >= 0) {
      let rowInner = row % uniforms.outBackprop[3];
      let coord = vec4<i32>(coordX, coordY, col, rowInner);
      ${getWSnippet(innerElementSize)}
    }
    return ${typeSnippet(innerElementSize)}(0.0);
  }

  fn mm_write(batch: i32, row : i32, colIn : i32, valueInput : ${typeSnippet(innerElementSize)}) {
    let col = colIn * ${innerElementSize};
    if (row < uniforms.dimAOuter && (col + ${innerElementSize - 1}) < uniforms.dimBOuter) {
      var value = valueInput;
      let outCoord = vec4<i32>(
          batch,
          row / uniforms.outShape[2],
          row % uniforms.outShape[2],
          col);
      result[getIndexFromCoords4D(outCoord, uniforms.outShape)/${innerElementSize}] = value;
    }
  }`;
  return userCode;
}
var Conv2DDerInputMMProgram = class {
  constructor(convInfo) {
    this.variableNames = ["x", "W"];
    this.uniforms = "filterDims : vec2<i32>, pads : vec2<i32>, stride : vec2<i32>, outBackprop : vec4<i32>, dimAOuter : i32, dimBOuter : i32, dimInner : i32,";
    this.outputShape = convInfo.inShape;
    util_exports.assert(convInfo.dataFormat === "channelsLast", () => "TODO: NCHW is unimplemented");
    this.isVec4 = convInfo.inChannels % 4 === 0 && convInfo.outChannels % 4 === 0;
    this.dispatchLayout = { x: [3], y: [1, 2], z: [0] };
    this.workgroupSize = computeWorkgroupSizeForConv2d(this.dispatchLayout, this.outputShape, this.isVec4);
    this.elementsPerThread = computeWorkPerThreadForConv2d(this.dispatchLayout, this.outputShape, this.isVec4);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize, this.elementsPerThread);
    if (this.isVec4) {
      this.variableTypes = ["vec4<f32>", "f32"];
    }
    this.shaderKey = `conv2DDerInputMM_${this.isVec4}_${this.elementsPerThread}`;
  }
  getUserCode() {
    const matMulSource = this.isVec4 ? makeMatMulPackedVec4Source(this.elementsPerThread, this.workgroupSize) : makeMatMulPackedSource(this.elementsPerThread, this.workgroupSize);
    const userCode = `
    ${conv2dTransposeCommonSnippet(this.isVec4 ? 4 : 1)}
    ${matMulSource}
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Conv2DBackpropInput.js
function conv2DBackpropInput(args) {
  const { inputs, backend, attrs } = args;
  const { dy, filter } = inputs;
  const { inputShape, strides, pad, dataFormat, dimRoundingMode } = attrs;
  const $dataFormat = backend_util_exports.convertConv2DDataFormat(dataFormat);
  const convInfo = backend_util_exports.computeConv2DInfo(inputShape, filter.shape, strides, 1, pad, dimRoundingMode, false, $dataFormat);
  const dimensions = [
    { type: "int32", data: [convInfo.filterHeight, convInfo.filterWidth] },
    {
      type: "int32",
      data: [
        convInfo.filterHeight - 1 - convInfo.padInfo.top,
        convInfo.filterWidth - 1 - convInfo.padInfo.left
      ]
    },
    { type: "int32", data: [convInfo.strideHeight, convInfo.strideWidth] },
    {
      type: "int32",
      data: [
        convInfo.batchSize,
        convInfo.outHeight,
        convInfo.outWidth,
        convInfo.outChannels
      ]
    }
  ];
  let program;
  if (env().getBool("WEBGPU_USE_NAIVE_CONV2D_TRANSPOSE") || convInfo.filterHeight <= 2 && convInfo.filterWidth <= 2 && convInfo.outChannels <= 16 && convInfo.inChannels === 1) {
    program = new Conv2DDerInputProgram(convInfo);
  } else {
    program = new Conv2DDerInputMMProgram(convInfo);
    const dimAOuter = convInfo.inHeight * convInfo.inWidth;
    const dimBOuter = convInfo.inChannels;
    const dimInner = convInfo.filterHeight * convInfo.filterWidth * convInfo.outChannels;
    dimensions.push({ type: "uint32", data: [dimAOuter] }, { type: "uint32", data: [dimBOuter] }, { type: "uint32", data: [dimInner] });
  }
  return backend.runWebGPUProgram(program, [dy, filter], "float32", dimensions);
}
var conv2DBackpropInputConfig = {
  kernelName: Conv2DBackpropInput,
  backendName: "webgpu",
  kernelFunc: conv2DBackpropInput
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Cos.js
var cos = unaryKernelFunc({ opType: UnaryOpType.COS });
var cosConfig = {
  kernelName: Cos,
  backendName: "webgpu",
  kernelFunc: cos
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Cosh.js
var cosh = unaryKernelFunc({ opType: UnaryOpType.COSH });
var coshConfig = {
  kernelName: Cosh,
  backendName: "webgpu",
  kernelFunc: cosh
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/crop_and_resize_webgpu.js
var CropAndResizeProgram = class {
  constructor(channnel, boxShape, cropSize, method) {
    this.variableNames = ["Image", "Boxes", "BoxInd"];
    this.uniforms = "extrapolationValue : f32,";
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    const [numBoxes] = boxShape;
    this.outputShape = [numBoxes, cropSize[0], cropSize[1], channnel];
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.methodId = method === "bilinear" ? 1 : 0;
    this.cropHeightBiggerThan1 = this.outputShape[1] > 1;
    this.cropWidthBiggerThan1 = this.outputShape[2] > 1;
    this.shaderKey = `cropAndResize_${this.methodId}_${this.cropHeightBiggerThan1}_${this.cropWidthBiggerThan1}`;
  }
  getUserCode() {
    const [inputHeightFloat, inputWidthFloat] = [`f32(uniforms.imageShape[1] - 1)`, `f32(uniforms.imageShape[2] - 1)`];
    const [heightRatio, heightScale, inY] = this.cropHeightBiggerThan1 ? [
      `(${inputHeightFloat} / f32(uniforms.outShape[1] - 1))`,
      "(y2-y1) * height_ratio",
      `y1*${inputHeightFloat} + f32(y)*(height_scale)`
    ] : [
      "0.0",
      "0.0",
      `0.5 * (y1+y2) * ${inputHeightFloat}`
    ];
    const [widthRatio, widthScale, inX] = this.cropWidthBiggerThan1 ? [
      `(${inputWidthFloat} / f32(uniforms.outShape[2] - 1))`,
      "(x2-x1) * width_ratio",
      `x1*${inputWidthFloat} + f32(x)*(width_scale)`
    ] : [
      "0.0",
      "0.0",
      `0.5 * (x1+x2) * ${inputWidthFloat}`
    ];
    const userCode = `
    ${getMainHeaderString("index")} {
      if (index < uniforms.size) {
        let coords = getCoordsFromIndex(index);
        let height_ratio = f32(${heightRatio});
        let width_ratio = f32(${widthRatio});
        let b = coords[0];
        let y = coords[1];
        let x = coords[2];
        let d = coords[3];
        // get box vals
        let y1 = getBoxes(b, 0);
        let x1 = getBoxes(b, 1);
        let y2 = getBoxes(b, 2);
        let x2 = getBoxes(b, 3);
        // get image in batch index
        let bInd = i32(round(getBoxInd(b)));
        if(bInd < 0 || bInd >= uniforms.outShape[0]) {
          return;
        }
        let height_scale = ${heightScale};
        let width_scale = ${widthScale};
        let in_y = ${inY};
        if( in_y < 0.0 || in_y > ${inputHeightFloat} ) {
          setOutputAtIndex(index, uniforms.extrapolationValue);
          return;
        }
        let in_x = ${inX};
        if( in_x < 0.0 || in_x > ${inputWidthFloat} ) {
          setOutputAtIndex(index, uniforms.extrapolationValue);
          return;
        }
        let sourceFracIndexCR = vec2<f32>(in_x,in_y);
        if(${this.methodId} == 1) {
          // Compute the four integer indices.
          let sourceFloorCR = vec2<i32>(sourceFracIndexCR);
          let sourceCeilCR = vec2<i32>(ceil(sourceFracIndexCR));
          let topLeft = getImage(bInd, sourceFloorCR.y, sourceFloorCR.x, d);
          let bottomLeft = getImage(bInd, sourceCeilCR.y, sourceFloorCR.x, d);
          let topRight = getImage(bInd, sourceFloorCR.y, sourceCeilCR.x, d);
          let bottomRight = getImage(bInd, sourceCeilCR.y, sourceCeilCR.x, d);
          let fracCR = sourceFracIndexCR - vec2<f32>(sourceFloorCR);
          let top = topLeft + (topRight - topLeft) * fracCR.x;
          let bottom = bottomLeft + (bottomRight - bottomLeft) * fracCR.x;
          let newValue = top + (bottom - top) * fracCR.y;
          setOutputAtIndex(index, newValue);
        } else {
          // Compute the coordinators of nearest neighbor point.
          let sourceNearestCR = vec2<i32>(floor(
            sourceFracIndexCR + vec2<f32>(0.5,0.5)));
          let newValue = getImage(
            bInd, sourceNearestCR.y, sourceNearestCR.x, d);
          setOutputAtIndex(index, newValue);
        }
      }
    }
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/CropAndResize.js
var cropAndResize = (args) => {
  const { inputs, backend, attrs } = args;
  const { image, boxes, boxInd } = inputs;
  const { cropSize, method, extrapolationValue } = attrs;
  const program = new CropAndResizeProgram(image.shape[3], boxes.shape, cropSize, method);
  const uniformData = [{ type: "float32", data: [extrapolationValue] }];
  return backend.runWebGPUProgram(program, [image, boxes, boxInd], "float32", uniformData);
};
var cropAndResizeConfig = {
  kernelName: CropAndResize,
  backendName: "webgpu",
  kernelFunc: cropAndResize
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/cum_webgpu.js
var CumOpType;
(function(CumOpType2) {
  CumOpType2["Prod"] = "*";
  CumOpType2["Sum"] = "+";
})(CumOpType || (CumOpType = {}));
var CumProgram = class {
  constructor(op, shape, exclusive, reverse2) {
    this.variableNames = ["x"];
    this.uniforms = "index : f32,";
    this.size = true;
    this.workgroupSize = [128, 1, 1];
    this.outputShape = shape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.exclusive = exclusive;
    this.reverse = reverse2;
    this.op = op;
    this.shaderKey = `cum_${this.op}_${this.exclusive}_${this.reverse}`;
  }
  getUserCode() {
    const rank = this.outputShape.length;
    const initVal = this.op === CumOpType.Prod ? "1.0" : "0.0";
    const val = this.exclusive ? initVal : `getX(${getCoords2(rank, "coords", this.op)})`;
    const length = this.outputShape[this.outputShape.length - 1];
    let condition = "";
    let idxString = "";
    if (this.exclusive) {
      condition = this.reverse ? `end != ${length - 1}` : "end != 0";
      idxString = this.reverse ? "end + 1" : "end - 1";
    } else {
      condition = this.reverse ? `end + pow2 < ${length}` : "end >= pow2";
      idxString = this.reverse ? "end + pow2" : "end - pow2";
    }
    return `
      ${getMainHeaderString("index")} {
       if (index < uniforms.size) {
         var coords = getCoordsFromIndex(index);

         let end = ${getFinalCoord(rank, "coords", this.op)};
         var val = ${val};
         let pow2 = i32(pow(2.0, uniforms.index));
         if (${condition}) {
           let idx = ${idxString};
           ${getFinalCoord(rank, "coords", this.op)} = idx;
           val ${this.op}= getX(${getCoords2(rank, "coords", this.op)});
         }
         setOutputAtIndex(index, val);
       }
      }
    `;
  }
};
function getCoords2(rank, name, op) {
  if (rank === 1) {
    return `${name}`;
  } else if (rank === 2) {
    return `${name}.x, ${name}.y`;
  } else if (rank === 3) {
    return `${name}.x, ${name}.y, ${name}.z`;
  } else if (rank === 4) {
    return `${name}.x, ${name}.y, ${name}.z, ${name}.w`;
  } else {
    throw Error(`Cumulative ${op} for rank ${rank} is not yet supported`);
  }
}
function getFinalCoord(rank, name, op) {
  if (rank === 1) {
    return `${name}`;
  } else if (rank === 2) {
    return `${name}.y`;
  } else if (rank === 3) {
    return `${name}.z`;
  } else if (rank === 4) {
    return `${name}.w`;
  } else {
    throw Error(`Cumulative ${op} for rank ${rank} is not yet supported`);
  }
}

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Cum_impl.js
function cumImpl(op, x, backend, axis, exclusive, reverse2) {
  const xRank = x.shape.length;
  const permutation = backend_util_exports.getAxesPermutation([axis], xRank);
  let permutedX = x;
  if (permutation != null) {
    permutedX = transpose({ inputs: { x }, backend, attrs: { perm: permutation } });
  }
  const permutedAxis = backend_util_exports.getInnerMostAxes(1, xRank)[0];
  if (permutedAxis !== xRank - 1) {
    throw new Error(`WebGPU cumprod shader expects an inner-most axis=${x.shape.length - 1} but got axis=${axis}`);
  }
  const size = permutedX.shape[permutedAxis];
  let result = identity({ inputs: { x: permutedX }, backend });
  for (let i = 0; i <= Math.ceil(Math.log2(size)) - 1; i++) {
    const program = new CumProgram(op, permutedX.shape, false, reverse2);
    const prevResult = result;
    const uniformData = [{ type: "float32", data: [i] }];
    result = backend.runWebGPUProgram(program, [result], result.dtype, uniformData);
    backend.disposeData(prevResult.dataId);
  }
  if (exclusive) {
    const program = new CumProgram(op, permutedX.shape, exclusive, reverse2);
    const prevResult = result;
    const uniformData = [{ type: "float32", data: [0] }];
    result = backend.runWebGPUProgram(program, [result], result.dtype, uniformData);
    backend.disposeData(prevResult.dataId);
  }
  if (permutation != null) {
    const reversePermutation = backend_util_exports.getUndoAxesPermutation(permutation);
    const reverseTransposedResult = transpose({ inputs: { x: result }, backend, attrs: { perm: reversePermutation } });
    backend.disposeData(result.dataId);
    backend.disposeData(permutedX.dataId);
    return reverseTransposedResult;
  }
  return result;
}

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Cumprod.js
function cumprod(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { axis, exclusive, reverse: reverse2 } = attrs;
  return cumImpl(CumOpType.Prod, x, backend, axis, exclusive, reverse2);
}
var cumprodConfig = {
  kernelName: Cumprod,
  backendName: "webgpu",
  kernelFunc: cumprod
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Cumsum.js
function cumsum(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { axis, exclusive, reverse: reverse2 } = attrs;
  return cumImpl(CumOpType.Sum, x, backend, axis, exclusive, reverse2);
}
var cumsumConfig = {
  kernelName: Cumsum,
  backendName: "webgpu",
  kernelFunc: cumsum
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/DenseBincount.js
function denseBincount(args) {
  const { inputs, backend, attrs } = args;
  const { x, weights } = inputs;
  const { size, binaryOutput } = attrs;
  const xRankOne = x.shape.length === 1;
  const weightsSize = util_exports.sizeFromShape(weights.shape);
  const hasWeights = weightsSize > 0;
  const dtype = weights.dtype;
  const xSize = xRankOne ? [x.shape[0]] : [x.shape[0], x.shape[1]];
  const outputSize = xRankOne ? [size] : [x.shape[0], size];
  const output = fill({ backend, attrs: { shape: outputSize, value: 0, dtype } });
  const program = new BincountProgram(xSize, hasWeights, binaryOutput);
  const uniformData = [{ type: "int32", data: [size] }];
  const bincountInputs = hasWeights ? [x, weights] : [x];
  const res = backend.runWebGPUProgram(program, bincountInputs, dtype, uniformData, output);
  return res;
}
var denseBincountConfig = {
  kernelName: DenseBincount,
  backendName: "webgpu",
  kernelFunc: denseBincount
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/depth_to_space_webgpu.js
var DepthToSpaceProgram = class {
  constructor(outputShape, dataFormat) {
    this.variableNames = ["x"];
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.uniforms = "blockSize : i32,";
    this.outputShape = outputShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.shaderKey = `depthToSpace_${dataFormat}`;
    this.dataFormat = dataFormat;
  }
  getUserCode() {
    const userCode = `
      ${getMainHeaderString("index")} {
        if (index < uniforms.size) {
          let coords = getCoordsFromIndex(index);
          let b = coords[0];
          let h = ${this.getHeightCoordString()};
          let w = ${this.getWidthCoordString()};
          let d = ${this.getDepthCoordString()};

          let in_h = h / uniforms.blockSize;
          let offset_h = h % uniforms.blockSize;
          let in_w = w / uniforms.blockSize;
          let offset_w = w % uniforms.blockSize;
          let offset_d = (offset_h * uniforms.blockSize + offset_w) *
            ${this.getOutputDepthSize()};
          let in_d = d + offset_d;

          let rlt = ${this.getInputSamplingString()};
          setOutputAtIndex(index, rlt);
        }
      }`;
    return userCode;
  }
  getHeightCoordString() {
    if (this.dataFormat === "NHWC") {
      return `coords[1]`;
    } else {
      return `coords[2]`;
    }
  }
  getWidthCoordString() {
    if (this.dataFormat === "NHWC") {
      return `coords[2]`;
    } else {
      return `coords[3]`;
    }
  }
  getDepthCoordString() {
    if (this.dataFormat === "NHWC") {
      return `coords[3]`;
    } else {
      return `coords[1]`;
    }
  }
  getOutputDepthSize() {
    if (this.dataFormat === "NHWC") {
      return `uniforms.outShape[3]`;
    } else {
      return `uniforms.outShape[1]`;
    }
  }
  getInputSamplingString() {
    if (this.dataFormat === "NHWC") {
      return `getX(b, in_h, in_w, in_d)`;
    } else {
      return `getX(b, in_d, in_h, in_w)`;
    }
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/DepthToSpace.js
function depthToSpace(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { blockSize, dataFormat } = attrs;
  const batchSize = x.shape[0];
  const inputHeight = dataFormat === "NHWC" ? x.shape[1] : x.shape[2];
  const inputWidth = dataFormat === "NHWC" ? x.shape[2] : x.shape[3];
  const inputDepth = dataFormat === "NHWC" ? x.shape[3] : x.shape[1];
  const outputHeight = inputHeight * blockSize;
  const outputWidth = inputWidth * blockSize;
  const outputDepth = inputDepth / (blockSize * blockSize);
  const outputShape = dataFormat === "NHWC" ? [batchSize, outputHeight, outputWidth, outputDepth] : [batchSize, outputDepth, outputHeight, outputWidth];
  const uniformData = [
    { type: "int32", data: [blockSize] }
  ];
  const program = new DepthToSpaceProgram(outputShape, dataFormat);
  return backend.runWebGPUProgram(program, [x], x.dtype, uniformData);
}
var depthToSpaceConfig = {
  kernelName: DepthToSpace,
  backendName: "webgpu",
  kernelFunc: depthToSpace
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/depthwise_conv2d_nchw_shared_webgpu.js
var DepthwiseConv2DNCHWSharedProgram = class {
  constructor(outputShape, filterHeight, filterWidth, addBias = false, activation = null, hasPreluActivation = false) {
    this.variableNames = ["x", "W"];
    this.uniforms = `pad : vec2<i32>, inDims : vec2<i32>,`;
    this.workgroupSize = [16, 16, 1];
    this.outputShape = outputShape;
    this.dispatchLayout = { x: [3], y: [2], z: [0, 1] };
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    if (addBias) {
      this.variableNames.push("bias");
    }
    if (hasPreluActivation) {
      this.variableNames.push("preluActivationWeights");
    }
    this.addBias = addBias;
    this.activation = activation;
    this.hasPreluActivation = hasPreluActivation;
    this.filterHeight = filterHeight;
    this.filterWidth = filterWidth;
    this.shaderKey = `depthwiseNCHW_${this.activation}_${this.filterHeight}_${this.filterWidth}`;
  }
  getUserCode() {
    const filterSize = this.filterWidth * this.filterHeight;
    const flatWorkgroupSize = this.workgroupSize[0] * this.workgroupSize[1] * this.workgroupSize[2];
    const tileAHeight = this.workgroupSize[1] + this.filterHeight - 1;
    const tileAWidth = this.workgroupSize[0] + this.filterWidth - 1;
    const userCode = `
      ${activationFnSnippet(this.activation, this.hasPreluActivation, false, 4)}

      var<workgroup> mm_Asub : array<array<f32, ${tileAWidth}>, ${tileAHeight}>;
      var<workgroup> mm_Bsub : array<array<f32, ${this.filterWidth}>, ${this.filterHeight}>;
      fn readX(batch : i32, channel : i32, row : i32, col : i32) -> f32 {
        var value = 0.0;
        if (row >=0 && row < uniforms.inDims[0] && col >=0 && col < uniforms.inDims[1])
        {
          value = getX(batch, channel, row, col);
        }
        return value;
      }

      ${getMainHeaderString()} {
        let coords = getOutputCoords();
        let batch = coords[0];
        let xRCCorner = vec2<i32>(coords.zw) - uniforms.pad;
        let channelMul = uniforms.wShape[3];
        let d1 = coords[1] / channelMul;
        let q = coords[1] % channelMul;

        let inputRowStart = xRCCorner.x;
        let inputColStart = xRCCorner.y;

        let localRow = i32(localId.y);
        let localCol = i32(localId.x);

        // Load one tile of X into local memory.
        for (var inputRow = localRow; inputRow < ${tileAHeight}; inputRow = inputRow + ${this.workgroupSize[1]}) {
          for (var inputCol = localCol; inputCol < ${tileAWidth}; inputCol = inputCol + ${this.workgroupSize[0]}) {
            let rowOffset = inputRow - localRow;
            let colOffset = inputCol - localCol;
            mm_Asub[inputRow][inputCol] = readX(batch, d1, inputRowStart + rowOffset, inputColStart + colOffset);
          }
        }

        // Load one tile of W into local memory.
        var wIndex = i32(localIndex);
        ${filterSize < flatWorkgroupSize ? `if (wIndex < ${filterSize})` : `for(; wIndex < ${filterSize}; wIndex = wIndex + ${flatWorkgroupSize})`}

        {
          let wRow = wIndex / ${this.filterWidth};
          let wCol = wIndex % ${this.filterWidth};
          mm_Bsub[wRow][wCol] = getW(wRow, wCol, d1, q);
        }

        workgroupBarrier();

        var value = 0.0;
        for (var wR = 0; wR < ${this.filterHeight}; wR = wR + 1) {
          for (var wC = 0; wC < ${this.filterWidth}; wC = wC + 1) {
            let xVal = mm_Asub[localRow + wR][localCol + wC];
            let wVal = mm_Bsub[wR][wC];
            value = fma(xVal, wVal, value);
          }
        }
        ${biasActivationSnippet(this.addBias, this.activation)}
        if (coordsInBounds4D(coords, uniforms.outShape)) {
          setOutputAtCoords(coords[0], coords[1], coords[2], coords[3], value);
        }
      }
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/depthwise_conv2d_vec4_webgpu.js
var DepthwiseConv2DVec4Program = class {
  constructor(convInfo, addBias = false, activation = null, hasPreluActivation = false) {
    this.variableNames = ["x", "W"];
    this.uniforms = "pad : vec2<i32>, inDims : vec2<i32>,";
    this.workgroupSize = [4, 4, 4];
    this.workPerThread = 4;
    this.isVec4 = true;
    this.outputShape = convInfo.outShape;
    this.dispatchLayout = { x: [3], y: [2], z: [0, 1] };
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize, [4, this.workPerThread, 1]);
    util_exports.assert(convInfo.dataFormat === "channelsLast", () => "TODO: NCHW is unimplemented");
    if (addBias) {
      this.variableNames.push("bias");
    }
    if (hasPreluActivation) {
      this.variableNames.push("preluActivationWeights");
    }
    this.convInfo = convInfo;
    this.addBias = addBias;
    this.activation = activation;
    this.hasPreluActivation = hasPreluActivation;
    this.shaderKey = `depthwiseVec4_${activation}_${this.convInfo.filterHeight}_${this.convInfo.filterWidth}_${this.convInfo.strideHeight}_${this.convInfo.strideWidth}_${this.workPerThread}`;
  }
  getUserCode() {
    const xNumber = (this.workPerThread - 1) * this.convInfo.strideWidth + this.convInfo.filterWidth;
    const strideHeight = this.convInfo.strideHeight;
    const strideWidth = this.convInfo.strideWidth;
    const userCode = `
      ${activationFnSnippet(this.activation, this.hasPreluActivation, true, 4)}
      fn readX(batch : i32, row : i32, col : i32, channel : i32) -> vec4<f32> {
        var value = vec4<f32>(0.0);
        if (col >=0 && col < uniforms.inDims[1]) {
          value = getX(batch, row, col, channel);
        }
        return value;
      }

      ${getMainHeaderString()} {
        let batch = i32(globalId.z) / uniforms.outShape[1];
        let r = i32(globalId.z) % uniforms.outShape[1];
        let c = i32(globalId.y) * ${this.workPerThread};
        let d1 = i32(globalId.x) * 4;
        let xRCCorner = vec2<i32>(r, c) * vec2<i32>(${strideHeight}, ${strideWidth}) - uniforms.pad;

        let xRCorner = xRCCorner.x;
        let xCCorner = xRCCorner.y;
        var xVals : array<vec4<f32>, ${xNumber}>;
        var dotProd : array<vec4<f32>, ${this.workPerThread}>;
        for (var i = 0; i < ${this.workPerThread}; i++) {
          dotProd[i] = vec4<f32>(0.0);
        }

        // Use constant instead of uniform can give better performance.
        for (var wR = 0; wR < ${this.convInfo.filterHeight}; wR = wR + 1) {
          let xR = xRCorner + wR;
          if (xR >=0 && xR < uniforms.inDims[0]) {
            for (var i = 0; i < ${xNumber}; i++) {
              xVals[i] = readX(batch, xR, xCCorner + i, d1);
            }
            for (var wC = 0; wC < ${this.convInfo.filterWidth}; wC = wC + 1) {
              let wValue = getW(wR, wC, d1, 0);
              for (var i = 0; i < ${this.workPerThread}; i++) {
                dotProd[i] = fma(xVals[i * ${strideWidth} + wC], wValue, dotProd[i]);
              }
            }
          }
        }

        for (var i = 0; i < ${this.workPerThread}; i = i + 1) {
          let coords = vec4<i32>(batch, r, c + i, d1);
          if (coordsInBounds4D(coords, uniforms.outShape)) {
            var value = dotProd[i];
            ${biasActivationSnippet(this.addBias, this.activation)}
            setOutputAtCoords(coords[0], coords[1], coords[2], coords[3], value);
          }
        }
      }
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/depthwise_conv2d_webgpu.js
var DepthwiseConv2DProgram = class {
  constructor(convInfo, addBias = false, activation = null, hasPreluActivation = false) {
    this.variableNames = ["x", "W"];
    this.uniforms = `pad : vec2<i32>, inDims : vec2<i32>, filterHeight : i32,
      filterWidth : i32, stride : vec2<i32>, dilation : vec2<i32>,`;
    this.workgroupSize = [256, 1, 1];
    this.size = true;
    this.outputShape = convInfo.outShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.isChannelsLast = convInfo.dataFormat === "channelsLast";
    if (addBias) {
      this.variableNames.push("bias");
    }
    if (hasPreluActivation) {
      this.variableNames.push("preluActivationWeights");
    }
    this.convInfo = convInfo;
    this.addBias = addBias;
    this.activation = activation;
    this.hasPreluActivation = hasPreluActivation;
    this.shaderKey = `depthwise_${this.activation}_${this.isChannelsLast}`;
  }
  getUserCode() {
    const getXSnippet = this.isChannelsLast ? "getX(batch, xR, xC, d1);" : "getX(batch, d1, xR, xC);";
    const userCode = `
      ${activationFnSnippet(this.activation, this.hasPreluActivation, false, 4)}

      ${getMainHeaderString("index")} {
        if (index < uniforms.size) {
          let coords = getOutputCoords();
          let batch = coords[0];
          let xRCCorner = vec2<i32>(coords.${this.isChannelsLast ? "yz" : "zw"}) * uniforms.stride - uniforms.pad;
          let d2 = coords[${this.isChannelsLast ? 3 : 1}];
          let channelMul = uniforms.wShape[3];
          let d1 = d2 / channelMul;
          let q = d2 % channelMul;

          let inputRowStart = xRCCorner.x;
          let inputColStart = xRCCorner.y;
          let inputRowEnd = inputRowStart + uniforms.filterHeight *
              uniforms.dilation[0];
          let inputColEnd = inputColStart + uniforms.filterWidth *
              uniforms.dilation[1];

          // Convolve x(?, ?, d1)|x(d1, ?, ?) with w(:, :, d1, q) to get
          // y(yR, yC, d2)|y(d2, yR, yC). ? = to be determined. : = across all
          // values in that axis. x(?, ?, d1) and y(yR, yC, d2) is for NHWC.
          // x(d1, ?, ?) and y(d2, yR, yC) is for NCHW.
          var value = 0.0;

          // Extract if checking out of for loop for performance.
          if (inputRowStart >= 0 && inputColStart >= 0 &&
            inputRowEnd < uniforms.inDims[0] &&
                inputColEnd < uniforms.inDims[1]) {
              for (var wR = 0; wR < uniforms.filterHeight; wR = wR + 1) {
                let xR = inputRowStart + wR * uniforms.dilation[0];

                for (var wC = 0; wC < uniforms.filterWidth; wC = wC + 1) {
                  let xC = inputColStart + wC * uniforms.dilation[1];

                  let xVal = ${getXSnippet};
                  let wVal = getW(wR, wC, d1, q);
                  value = value + xVal * wVal;
                }
              }
            } else {
              for (var wR = 0; wR < uniforms.filterHeight; wR = wR + 1) {
                let xR = inputRowStart + wR * uniforms.dilation[0];

                if (xR < 0 || xR >= uniforms.inDims[0]) {
                  continue;
                }

                for (var wC = 0; wC < uniforms.filterWidth; wC = wC + 1) {
                  let xC = inputColStart + wC * uniforms.dilation[1];

                  if (xC < 0 || xC >= uniforms.inDims[1]) {
                    continue;
                  }

                  let xVal = ${getXSnippet};
                  let wVal = getW(wR, wC, d1, q);
                  value = value + xVal * wVal;
                }
              }
            }
            ${biasActivationSnippet(this.addBias, this.activation)}
          setOutputAtCoords(coords[0], coords[1], coords[2], coords[3], value);
        }
      }
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/DepthwiseConv2dNative.js
function depthwiseConv2dNative(args) {
  const { inputs, backend, attrs } = args;
  const { x, filter } = inputs;
  const { strides, pad, dataFormat, dilations, dimRoundingMode } = attrs;
  const $dataFormat = backend_util_exports.convertConv2DDataFormat(dataFormat);
  let $dilations = dilations;
  if ($dilations == null) {
    $dilations = [1, 1];
  }
  const convInfo = backend_util_exports.computeConv2DInfo(x.shape, filter.shape, strides, $dilations, pad, dimRoundingMode, true, $dataFormat);
  const dimensions = [
    { type: "int32", data: [convInfo.padInfo.top, convInfo.padInfo.left] },
    { type: "int32", data: [convInfo.inHeight, convInfo.inWidth] }
  ];
  const isChannelsLast = convInfo.dataFormat === "channelsLast";
  let program;
  if (!isChannelsLast && convInfo.inHeight > 16 && convInfo.inWidth > 16 && convInfo.strideHeight === 1 && convInfo.strideWidth === 1 && convInfo.dilationWidth === 1 && convInfo.dilationHeight === 1 && convInfo.inChannels === convInfo.outChannels) {
    program = new DepthwiseConv2DNCHWSharedProgram(convInfo.outShape, convInfo.filterHeight, convInfo.filterWidth);
  } else if (isChannelsLast && convInfo.outHeight > 4 && convInfo.outWidth > 4 && convInfo.strideWidth <= 2 && convInfo.inChannels === convInfo.outChannels && convInfo.dilationHeight === 1 && convInfo.dilationWidth === 1 && convInfo.inChannels % 4 === 0) {
    program = new DepthwiseConv2DVec4Program(convInfo);
  } else {
    program = new DepthwiseConv2DProgram(convInfo);
    dimensions.push({ type: "int32", data: [convInfo.filterHeight] }, { type: "int32", data: [convInfo.filterWidth] }, { type: "int32", data: [convInfo.strideHeight, convInfo.strideWidth] }, {
      type: "int32",
      data: [convInfo.dilationHeight, convInfo.dilationWidth]
    });
  }
  return backend.runWebGPUProgram(program, [x, filter], x.dtype, dimensions);
}
var depthwiseConv2dNativeConfig = {
  kernelName: DepthwiseConv2dNative,
  backendName: "webgpu",
  kernelFunc: depthwiseConv2dNative
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/diag_webgpu.js
var DiagProgram = class {
  constructor(size) {
    this.variableNames = ["x"];
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = [size, size];
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.shaderKey = "diag";
  }
  getUserCode() {
    const userCode = `
      ${getMainHeaderString("index")} {
        if (index < uniforms.size) {
          let coords = getOutputCoords();
          let value = select(0.0, getX(coords[0]), coords[0] == coords[1]);
          setOutputAtIndex(index, value);
        }
      }
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Diag.js
function diag(args) {
  const { inputs, backend } = args;
  const { x } = inputs;
  const outShape = [...x.shape, ...x.shape];
  const xSize = util_exports.sizeFromShape(x.shape);
  const flat = reshape({ inputs: { x }, backend, attrs: { shape: [xSize] } });
  const program = new DiagProgram(xSize);
  const res = backend.runWebGPUProgram(program, [flat], flat.dtype);
  const out = reshape({ inputs: { x: res }, backend, attrs: { shape: outShape } });
  backend.disposeData(flat.dataId);
  backend.disposeData(res.dataId);
  return out;
}
var diagConfig = {
  kernelName: Diag,
  backendName: "webgpu",
  kernelFunc: diag
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/dilation_webgpu.js
var Dilation2DProgram = class {
  constructor(convInfo) {
    this.variableNames = ["x", "w"];
    this.uniforms = "filterDims: vec2<i32>, pad: vec2<i32>, stride: vec2<i32>, dilation: vec2<i32>";
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = convInfo.outShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.shaderKey = "dilation2d";
  }
  getUserCode() {
    const userCode = `
       ${getMainHeaderString("index")} {
         if (index < uniforms.size) {
           let neg_infinity = -3.4e38;
           let coords = getOutputCoords();
           let batch = coords.x;
           let d1 = coords.w;
           let outTopLeftCorner = coords.yz * uniforms.stride - uniforms.pad;
           let hBeg = outTopLeftCorner.x;
           let wBeg = outTopLeftCorner.y;

           var curVal = neg_infinity;
           for (var h = 0; h < uniforms.filterDims[0]; h = h + 1) {
             let hIn = hBeg + h * uniforms.dilation[0];

             if (hIn >= 0 && hIn < uniforms.xShape[1]) {
               for (var w = 0; w < uniforms.filterDims[1]; w = w + 1) {
                 let wIn = wBeg + w * uniforms.dilation[1];

                 if (wIn >= 0 && wIn < uniforms.xShape[2]) {
                   let val = getX(batch, hIn, wIn, d1) + getW(h, w, d1);
                   if (val > curVal) {
                     curVal = val;
                   }
                 }
               }
             }
           }

           setOutputAtIndex(index, curVal);
         }
       }
     `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Dilation2D.js
function dilation2D(args) {
  const { inputs, backend, attrs } = args;
  const { x, filter } = inputs;
  const { strides, pad, dilations } = attrs;
  const convInfo = backend_util_exports.computeDilation2DInfo(x.shape, filter.shape, strides, pad, "NHWC", dilations);
  const padInfo = [convInfo.padInfo.top, convInfo.padInfo.left];
  const uniformData = [
    { type: "int32", data: [convInfo.filterHeight, convInfo.filterWidth] },
    { type: "int32", data: [...padInfo] },
    { type: "int32", data: [convInfo.strideHeight, convInfo.strideWidth] },
    { type: "int32", data: [convInfo.dilationHeight, convInfo.dilationWidth] }
  ];
  const program = new Dilation2DProgram(convInfo);
  const out = backend.runWebGPUProgram(program, [x, filter], x.dtype, uniformData);
  return out;
}
var dilation2DConfig = {
  kernelName: Dilation2D,
  backendName: "webgpu",
  kernelFunc: dilation2D
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Multiply.js
var multiplyKernelFunc = binaryKernelFunc({
  opType: BinaryOpType.MUL,
  cpuKernelImpl: multiplyImplCPU,
  supportsComplex: true
});
var multiplyConfig = {
  kernelName: Multiply,
  backendName: "webgpu",
  kernelFunc: multiplyKernelFunc
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Sum.js
function sum(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { axis, keepDims } = attrs;
  return reduce(x, axis, keepDims, "sum", backend);
}
var sumConfig = {
  kernelName: Sum,
  backendName: "webgpu",
  kernelFunc: sum
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Einsum.js
function einsum(args) {
  const { inputs, backend, attrs } = args;
  const { equation } = attrs;
  const tensors = inputs;
  const { allDims, summedDims, idDims } = backend_util_exports.decodeEinsumEquation(equation, tensors.length);
  backend_util_exports.checkEinsumDimSizes(allDims.length, idDims, tensors);
  const { path, steps } = backend_util_exports.getEinsumComputePath(summedDims, idDims);
  const nSteps = steps.length;
  let out = null;
  let numDimsRemaining = allDims.length;
  const tensorsToDispose = [];
  for (let i = 0; i < nSteps; ++i) {
    for (const idTerm of steps[i]) {
      const { permutationIndices: perm, expandDims: dimsToExpand } = backend_util_exports.getEinsumPermutation(numDimsRemaining, idDims[idTerm]);
      let x;
      if (backend_util_exports.isIdentityPermutation(perm)) {
        x = tensors[idTerm];
      } else {
        x = transpose({ inputs: { x: tensors[idTerm] }, backend, attrs: { perm } });
        tensorsToDispose.push(x);
      }
      const targetShape = x.shape.slice();
      for (let k = 0; k < dimsToExpand.length; ++k) {
        targetShape.splice(dimsToExpand[k], 0, 1);
      }
      if (!util_exports.arraysEqual(x.shape, targetShape)) {
        x = reshape({ inputs: { x }, backend, attrs: { shape: targetShape } });
        tensorsToDispose.push(x);
      }
      if (out === null) {
        out = x;
      } else {
        out = multiplyKernelFunc({ inputs: { a: x, b: out }, backend });
        tensorsToDispose.push(out);
      }
    }
    if (i < nSteps - 1) {
      if (path[i] >= 0) {
        out = sum({
          inputs: { x: out },
          backend,
          attrs: {
            axis: path[i] - (allDims.length - numDimsRemaining),
            keepDims: false
          }
        });
        tensorsToDispose.push(out);
      }
      numDimsRemaining--;
    }
  }
  for (const tensorInfo of tensorsToDispose) {
    if (tensorInfo === out) {
      continue;
    }
    backend.disposeData(tensorInfo.dataId);
  }
  return out;
}
var einsumConfig = {
  kernelName: Einsum,
  backendName: "webgpu",
  kernelFunc: einsum
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Elu.js
var elu = unaryKernelFunc({ opType: UnaryOpType.ELU });
var eluConfig = {
  kernelName: Elu,
  backendName: "webgpu",
  kernelFunc: elu
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Equal.js
var equal = binaryKernelFunc({ opType: BinaryOpType.EQUAL, dtype: "bool", cpuKernelImpl: equalImplCPU });
var equalConfig = {
  kernelName: Equal,
  backendName: "webgpu",
  kernelFunc: equal
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Erf.js
var erf = unaryKernelFunc({ opType: UnaryOpType.ERF });
var erfConfig = {
  kernelName: Erf,
  backendName: "webgpu",
  kernelFunc: erf
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Exp.js
var exp = unaryKernelFunc({
  opType: UnaryOpType.EXP,
  cpuKernelImpl: expImplCPU,
  dtype: "float32"
});
var expConfig = {
  kernelName: Exp,
  backendName: "webgpu",
  kernelFunc: exp
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/ExpandDims.js
function expandDims(args) {
  const { inputs, attrs, backend } = args;
  const { dim } = attrs;
  const { input } = inputs;
  const inputRank = input.shape.length;
  const newShape = input.shape.slice();
  let $dim = dim;
  if (dim < 0) {
    util_exports.assert(-(inputRank + 1) <= dim, () => `Axis must be in the interval [${-(inputRank + 1)}, ${inputRank}]`);
    $dim = inputRank + dim + 1;
  }
  newShape.splice($dim, 0, 1);
  return reshape({ inputs: { x: input }, backend, attrs: { shape: newShape } });
}
var expandDimsConfig = {
  kernelName: ExpandDims,
  backendName: "webgpu",
  kernelFunc: expandDims
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Expm1.js
var expm1 = unaryKernelFunc({ opType: UnaryOpType.EXPM1, cpuKernelImpl: expm1ImplCPU });
var expm1Config = {
  kernelName: Expm1,
  backendName: "webgpu",
  kernelFunc: expm1
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/fft_webgpu.js
var FFTProgram = class {
  constructor(component, shape) {
    this.variableNames = ["real", "imag"];
    this.outputShape = [];
    this.uniforms = "exponentMultiplier : f32, denominator: f32,";
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = shape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.component = component;
    this.shaderKey = `fft_${component}`;
  }
  getUserCode() {
    const opString = this.component === "real" ? "return real * expR - imag * expI;" : "return real * expI + imag * expR;";
    const userCode = `
    fn unaryOpComplex(real: f32, expR: f32, imag: f32, expI: f32) -> f32 {
      ${opString}
    }

    fn mulMatDFT(batch: i32, index: i32) -> f32 {
      let indexRatio = f32(index) / f32(uniforms.realShape[1]);
      let exponentMultiplierTimesIndexRatio =
          uniforms.exponentMultiplier * indexRatio;

      var result = 0.0;

      for (var i = 0; i < uniforms.realShape[1]; i = i + 1) {
        // x = (-2|2 * PI / N) * index * i;
        let x = exponentMultiplierTimesIndexRatio * f32(i);
        let expR = cos(x);
        let expI = sin(x);
        let real = getReal(batch, i);
        let imag = getImag(batch, i);

        result = result +
            unaryOpComplex(real, expR, imag, expI) / uniforms.denominator;
      }

      return result;
    }

    ${getMainHeaderString("index")} {
      if (index < uniforms.size) {
        let coords = getOutputCoords();
        setOutputAtIndex(index, mulMatDFT(coords[0], coords[1]));
      }
    }
  `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/FFT_impl.js
function fftImpl(x, inverse, backend) {
  const xData = backend.tensorMap.get(x.dataId);
  const inputSize = util_exports.sizeFromShape(x.shape);
  const innerDimensionSize = x.shape[x.shape.length - 1];
  const batch = inputSize / innerDimensionSize;
  const toDispose = [];
  const input2D = reshape({ inputs: { x }, backend, attrs: { shape: [batch, innerDimensionSize] } });
  toDispose.push(input2D);
  const xShape = input2D.shape;
  const realProgram = new FFTProgram("real", xShape);
  const imagProgram = new FFTProgram("imag", xShape);
  const inputs = [
    {
      dataId: xData.complexTensorInfos.real.dataId,
      dtype: xData.complexTensorInfos.real.dtype,
      shape: xShape
    },
    {
      dataId: xData.complexTensorInfos.imag.dataId,
      dtype: xData.complexTensorInfos.imag.dtype,
      shape: xShape
    }
  ];
  const exponentMultiplier = inverse ? 2 * Math.PI : -2 * Math.PI;
  const denominator = inverse ? xShape[1] : 1;
  const uniformData = [
    { type: "float32", data: [exponentMultiplier] },
    { type: "float32", data: [denominator] }
  ];
  const realPart = backend.runWebGPUProgram(realProgram, inputs, "float32", uniformData);
  toDispose.push(realPart);
  const imagPart = backend.runWebGPUProgram(imagProgram, inputs, "float32", uniformData);
  toDispose.push(imagPart);
  const complexOutput = complex({ inputs: { real: realPart, imag: imagPart }, backend });
  toDispose.push(complexOutput);
  const complexOutputReshaped = reshape({ inputs: { x: complexOutput }, backend, attrs: { shape: x.shape } });
  toDispose.forEach((t) => backend.disposeData(t.dataId));
  return complexOutputReshaped;
}

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/FFT.js
function fft(args) {
  const { inputs, backend } = args;
  const { input } = inputs;
  return fftImpl(input, false, backend);
}
var fftConfig = {
  kernelName: FFT,
  backendName: "webgpu",
  kernelFunc: fft
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/flip_left_right_webgpu.js
var FlipLeftRightProgram = class {
  constructor(imageShape) {
    this.outputShape = [];
    this.variableNames = ["x"];
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = imageShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.shaderKey = "flipLeftRight";
  }
  getUserCode() {
    const userCode = `
      ${getMainHeaderString("index")} {
        if (index < uniforms.size) {
          let coords = getCoordsFromIndex(index);
          let coordX = uniforms.xShape[2] - coords[2] - 1;
          let outputValue = getX(coords[0], coords[1], coordX, coords[3]);
          setOutputAtIndex(index, outputValue);
        }
      }
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/FlipLeftRight.js
var flipLeftRightConfig = {
  kernelName: FlipLeftRight,
  backendName: "webgpu",
  kernelFunc: ({ inputs, backend }) => {
    const { image } = inputs;
    const webgpuBackend = backend;
    const program = new FlipLeftRightProgram(image.shape);
    const output = webgpuBackend.runWebGPUProgram(program, [image], image.dtype);
    return output;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Floor.js
var floor = unaryKernelFunc({ opType: UnaryOpType.FLOOR, cpuKernelImpl: floorImplCPU });
var floorConfig = {
  kernelName: Floor,
  backendName: "webgpu",
  kernelFunc: floor
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/FloorDiv.js
var floorDiv = binaryKernelFunc({ opType: BinaryOpType.INT_DIV, dtype: "int32" });
var floorDivConfig = {
  kernelName: FloorDiv,
  backendName: "webgpu",
  kernelFunc: floorDiv
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/from_pixels_webgpu.js
var FromPixelsProgram = class {
  constructor(outputShape, numChannels, importVideo = false) {
    this.isFromPixels = true;
    this.outputShape = [0];
    this.variableNames = [];
    this.workgroupSize = [256, 1, 1];
    this.outputShape = outputShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize, [numChannels, 1, 1]);
    this.importVideo = importVideo;
    this.shaderKey = `fromPixels_${this.importVideo}`;
  }
  getUserCode() {
    const textureLoad = this.importVideo ? "textureLoad(src, vec2<i32>(coords.yx));" : "textureLoad(src, vec2<i32>(coords.yx), 0)";
    const textureType = this.importVideo ? "texture_external" : "texture_2d<f32>";
    return `
      @binding(1) @group(0) var src: ${textureType};
      ${getMainHeaderString("index")} {
        let flatIndex = index * uniforms.numChannels;
        if (flatIndex < uniforms.size) {
          let coords = getCoordsFromIndex(flatIndex);
          let values = ${textureLoad};
          for (var i = 0; i < uniforms.numChannels; i = i + 1) {
            result[flatIndex + i] = i32(floor(255.0 * values[i]));
          }
        }
      }
  `;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/FromPixels.js
var fromPixelsConfig = {
  kernelName: FromPixels,
  backendName: "webgpu",
  kernelFunc: fromPixels
};
var fromPixels2DContext;
var willReadFrequently = env().getBool("CANVAS2D_WILL_READ_FREQUENTLY_FOR_GPU");
var videoToTextureMap = /* @__PURE__ */ new Map();
function fromPixels(args) {
  const { inputs, backend, attrs } = args;
  let { pixels } = inputs;
  const { numChannels } = attrs;
  if (pixels == null) {
    throw new Error("pixels passed to tf.browser.fromPixels() can not be null");
  }
  const isVideo = typeof HTMLVideoElement !== "undefined" && pixels instanceof HTMLVideoElement;
  const isImage = typeof HTMLImageElement !== "undefined" && pixels instanceof HTMLImageElement;
  const isCanvas = typeof HTMLCanvasElement !== "undefined" && pixels instanceof HTMLCanvasElement || typeof OffscreenCanvas !== "undefined" && pixels instanceof OffscreenCanvas;
  const isImageBitmap = typeof ImageBitmap !== "undefined" && pixels instanceof ImageBitmap;
  const [width, height] = isVideo ? [
    pixels.videoWidth,
    pixels.videoHeight
  ] : [pixels.width, pixels.height];
  const outputShape = [height, width, numChannels];
  const importVideo = false;
  const isVideoOrImage = isVideo || isImage;
  if (isImageBitmap || isCanvas || isVideoOrImage) {
    let textureInfo;
    if (importVideo) {
      const videoElement = pixels;
      if (!videoToTextureMap.has(videoElement) || videoToTextureMap.get(videoElement).expired) {
        const externalTextureDescriptor = { source: videoElement };
        videoToTextureMap.set(videoElement, backend.device.importExternalTexture(externalTextureDescriptor));
      }
      textureInfo = {
        width,
        height,
        format: null,
        usage: null,
        texture: videoToTextureMap.get(videoElement)
      };
    } else {
      if (isVideoOrImage) {
        const newWillReadFrequently = env().getBool("CANVAS2D_WILL_READ_FREQUENTLY_FOR_GPU");
        if (fromPixels2DContext == null || newWillReadFrequently !== willReadFrequently) {
          willReadFrequently = newWillReadFrequently;
          fromPixels2DContext = document.createElement("canvas").getContext("2d", { willReadFrequently });
        }
        fromPixels2DContext.canvas.width = width;
        fromPixels2DContext.canvas.height = height;
        fromPixels2DContext.drawImage(pixels, 0, 0, width, height);
        pixels = fromPixels2DContext.canvas;
      }
      const usage = GPUTextureUsage.COPY_DST | GPUTextureUsage.RENDER_ATTACHMENT | GPUTextureUsage.TEXTURE_BINDING;
      const format = "rgba8unorm";
      const texture = backend.textureManager.acquireTexture(outputShape[1], outputShape[0], format, usage);
      backend.queue.copyExternalImageToTexture({ source: pixels }, { texture }, [outputShape[1], outputShape[0]]);
      textureInfo = { width, height, format, usage, texture };
    }
    const size = util_exports.sizeFromShape(outputShape);
    const strides = util_exports.computeStrides(outputShape);
    const program = new FromPixelsProgram(outputShape, numChannels, importVideo);
    const uniformData = [
      { type: "uint32", data: [size] },
      { type: "uint32", data: [numChannels] },
      { type: "uint32", data: [...strides] }
    ];
    const input = backend.makeTensorInfo([height, width], "int32");
    const info = backend.tensorMap.get(input.dataId);
    info.resourceInfo = textureInfo;
    const result = backend.runWebGPUProgram(program, [input], "int32", uniformData);
    backend.disposeData(input.dataId);
    return result;
  }
  const imageData = pixels.data;
  let pixelArray = imageData;
  if (numChannels != null && numChannels !== 4) {
    pixelArray = new Uint8Array(pixels.width * pixels.height * numChannels);
    const dataLength = imageData.length;
    let j = 0;
    for (let i = 0; i < dataLength; i++) {
      if (i % 4 < numChannels) {
        pixelArray[j++] = imageData[i];
      }
    }
  }
  const output = backend.makeTensorInfo(outputShape, "int32", new Int32Array(pixelArray));
  backend.uploadToGPU(output.dataId);
  return output;
}

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/batchnorm_webgpu.js
var BatchNormProgram = class {
  constructor(xShape, meanShape, varianceShape, offsetShape, scaleShape) {
    this.uniforms = "varianceEpsilon : f32,";
    this.workgroupSize = [128, 1, 1];
    this.size = true;
    this.variableNames = ["x", "mean", "variance"];
    backend_util_exports.assertAndGetBroadcastShape(xShape, meanShape);
    backend_util_exports.assertAndGetBroadcastShape(xShape, varianceShape);
    this.outputShape = xShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    if (offsetShape != null) {
      backend_util_exports.assertAndGetBroadcastShape(xShape, offsetShape);
      this.variableNames.push("offset");
    }
    if (scaleShape != null) {
      backend_util_exports.assertAndGetBroadcastShape(xShape, scaleShape);
      this.variableNames.push("scale");
    }
    this.offsetShape = offsetShape;
    this.scaleShape = scaleShape;
    this.shaderKey = "batchNorm";
  }
  getUserCode() {
    let offsetSnippet = "0.0";
    if (this.offsetShape != null) {
      offsetSnippet = "getOffsetByOutputIndex(index)";
    }
    let scaleSnippet = "1.0";
    if (this.scaleShape != null) {
      scaleSnippet = "getScaleByOutputIndex(index)";
    }
    const userCode = `
      ${getMainHeaderString("index")} {
        if (index < uniforms.size)
        {
          let xValue = getXByOutputIndex(index);
          let meanValue = getMeanByOutputIndex(index);
          let varianValue = getVarianceByOutputIndex(index);
          let offsetValue = ${offsetSnippet};
          let scaleValue = ${scaleSnippet};
          let inv = scaleValue * inverseSqrt(varianValue + f32(uniforms.varianceEpsilon));
          setOutputAtIndex(index,dot(vec3<f32>(xValue, -meanValue, offsetValue), vec3<f32>(inv, inv, 1.0)));
        }
      }
  `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/FusedBatchNorm.js
var fusedBatchNormConfig = {
  kernelName: FusedBatchNorm,
  backendName: "webgpu",
  kernelFunc: ({ inputs, attrs, backend }) => {
    const { x, scale, offset, mean: mean2, variance } = inputs;
    const { varianceEpsilon } = attrs;
    const webGPUBackend = backend;
    const batchNormInputs = [x, mean2, variance];
    let offsetShape = null;
    if (offset != null) {
      offsetShape = offset.shape;
      batchNormInputs.push(offset);
    }
    let scaleShape = null;
    if (scale != null) {
      scaleShape = scale.shape;
      batchNormInputs.push(scale);
    }
    const program = new BatchNormProgram(x.shape, mean2.shape, variance.shape, offsetShape, scaleShape);
    const uniformData = [{ type: "float32", data: [varianceEpsilon] }];
    return webGPUBackend.runWebGPUProgram(program, batchNormInputs, x.dtype, uniformData);
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/FusedConv2D.js
function fusedConv2d(args) {
  const { inputs, backend, attrs } = args;
  const { x, filter, bias, preluActivationWeights } = inputs;
  const { strides, pad, dataFormat, dilations, dimRoundingMode, activation, leakyreluAlpha } = attrs;
  const $dataFormat = backend_util_exports.convertConv2DDataFormat(dataFormat);
  const convInfo = backend_util_exports.computeConv2DInfo(x.shape, filter.shape, strides, dilations, pad, dimRoundingMode, false, $dataFormat);
  return conv2DImpl({
    x,
    filter,
    convInfo,
    backend,
    bias,
    preluActivationWeights,
    leakyreluAlpha,
    activation
  });
}
var fusedConv2DConfig = {
  kernelName: FusedConv2D,
  backendName: "webgpu",
  kernelFunc: fusedConv2d
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/FusedDepthwiseConv2D.js
function fusedDepthwiseConv2D(args) {
  const { inputs, backend, attrs } = args;
  const { x, filter, bias, preluActivationWeights } = inputs;
  const { strides, pad, dilations, dimRoundingMode, activation, leakyreluAlpha } = attrs;
  let $dilations = dilations;
  if ($dilations == null) {
    $dilations = [1, 1];
  }
  util_exports.assert(backend_util_exports.eitherStridesOrDilationsAreOne(strides, $dilations), () => `Error in depthwiseConv2d: Either strides or dilations must be 1. Got strides ${strides} and dilations '${$dilations}'`);
  const convInfo = backend_util_exports.computeConv2DInfo(
    x.shape,
    filter.shape,
    strides,
    $dilations,
    pad,
    dimRoundingMode,
    true
    /* depthwise */
  );
  const programInputs = [x, filter];
  const hasBias = bias != null;
  const hasPreluActivationWeights = preluActivationWeights != null;
  if (hasBias) {
    programInputs.push(bias);
  }
  if (hasPreluActivationWeights) {
    programInputs.push(preluActivationWeights);
  }
  const dimensions = [
    { type: "int32", data: [convInfo.padInfo.top, convInfo.padInfo.left] },
    { type: "int32", data: [convInfo.inHeight, convInfo.inWidth] }
  ];
  let program;
  if (convInfo.outHeight > 4 && convInfo.outWidth > 4 && convInfo.strideWidth <= 2 && convInfo.inChannels === convInfo.outChannels && convInfo.dilationHeight === 1 && convInfo.dilationWidth === 1 && convInfo.inChannels % 4 === 0) {
    program = new DepthwiseConv2DVec4Program(convInfo, hasBias, activation, hasPreluActivationWeights);
  } else {
    program = new DepthwiseConv2DProgram(convInfo, hasBias, activation, hasPreluActivationWeights);
    dimensions.push({ type: "int32", data: [convInfo.filterHeight] }, { type: "int32", data: [convInfo.filterWidth] }, { type: "int32", data: [convInfo.strideHeight, convInfo.strideWidth] }, {
      type: "int32",
      data: [convInfo.dilationHeight, convInfo.dilationWidth]
    });
  }
  if (activation === "leakyrelu") {
    dimensions.push({ type: "float32", data: [leakyreluAlpha] });
    program.uniforms += " alpha : f32,";
  }
  const result = backend.runWebGPUProgram(program, programInputs, "float32", dimensions);
  return result;
}
var fusedDepthwiseConv2DConfig = {
  kernelName: FusedDepthwiseConv2D,
  backendName: "webgpu",
  kernelFunc: fusedDepthwiseConv2D
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/gather_nd_webgpu.js
var GatherNDProgram = class {
  constructor(sliceDim, shape) {
    this.variableNames = ["A", "indices"];
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = shape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.shaderKey = `gathernd_${sliceDim}`;
    this.sliceDim = sliceDim;
    this.uniforms = `sliceDim : i32, strides : ${getCoordsDataType(sliceDim)},`;
  }
  getUserCode() {
    let strideString;
    if (this.sliceDim > 1) {
      strideString = "uniforms.strides[j]";
    } else {
      strideString = "uniforms.strides";
    }
    const userCode = `
      ${getMainHeaderString("index")} {
        if (index < uniforms.size) {
          let coords = getCoordsFromIndex(index);
          var flattenIndex = 0;
          for (var j = 0; j < uniforms.sliceDim; j = j + 1) {
            let indexTemp = i32(round(getIndices(coords[0], j)));
            let strideNum = ${strideString};
            flattenIndex = flattenIndex + indexTemp * strideNum;
          }

          setOutputAtIndex(index, getA(flattenIndex, coords[1]));
        }
      }
      `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/GatherNd.js
function gatherNd(args) {
  const { inputs, backend } = args;
  const { params, indices } = inputs;
  const indicesShape = indices.shape;
  const sliceRank = indicesShape[indicesShape.length - 1];
  const paramsSize = util_exports.sizeFromShape(params.shape);
  const [resultShape, numSlices, sliceSize, strides] = backend_util_exports.prepareAndValidate(params, indices);
  const flattenIndices = reshape({ inputs: { x: indices }, backend, attrs: { shape: [numSlices, sliceRank] } });
  const flattenX = reshape({
    inputs: { x: params },
    backend,
    attrs: { shape: [util_exports.sizeFromShape(params.shape) / sliceSize, sliceSize] }
  });
  if (backend.shouldExecuteOnCPU([params, indices]) || params.dtype === "string") {
    const indicesData = backend.readSync(indices.dataId);
    const paramsBuf = backend.bufferSync(params);
    const outValue = gatherNdImplCPU(indicesData, paramsBuf, params.dtype, numSlices, sliceRank, sliceSize, strides, params.shape, paramsSize);
    return backend.makeTensorInfo(resultShape, params.dtype, outValue.values);
  }
  const program = new GatherNDProgram(sliceRank, [numSlices, sliceSize]);
  const uniformData = [{ type: "int32", data: [sliceRank] }, { type: "int32", data: strides }];
  const res = backend.runWebGPUProgram(program, [flattenX, flattenIndices], flattenX.dtype, uniformData);
  const reshaped = reshape({ inputs: { x: res }, backend, attrs: { shape: resultShape } });
  backend.disposeData(flattenIndices.dataId);
  backend.disposeData(flattenX.dataId);
  backend.disposeData(res.dataId);
  return reshaped;
}
var gatherNdConfig = {
  kernelName: GatherNd,
  backendName: "webgpu",
  kernelFunc: gatherNd
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/gather_webgpu.js
var GatherProgram = class {
  constructor(aShape, outputShape) {
    this.variableNames = ["A", "indices"];
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = aShape.slice();
    this.aShape = aShape;
    this.outputShape = outputShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.shaderKey = `gather`;
  }
  getUserCode() {
    const sourceCoords = getSourceCoords(this.aShape);
    const userCode = `
      ${getMainHeaderString("index")} {
        if (index < uniforms.size) {
          let resRC = getCoordsFromIndex(index);
          let indexZ = i32(getIndices(resRC.x, resRC.z));
          let inBounds = select(0.0, 1.0, indexZ >= 0 && indexZ < uniforms.aShape[2]);
          setOutputAtIndex(index, inBounds * getA(${sourceCoords}));
        }
      }
    `;
    return userCode;
  }
};
function getSourceCoords(aShape) {
  const currentCoords = ["resRC.x", "resRC.y", "resRC.z", "resRC.w"];
  const sourceCoords = [];
  for (let i = 0; i < aShape.length; i++) {
    if (i === 2) {
      sourceCoords.push("indexZ");
    } else {
      sourceCoords.push(`${currentCoords[i]}`);
    }
  }
  return sourceCoords.join();
}

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/GatherV2.js
function gatherV2(args) {
  const { inputs, backend, attrs } = args;
  const { x, indices } = inputs;
  const { axis, batchDims } = attrs;
  const parsedAxis = util_exports.parseAxisParam(axis, x.shape)[0];
  const shapeInfo = backend_util_exports.segment_util.collectGatherOpShapeInfo(x, indices, parsedAxis, batchDims);
  const indicesSize = util_exports.sizeFromShape(indices.shape);
  const toDispose = [];
  const flattenX = reshape({
    inputs: { x },
    backend,
    attrs: {
      shape: [
        shapeInfo.batchSize,
        shapeInfo.outerSize,
        shapeInfo.dimSize,
        shapeInfo.sliceSize
      ]
    }
  });
  const flattenIndex = reshape({
    inputs: { x: indices },
    backend,
    attrs: { shape: [shapeInfo.batchSize, indicesSize / shapeInfo.batchSize] }
  });
  toDispose.push(flattenX);
  toDispose.push(flattenIndex);
  const flattenOutputShape = [
    shapeInfo.batchSize,
    shapeInfo.outerSize,
    indicesSize / shapeInfo.batchSize,
    shapeInfo.sliceSize
  ];
  if (backend.shouldExecuteOnCPU([x, indices])) {
    const indicesBufferInfo = backend.tensorMap.get(flattenIndex.dataId);
    const indicesValues = indicesBufferInfo.values;
    const indicesBuf = buffer(flattenIndex.shape, flattenIndex.dtype, indicesValues);
    const xBufferInfo = backend.tensorMap.get(flattenX.dataId);
    const xValues = xBufferInfo.values;
    const xBuf = buffer(flattenX.shape, flattenX.dtype, xValues);
    const outBuf = gatherV2ImplCPU(xBuf, indicesBuf, flattenOutputShape);
    toDispose.forEach((t) => backend.disposeData(t.dataId));
    return backend.makeTensorInfo(shapeInfo.outputShape, outBuf.dtype, outBuf.values);
  }
  const program = new GatherProgram(flattenX.shape, flattenOutputShape);
  const res = backend.runWebGPUProgram(program, [flattenX, flattenIndex], flattenX.dtype);
  toDispose.push(res);
  const reshaped = reshape({ inputs: { x: res }, backend, attrs: { shape: shapeInfo.outputShape } });
  toDispose.forEach((t) => backend.disposeData(t.dataId));
  return reshaped;
}
var gatherV2Config = {
  kernelName: GatherV2,
  backendName: "webgpu",
  kernelFunc: gatherV2
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Greater.js
var greater = binaryKernelFunc({
  opType: BinaryOpType.GREATER,
  cpuKernelImpl: greaterImplCPU,
  dtype: "bool"
});
var greaterConfig = {
  kernelName: Greater,
  backendName: "webgpu",
  kernelFunc: greater
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/GreaterEqual.js
var greaterEqual = binaryKernelFunc({
  opType: BinaryOpType.GREATER_EQUAL,
  dtype: "bool",
  cpuKernelImpl: greaterEqualImplCPU
});
var greaterEqualConfig = {
  kernelName: GreaterEqual,
  backendName: "webgpu",
  kernelFunc: greaterEqual
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/IFFT.js
function ifft(args) {
  const { inputs, backend } = args;
  const { input } = inputs;
  return fftImpl(input, true, backend);
}
var ifftConfig = {
  kernelName: IFFT,
  backendName: "webgpu",
  kernelFunc: ifft
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/IsFinite.js
var isFinite = unaryKernelFunc({ opType: UnaryOpType.IS_FINITE, dtype: "bool" });
var isFiniteConfig = {
  kernelName: IsFinite,
  backendName: "webgpu",
  kernelFunc: isFinite
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/IsInf.js
var isInf = unaryKernelFunc({ opType: UnaryOpType.IS_INF, dtype: "bool" });
var isInfConfig = {
  kernelName: IsInf,
  backendName: "webgpu",
  kernelFunc: isInf
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/IsNaN.js
var isNaN = unaryKernelFunc({ opType: UnaryOpType.IS_NAN, dtype: "bool" });
var isNaNConfig = {
  kernelName: IsNan,
  backendName: "webgpu",
  kernelFunc: isNaN
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/LeakyRelu.js
function leakyRelu(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { alpha } = attrs;
  const uniformData = [{ type: "float32", data: [alpha] }];
  const program = new UnaryOpProgram(x.shape, UnaryOpType.LEAKYRELU, "alpha : f32,");
  return backend.runWebGPUProgram(program, [x], "float32", uniformData);
}
var leakyReluConfig = {
  kernelName: LeakyRelu,
  backendName: "webgpu",
  kernelFunc: leakyRelu
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Less.js
var less = binaryKernelFunc({ opType: BinaryOpType.LESS, dtype: "bool", cpuKernelImpl: lessImplCPU });
var lessConfig = {
  kernelName: Less,
  backendName: "webgpu",
  kernelFunc: less
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/LessEqual.js
var lessEqual = binaryKernelFunc({
  opType: BinaryOpType.LESS_EQUAL,
  dtype: "bool",
  cpuKernelImpl: lessEqualImplCPU
});
var lessEqualConfig = {
  kernelName: LessEqual,
  backendName: "webgpu",
  kernelFunc: lessEqual
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/lin_space_webgpu.js
var LinSpaceProgram = class {
  constructor(shape) {
    this.variableNames = [];
    this.outputShape = [];
    this.uniforms = "start : f32, step : f32,";
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = [shape];
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.shaderKey = "linSpace";
  }
  getUserCode() {
    const userCode = `
      ${getMainHeaderString("index")} {
        if (index < uniforms.size) {
          setOutputAtIndex(index, uniforms.start + f32(index) * uniforms.step);
        }
      }
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/LinSpace.js
function linSpace(args) {
  const { backend, attrs } = args;
  const { start, stop, num } = attrs;
  const step2 = (stop - start) / (num - 1);
  const program = new LinSpaceProgram(num);
  const uniformData = [{ type: "float32", data: [start] }, { type: "float32", data: [step2] }];
  return backend.runWebGPUProgram(program, [], "float32", uniformData);
}
var linSpaceConfig = {
  kernelName: LinSpace,
  backendName: "webgpu",
  kernelFunc: linSpace
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Log.js
var log = unaryKernelFunc({ opType: UnaryOpType.LOG, cpuKernelImpl: logImplCPU });
var logConfig = {
  kernelName: Log,
  backendName: "webgpu",
  kernelFunc: log
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Log1p.js
var log1p = unaryKernelFunc({ opType: UnaryOpType.LOG1P });
var log1pConfig = {
  kernelName: Log1p,
  backendName: "webgpu",
  kernelFunc: log1p
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/LogicalAnd.js
var logicalAnd = binaryKernelFunc({ opType: BinaryOpType.LOGICAL_AND, dtype: "bool" });
var logicalAndConfig = {
  kernelName: LogicalAnd,
  backendName: "webgpu",
  kernelFunc: logicalAnd
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/LogicalNot.js
var logicalNot = unaryKernelFunc({ opType: UnaryOpType.LOGICAL_NOT });
var logicalNotConfig = {
  kernelName: LogicalNot,
  backendName: "webgpu",
  kernelFunc: logicalNot
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/LogicalOr.js
var logicalOr = binaryKernelFunc({ opType: BinaryOpType.LOGICAL_OR });
var logicalOrConfig = {
  kernelName: LogicalOr,
  backendName: "webgpu",
  kernelFunc: logicalOr
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/lrn_webgpu.js
var powOperatorSnippet = `
  var powValue = 0.0;
  let basis = uniforms.bias + uniforms.alpha * sum;
  if (uniforms.beta == 0.5) {
    powValue = inverseSqrt(basis);
  } else if (uniforms.beta == 1.0) {
    powValue = 1.0 / basis;
  } else {
    powValue = exp(log(basis) * (-uniforms.beta));
  }
`;
var LRNProgram = class {
  constructor(xShape) {
    this.outputShape = [];
    this.variableNames = ["x"];
    this.uniforms = "radius : i32, bias : f32, alpha : f32, beta : f32,";
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = xShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.shaderKey = "lrn";
  }
  getUserCode() {
    const userCode = `
    ${getMainHeaderString("index")} {
      if (index < uniforms.size) {
        let coords = getOutputCoords();
        let b = coords[0];
        let r = coords[1];
        let c = coords[2];
        let d = coords[3];

        let x = getX(b, r, c, d);
        var sum = 0.0;
        for (var i = -uniforms.radius; i <= uniforms.radius; i = i + 1) {
          let idx = d + i;
          if (idx >= 0 && idx < uniforms.xShape[3]) {
            let z = getX(b, r, c, idx);
            sum = sum + z * z;
          }
        }
        ${powOperatorSnippet}

        setOutputAtIndex(index, x * powValue);
      }
    }
  `;
    return userCode;
  }
};
var LRNSharedProgram = class {
  constructor(xShape, radius) {
    this.outputShape = [];
    this.variableNames = ["x"];
    this.uniforms = "radius : i32, bias : f32, alpha : f32, beta : f32,";
    this.workgroupSize = [256, 1, 1];
    this.maxAllowRadius = 16;
    util_exports.assert(radius <= this.maxAllowRadius, () => `Radius must be less than or equal to ${this.maxAllowRadius}, current radius is ${radius}`);
    this.outputShape = xShape;
    this.elementsPerWorkgroup = this.workgroupSize[0] - 2 * this.maxAllowRadius;
    this.dispatchLayout = { x: [3], y: [2], z: [0, 1] };
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, [
      this.elementsPerWorkgroup,
      this.workgroupSize[1],
      this.workgroupSize[2]
    ]);
    this.shaderKey = "lrn_shared";
  }
  getUserCode() {
    const userCode = `
    var <workgroup>lrnSub: array<f32, ${this.workgroupSize[0]}>;
    const elementsPerWorkgroup = ${this.elementsPerWorkgroup};
    const maxAllowRadius = ${this.maxAllowRadius};

    ${getMainHeaderString()} {
      let localDepth = i32(localId.x);
      let workgroupDepth = i32(workgroupId.x) * elementsPerWorkgroup;
      let xDepth = workgroupDepth + localDepth - maxAllowRadius;
      let b = i32(globalId.z) / uniforms.xShape[1];
      let r = i32(globalId.z) - b * uniforms.xShape[1];
      let c = i32(globalId.y);
      let d = workgroupDepth + localDepth;

      var x = 0.0;
      if (xDepth >= 0 && xDepth < uniforms.xShape[3]) {
        x = getX(b, r, c, xDepth);
      }
      lrnSub[localDepth] = x;
      workgroupBarrier();

      if (localDepth < elementsPerWorkgroup && d < uniforms.outShape[3]) {
        var sum = 0.0;
        let index = localDepth + maxAllowRadius;
        for (var i = -uniforms.radius; i <= uniforms.radius; i = i + 1) {
          let z = lrnSub[index + i];
          sum = sum + z * z;
        }
        ${powOperatorSnippet}

        setOutputAtCoords(b, r, c, d, lrnSub[index] * powValue);
      }
    } `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/LRN.js
function lrn(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { depthRadius, bias, alpha, beta } = attrs;
  let program;
  if (depthRadius > 16) {
    program = new LRNProgram(x.shape);
  } else {
    program = new LRNSharedProgram(x.shape, depthRadius);
  }
  const uniformData = [
    { type: "int32", data: [depthRadius] },
    { type: "float32", data: [bias] },
    { type: "float32", data: [alpha] },
    { type: "float32", data: [beta] }
  ];
  const res = backend.runWebGPUProgram(program, [x], x.dtype, uniformData);
  return res;
}
var lrnConfig = {
  kernelName: LRN,
  backendName: "webgpu",
  kernelFunc: lrn
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Maximum.js
var maximum = binaryKernelFunc({
  opType: BinaryOpType.MAX,
  cpuKernelImpl: maximumImplCPU
});
var maximumConfig = {
  kernelName: Maximum,
  backendName: "webgpu",
  kernelFunc: maximum
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/MaxPool.js
function maxPool(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { filterSize, strides, pad, dimRoundingMode } = attrs;
  const dilations = 1;
  const convInfo = backend_util_exports.computePool2DInfo(x.shape, filterSize, strides, dilations, pad, dimRoundingMode);
  return poolImpl(x, convInfo, "max", backend);
}
var maxPoolConfig = {
  kernelName: MaxPool,
  backendName: "webgpu",
  kernelFunc: maxPool
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Min.js
function min(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { axis, keepDims } = attrs;
  return reduce(x, axis, keepDims, "min", backend);
}
var minConfig = {
  kernelName: Min,
  backendName: "webgpu",
  kernelFunc: min
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Minimum.js
var minimum = binaryKernelFunc({
  opType: BinaryOpType.MIN,
  cpuKernelImpl: minimumImplCPU
});
var minimumConfig = {
  kernelName: Minimum,
  backendName: "webgpu",
  kernelFunc: minimum
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/mirror_pad_webgpu.js
var MirrorPadProgram = class {
  constructor(xShape, paddings, mode) {
    this.uniforms = "";
    this.variableNames = ["x"];
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = paddings.map(
      (p, i) => p[0] + xShape[i] + p[1]
      /* afterPad */
    );
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.xShape = xShape;
    paddings.map((_, i) => {
      this.uniforms += ` pad${i} : vec2<i32>,`;
    });
    this.offset = mode === "reflect" ? 0 : 1;
    this.shaderKey = `mirrorPad_${mode}`;
  }
  getUserCode() {
    const rank = this.xShape.length;
    const start = this.xShape.map((_, i) => `uniforms.pad${i}[0]`).join(",");
    const end = this.xShape.map((_, i) => `uniforms.pad${i}[0] + uniforms.xShape${rank > 1 ? `[${i}]` : ""}`).join(",");
    const shaderStart = rank === 1 ? "start" : "start[i]";
    const shaderEnd = rank === 1 ? "end" : "end[i]";
    const shaderOutC = rank === 1 ? "outC" : "outC[i]";
    const dtype = getCoordsDataType(rank);
    const unpackedCoords = rank > 1 ? ["coords[0]", "coords[1]", "coords[2]", "coords[3]"].slice(0, rank) : "coords";
    return `
      ${getMainHeaderString("index")} {
        if (index < uniforms.size) {
          let start = ${dtype}(${start});
          let end = ${dtype}(${end});
          var outC = getCoordsFromIndex(index);
          for (var i = 0; i < ${rank}; i = i + 1) {
            if (${shaderOutC} < ${shaderStart}) {
              ${shaderOutC} = ${shaderStart} * 2 - ${shaderOutC} - ${this.offset};
            } else if(${shaderOutC} >= ${shaderEnd}) {
              ${shaderOutC} = (${shaderEnd} - 1) * 2 - ${shaderOutC} + ${this.offset};
            }
          }
          let coords = outC - start;
          setOutputAtIndex(index, getX(${unpackedCoords}));
        }
      }
    `;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/MirrorPad.js
var mirrorPadConfig = {
  kernelName: MirrorPad,
  backendName: "webgpu",
  kernelFunc: ({ inputs, attrs, backend }) => {
    const { x } = inputs;
    const { paddings, mode } = attrs;
    const webGPUBackend = backend;
    const uniformData = paddings.map((p) => {
      return { type: "int32", data: [p[0], p[1]] };
    });
    const program = new MirrorPadProgram(x.shape, paddings, mode);
    const output = webGPUBackend.runWebGPUProgram(program, [x], x.dtype, uniformData);
    return output;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Mod.js
var mod = binaryKernelFunc({ opType: BinaryOpType.MOD });
var modConfig = {
  kernelName: Mod,
  backendName: "webgpu",
  kernelFunc: mod
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Neg.js
function neg(args) {
  const { inputs, backend } = args;
  const { x } = inputs;
  if (backend.shouldExecuteOnCPU([x])) {
    const xData = backend.tensorMap.get(x.dataId);
    const [outValues, newShape] = negImplCPU(xData.values, x.shape, x.dtype);
    return backend.makeTensorInfo(newShape, x.dtype, outValues);
  }
  const program = new UnaryOpProgram(x.shape, UnaryOpType.NEG);
  return backend.runWebGPUProgram(program, [x], x.dtype);
}
var negConfig = {
  kernelName: Neg,
  backendName: "webgpu",
  kernelFunc: neg
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/NonMaxSuppressionV3.js
function nonMaxSuppressionV3(args) {
  console.warn("tf.nonMaxSuppression() in webgpu locks the UI thread. Call tf.nonMaxSuppressionAsync() instead");
  const { inputs, backend, attrs } = args;
  const { boxes, scores } = inputs;
  const { maxOutputSize, iouThreshold, scoreThreshold } = attrs;
  const boxesVals = backend.readSync(boxes.dataId);
  const scoresVals = backend.readSync(scores.dataId);
  const { selectedIndices } = kernel_impls_exports.nonMaxSuppressionV3Impl(boxesVals, scoresVals, maxOutputSize, iouThreshold, scoreThreshold);
  return backend.makeTensorInfo([selectedIndices.length], "int32", new Int32Array(selectedIndices));
}
var nonMaxSuppressionV3Config = {
  kernelName: NonMaxSuppressionV3,
  backendName: "webgpu",
  kernelFunc: nonMaxSuppressionV3
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/NonMaxSuppressionV5.js
function nonMaxSuppressionV5(args) {
  console.warn("tf.nonMaxSuppression() in webgpu locks the UI thread. Call tf.nonMaxSuppressionAsync() instead");
  const { inputs, backend, attrs } = args;
  const { boxes, scores } = inputs;
  const { maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma } = attrs;
  const boxesVals = backend.readSync(boxes.dataId);
  const scoresVals = backend.readSync(scores.dataId);
  const maxOutputSizeVal = maxOutputSize;
  const iouThresholdVal = iouThreshold;
  const scoreThresholdVal = scoreThreshold;
  const softNmsSigmaVal = softNmsSigma;
  const { selectedIndices, selectedScores } = kernel_impls_exports.nonMaxSuppressionV5Impl(boxesVals, scoresVals, maxOutputSizeVal, iouThresholdVal, scoreThresholdVal, softNmsSigmaVal);
  return [
    backend.makeTensorInfo([selectedIndices.length], "int32", new Int32Array(selectedIndices)),
    backend.makeTensorInfo([selectedScores.length], "float32", new Float32Array(selectedScores))
  ];
}
var nonMaxSuppressionV5Config = {
  kernelName: NonMaxSuppressionV5,
  backendName: "webgpu",
  kernelFunc: nonMaxSuppressionV5
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/onehot_webgpu.js
var OneHotProgram = class {
  constructor(numIndices, depth) {
    this.variableNames = ["x"];
    this.uniforms = "onValue : f32, offValue : f32,";
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = [numIndices, depth];
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.shaderKey = "onehot";
  }
  getUserCode() {
    const userCode = `
      ${getMainHeaderString("index")} {
        if(index < uniforms.size) {
          let coords = getCoordsFromIndex(index);
          setOutputAtIndex(index, mix(uniforms.offValue, uniforms.onValue,
                                      f32(i32(round(getX(coords.x))) == coords.y)));
        }
      }
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/OneHot.js
function oneHot(args) {
  const { inputs, backend, attrs } = args;
  const { indices } = inputs;
  const { dtype, depth, onValue, offValue } = attrs;
  const indicesSize = util_exports.sizeFromShape(indices.shape);
  const program = new OneHotProgram(indicesSize, depth);
  const reshaped = reshape({ inputs: { x: indices }, backend, attrs: { shape: [indicesSize] } });
  const uniformData = [{ type: "float32", data: [onValue] }, { type: "float32", data: [offValue] }];
  const result = backend.runWebGPUProgram(program, [reshaped], dtype, uniformData);
  backend.disposeData(reshaped.dataId);
  const outShape = [...indices.shape, depth];
  const out = reshape({ inputs: { x: result }, backend, attrs: { shape: outShape } });
  backend.disposeData(result.dataId);
  return out;
}
var oneHotConfig = {
  kernelName: OneHot,
  backendName: "webgpu",
  kernelFunc: oneHot
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/ZerosLike.js
function zerosLike(args) {
  const { inputs, backend } = args;
  const { x } = inputs;
  if (x.dtype === "complex64") {
    const realPart = real({ inputs: { input: x }, backend });
    const r = zerosLike({ inputs: { x: realPart }, backend });
    const imagPart = imag({ inputs: { input: x }, backend });
    const i = zerosLike({ inputs: { x: imagPart }, backend });
    const result = complex({ inputs: { real: r, imag: i }, backend });
    backend.disposeData(realPart.dataId);
    backend.disposeData(r.dataId);
    backend.disposeData(imagPart.dataId);
    backend.disposeData(i.dataId);
    return result;
  } else {
    return fill({
      attrs: {
        shape: x.shape,
        dtype: x.dtype,
        value: x.dtype === "string" ? "" : 0
      },
      backend
    });
  }
}
var zerosLikeConfig = {
  kernelName: ZerosLike,
  backendName: "webgpu",
  kernelFunc: zerosLike
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/OnesLike.js
function onesLike(args) {
  const { inputs, backend } = args;
  const { x } = inputs;
  if (x.dtype === "string") {
    throw new Error("onesLike is not supported under string dtype");
  } else if (x.dtype === "complex64") {
    const realPart = real({ inputs: { input: x }, backend });
    const r = onesLike({ inputs: { x: realPart }, backend });
    const imagPart = imag({ inputs: { input: x }, backend });
    const i = zerosLike({ inputs: { x: imagPart }, backend });
    const result = complex({ inputs: { real: r, imag: i }, backend });
    backend.disposeData(realPart.dataId);
    backend.disposeData(r.dataId);
    backend.disposeData(imagPart.dataId);
    backend.disposeData(i.dataId);
    return result;
  } else {
    return fill({ attrs: { shape: x.shape, dtype: x.dtype, value: 1 }, backend });
  }
}
var onesLikeConfig = {
  kernelName: OnesLike,
  backendName: "webgpu",
  kernelFunc: onesLike
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Pack.js
function pack(args) {
  const { inputs, backend, attrs } = args;
  const { axis } = attrs;
  if (inputs.length === 1) {
    return expandDims({ inputs: { input: inputs[0] }, backend, attrs: { dim: axis } });
  }
  const shape = inputs[0].shape;
  const dtype = inputs[0].dtype;
  inputs.forEach((t) => {
    util_exports.assertShapesMatch(shape, t.shape, "All tensors passed to stack must have matching shapes");
    util_exports.assert(dtype === t.dtype, () => "All tensors passed to stack must have matching dtypes");
  });
  const intermediateTensorInfos = [];
  const expandedTensors = inputs.map((t) => {
    const expandedT = expandDims({ inputs: { input: t }, backend, attrs: { dim: axis } });
    intermediateTensorInfos.push(expandedT);
    return expandedT;
  });
  const result = concat({ inputs: expandedTensors, backend, attrs: { axis } });
  intermediateTensorInfos.forEach((t) => backend.disposeData(t.dataId));
  return result;
}
var packConfig = {
  kernelName: Pack,
  backendName: "webgpu",
  kernelFunc: pack
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/pad_webgpu.js
var PadProgram = class {
  constructor(xShape, paddings) {
    this.variableNames = ["x"];
    this.uniforms = "constantValue : f32,";
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = paddings.map(
      (p, i) => p[0] + xShape[i] + p[1]
      /* afterPad */
    );
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    paddings.map((_, i) => {
      this.uniforms += ` pad${i} : vec2<i32>,`;
    });
    this.xShape = xShape;
    this.shaderKey = "pad";
  }
  getUserCode() {
    const rank = this.xShape.length;
    const type = getCoordsDataType(rank);
    const start = this.xShape.map((_, i) => `uniforms.pad${i}[0]`).join(",");
    const end = this.xShape.map((_, i) => `uniforms.pad${i}[0] + uniforms.xShape${rank > 1 ? `[${i}]` : ""}`).join(",");
    const startValue = rank > 1 ? `${type}(${start})` : `${start}`;
    const endValue = rank > 1 ? `${type}(${end})` : `${end}`;
    const leftPadCondition = rank > 1 ? `any(outC < start)` : `outC < start`;
    const rightPadCondition = rank > 1 ? `any(outC >= end)` : `outC >= end`;
    const unpackedCoords = rank > 1 ? ["coords[0]", "coords[1]", "coords[2]", "coords[3]"].slice(0, rank) : "coords";
    const userCode = `
      ${getMainHeaderString("index")} {
        if (index < uniforms.size) {
          let start = ${startValue};
          let end = ${endValue};
          let outC = getCoordsFromIndex(index);

          if (${leftPadCondition} || ${rightPadCondition}) {
            setOutputAtIndex(index, uniforms.constantValue);
          } else {
            let coords = outC - start;
            setOutputAtIndex(index, getX(${unpackedCoords}));
          }
        }
      }
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/PadV2.js
var padV2 = (args) => {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { paddings, constantValue } = attrs;
  if (paddings.every((p) => util_exports.arraysEqual(p, [0, 0]))) {
    return identity({ inputs: { x }, backend });
  }
  if (util_exports.sizeFromShape(x.shape) === 0) {
    const outputShape = paddings.map(
      (p, i) => p[0] + x.shape[i] + p[1]
      /* afterPad */
    );
    return fill({
      backend,
      attrs: { shape: outputShape, value: constantValue, dtype: x.dtype }
    });
  }
  const uniformData = [{ type: "float32", data: [constantValue] }];
  paddings.map((p) => uniformData.push({ type: "int32", data: [p[0], p[1]] }));
  const program = new PadProgram(x.shape, paddings);
  return backend.runWebGPUProgram(program, [x], x.dtype, uniformData);
};
var padV2Config = {
  kernelName: PadV2,
  backendName: "webgpu",
  kernelFunc: padV2
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Pow.js
var pow = binaryKernelFunc({
  opType: BinaryOpType.POW
});
var powConfig = {
  kernelName: Pow,
  backendName: "webgpu",
  kernelFunc: pow
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Prelu.js
function prelu(args) {
  const { inputs, backend } = args;
  const { x, alpha } = inputs;
  const program = new BinaryOpProgram(BinaryOpType.PRELU, x.shape, alpha.shape);
  return backend.runWebGPUProgram(program, [x, alpha], "float32");
}
var preluConfig = {
  kernelName: Prelu,
  backendName: "webgpu",
  kernelFunc: prelu
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Prod.js
function prod(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { axis, keepDims } = attrs;
  return reduce(x, axis, keepDims, "prod", backend);
}
var prodConfig = {
  kernelName: Prod,
  backendName: "webgpu",
  kernelFunc: prod
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Range.js
var range = (args) => {
  const { backend, attrs } = args;
  const { start, stop, step: step2, dtype } = attrs;
  const values = rangeImplCPU(start, stop, step2, dtype);
  return backend.makeTensorInfo([values.length], dtype, values);
};
var rangeConfig = {
  kernelName: Range,
  backendName: "webgpu",
  kernelFunc: range
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/RealDiv.js
var realDiv = binaryKernelFunc({ opType: BinaryOpType.DIV });
var realDivConfig = {
  kernelName: RealDiv,
  backendName: "webgpu",
  kernelFunc: realDiv
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Reciprocal.js
var reciprocal = unaryKernelFunc({ opType: UnaryOpType.RECIPROCAL });
var reciprocalConfig = {
  kernelName: Reciprocal,
  backendName: "webgpu",
  kernelFunc: reciprocal
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Relu.js
var relu = unaryKernelFunc({ opType: UnaryOpType.RELU });
var reluConfig = {
  kernelName: Relu,
  backendName: "webgpu",
  kernelFunc: relu
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Relu6.js
var relu6 = unaryKernelFunc({ opType: UnaryOpType.RELU6 });
var relu6Config = {
  kernelName: Relu6,
  backendName: "webgpu",
  kernelFunc: relu6
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/resize_bilinear_webgpu.js
var ResizeBilinearProgram = class {
  constructor(inputShape, newHeight, newWidth) {
    this.variableNames = ["x"];
    this.uniforms = "adjustHeightWidth : vec2<f32>, halfPixelCenters : f32,";
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = [inputShape[0], newHeight, newWidth, inputShape[3]];
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.shaderKey = `resizeBilinear`;
  }
  getUserCode() {
    const userCode = `
      ${getMainHeaderString("index")} {
        if (index < uniforms.size) {
        let coords = getCoordsFromIndex(index);
          let b = coords[0];
          let d = coords[3];
          let rc = coords.yz;

          let effectiveInSize = vec2<f32>(
            f32(uniforms.xShape.y) - uniforms.adjustHeightWidth[0],
            f32(uniforms.xShape.z) - uniforms.adjustHeightWidth[1]);

          let effectiveOutSize = vec2<f32>(
            f32(uniforms.outShape.y) - uniforms.adjustHeightWidth[0],
            f32(uniforms.outShape.z) - uniforms.adjustHeightWidth[1]);

          let effectiveInputOverOutputRatioRC =
              effectiveInSize / effectiveOutSize;

          // Fractional source index
          let sourceFracIndexRC =
            (vec2<f32>(rc) + vec2<f32>(uniforms.halfPixelCenters)) *
            effectiveInputOverOutputRatioRC - vec2<f32>(uniforms.halfPixelCenters);

          // Compute the four integer indices.
          let sourceFloorRC = vec2<i32>(sourceFracIndexRC);
          let sourceCeilRC = vec2<i32>(
            min(vec2<f32>(uniforms.xShape.yz) - vec2<f32>(1.0), ceil(sourceFracIndexRC)));

          let topLeft = getX(b, sourceFloorRC.x, sourceFloorRC.y, d);
          let bottomLeft = getX(b, sourceCeilRC.x, sourceFloorRC.y, d);
          let topRight = getX(b, sourceFloorRC.x, sourceCeilRC.y, d);
          let bottomRight = getX(b, sourceCeilRC.x, sourceCeilRC.y, d);

          let fracRC = sourceFracIndexRC - vec2<f32>(sourceFloorRC);

          let top = topLeft + (topRight - topLeft) * fracRC.y;
          let bottom = bottomLeft + (bottomRight - bottomLeft) * fracRC.y;
          let newValue = top + (bottom - top) * fracRC.x;

          setOutputAtIndex(index, newValue);
        }
      }
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/ResizeBilinear.js
function resizeBilinear(args) {
  const { inputs, backend, attrs } = args;
  const { images } = inputs;
  const { alignCorners, size, halfPixelCenters } = attrs;
  const [newHeight, newWidth] = size;
  const adjustHeight = alignCorners && newHeight > 1 ? 1 : 0;
  const adjustWidth = alignCorners && newWidth > 1 ? 1 : 0;
  const halfPixelCentersValue = halfPixelCenters ? 0.5 : 0;
  const uniformData = [
    { type: "float32", data: [adjustHeight, adjustWidth] },
    { type: "float32", data: [halfPixelCentersValue] }
  ];
  const program = new ResizeBilinearProgram(images.shape, newHeight, newWidth);
  return backend.runWebGPUProgram(program, [images], "float32", uniformData);
}
var resizeBilinearConfig = {
  kernelName: ResizeBilinear,
  backendName: "webgpu",
  kernelFunc: resizeBilinear
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/resize_nearest_neighbor_webgpu.js
var ResizeNearestNeighborProgram = class {
  constructor(inputShape, newHeight, newWidth, halfPixelCenters) {
    this.variableNames = ["x"];
    this.uniforms = "adjustHeightWidth : vec2<f32>, roundBase : f32,";
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = [inputShape[0], newHeight, newWidth, inputShape[3]];
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.halfPixelCenters = halfPixelCenters;
    this.shaderKey = `resizeNearest_${halfPixelCenters}`;
  }
  getUserCode() {
    let sourceFracIndexRC;
    if (this.halfPixelCenters) {
      sourceFracIndexRC = `max((vec2<f32>(rc) + vec2<f32>(0.5)) * effectiveInputOverOutputRatioRC, vec2<f32>(0.0))`;
    } else {
      sourceFracIndexRC = `vec2<f32>(rc) * effectiveInputOverOutputRatioRC`;
    }
    const userCode = `
      ${getMainHeaderString("index")} {
        if (index < uniforms.size) {
          let coords = getCoordsFromIndex(index);
          let b = coords[0];
          let d = coords[3];
          let rc = coords.yz;

          let effectiveInSize = vec2<f32>(
            f32(uniforms.xShape.y) - uniforms.adjustHeightWidth[0],
            f32(uniforms.xShape.z) - uniforms.adjustHeightWidth[1]);

          let effectiveOutSize = vec2<f32>(
            f32(uniforms.outShape.y) - uniforms.adjustHeightWidth[0],
            f32(uniforms.outShape.z) - uniforms.adjustHeightWidth[1]);

          let effectiveInputOverOutputRatioRC =
              effectiveInSize / effectiveOutSize;

          // Fractional source index
          let sourceFracIndexRC = ${sourceFracIndexRC};

          // Compute the coordinators of nearest neighbor point.
          let inputShapeRC = vec2<f32>(f32(uniforms.xShape.y), f32(uniforms.xShape.z));
          let sourceNearestRC = vec2<i32>(
            min(inputShapeRC - 1.0, floor(sourceFracIndexRC + uniforms.roundBase)));
          let newValue = getX(b, sourceNearestRC.x, sourceNearestRC.y, d);

          setOutputAtIndex(index, newValue);
        }
      }
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/ResizeNearestNeighbor.js
function resizeNearestNeighbor(args) {
  const { inputs, backend, attrs } = args;
  const { images } = inputs;
  const { alignCorners, halfPixelCenters, size } = attrs;
  const [newHeight, newWidth] = size;
  const adjustHeight = alignCorners && newHeight > 1 ? 1 : 0;
  const adjustWidth = alignCorners && newWidth > 1 ? 1 : 0;
  const roundBase = alignCorners ? 0.5 : 0;
  const uniformData = [
    { type: "float32", data: [adjustHeight, adjustWidth] },
    { type: "float32", data: [roundBase] }
  ];
  const program = new ResizeNearestNeighborProgram(images.shape, newHeight, newWidth, halfPixelCenters);
  return backend.runWebGPUProgram(program, [images], images.dtype, uniformData);
}
var resizeNearestNeighborConfig = {
  kernelName: ResizeNearestNeighbor,
  backendName: "webgpu",
  kernelFunc: resizeNearestNeighbor
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/reverse_webgpu.js
var ReverseProgram = class {
  constructor(xShape) {
    this.variableNames = ["x"];
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = xShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.uniforms = ` axis : vec4<i32>,`;
    this.shaderKey = "reverse";
  }
  getUserCode() {
    const reverseCoordsSnippet = `
      // Using uniform variables as judging conditions, so the function has
      // coherent execution within all threads.
      fn getReverseCoords(coords : vec4<i32>) -> vec4<i32> {
        var reverseCoords = coords;
        if (uniforms.axis[0] == 1) {
          reverseCoords[0] = uniforms.xShape[0] - coords[0] - 1;
        }
        if (uniforms.axis[1] == 1) {
          reverseCoords[1] = uniforms.xShape[1] - coords[1] - 1;
        }
        if (uniforms.axis[2] == 1) {
          reverseCoords[2] = uniforms.xShape[2] - coords[2] - 1;
        }
        if (uniforms.axis[3] == 1) {
          reverseCoords[3] = uniforms.xShape[3] - coords[3] - 1;
        }

        return reverseCoords;
      }
    `;
    const userCode = `
      ${reverseCoordsSnippet}
      ${getMainHeaderString("index")} {
        if (index < uniforms.size) {
          let coords = getCoordsFromIndex(index);
          let reverseCoords = getReverseCoords(coords);
          setOutputAtIndex(index, getX(reverseCoords[0],
              reverseCoords[1], reverseCoords[2], reverseCoords[3]));
        }
      }
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Reverse.js
function reverse(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { dims } = attrs;
  const xRank = x.shape.length;
  if (xRank === 0) {
    return identity({ inputs: { x }, backend });
  }
  const xShape = x.shape;
  const xShape4D = [1, 1, 1, 1];
  xShape.forEach((d, i) => {
    const index = i + 4 - xRank;
    xShape4D[index] = d;
  });
  const axes = util_exports.parseAxisParam(dims, x.shape);
  const dims4D = [0, 0, 0, 0];
  axes.forEach((ax) => {
    const index = ax + 4 - xRank;
    dims4D[index] = 1;
  });
  const uniformData = [{ type: "int32", data: dims4D }];
  const xReshaped = reshape({ inputs: { x }, backend, attrs: { shape: xShape4D } });
  const program = new ReverseProgram(xShape4D);
  const values = backend.runWebGPUProgram(program, [xReshaped], xReshaped.dtype, uniformData);
  backend.disposeData(xReshaped.dataId);
  const result = reshape({ inputs: { x: values }, backend, attrs: { shape: xShape } });
  backend.disposeData(values.dataId);
  return result;
}
var reverseConfig = {
  kernelName: Reverse,
  backendName: "webgpu",
  kernelFunc: reverse
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/rotate_webgpu.js
var RotateProgram = class {
  constructor(imageShape, fillValue) {
    this.outputShape = [];
    this.variableNames = ["x"];
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = imageShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.uniforms = `centerX : f32, centerY : f32, sinRadians : f32,
          cosRadians : f32,`;
    this.shaderKey = "rotate";
    this.outputShape = imageShape;
    if (typeof fillValue === "number") {
      this.uniforms += ` fillValue : f32,`;
      this.fillSnippet = `var outputValue = uniforms.fillValue;`;
      this.shaderKey += "_float";
    } else {
      this.uniforms += ` fillValue : vec3<f32>,`;
      this.fillSnippet = `var outputValue = uniforms.fillValue[coords[3]];`;
      this.shaderKey += "_vec3";
    }
  }
  getUserCode() {
    const userCode = `
        ${getMainHeaderString("index")} {
          if (index < uniforms.size) {
            let coords = getCoordsFromIndex(index);
            let coordXFloat = (f32(coords[2]) - uniforms.centerX) *
                uniforms.cosRadians - (f32(coords[1]) - uniforms.centerY) *
                uniforms.sinRadians;
            let coordYFloat = (f32(coords[2]) - uniforms.centerX) *
                uniforms.sinRadians + (f32(coords[1]) - uniforms.centerY) *
                uniforms.cosRadians;
            let coordX = i32(round(coordXFloat + uniforms.centerX));
            let coordY = i32(round(coordYFloat + uniforms.centerY));
            ${this.fillSnippet}
            if(coordX >= 0 && coordX < uniforms.xShape[2] && coordY >= 0 &&
                coordY < uniforms.xShape[1]) {
              outputValue = getX(coords[0], coordY, coordX, coords[3]);
            }
            setOutputAtIndex(index, outputValue);
          }
        }
      `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/RotateWithOffset.js
var rotateWithOffsetConfig = {
  kernelName: RotateWithOffset,
  backendName: "webgpu",
  kernelFunc: ({ inputs, attrs, backend }) => {
    const { image } = inputs;
    const { radians, fillValue, center } = attrs;
    const webgpuBackend = backend;
    const program = new RotateProgram(image.shape, fillValue);
    const [centerX, centerY] = backend_util_exports.getImageCenter(center, image.shape[1], image.shape[2]);
    const uniformData = [
      { type: "float32", data: [centerX] },
      { type: "float32", data: [centerY] },
      { type: "float32", data: [Math.sin(radians)] },
      { type: "float32", data: [Math.cos(radians)] }
    ];
    if (typeof fillValue === "number") {
      uniformData.push({ type: "float32", data: [Number.parseFloat(fillValue.toFixed(2))] });
    } else {
      uniformData.push({ type: "float32", data: fillValue });
    }
    const output = webgpuBackend.runWebGPUProgram(program, [image], image.dtype, uniformData);
    return output;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Round.js
var round = unaryKernelFunc({ opType: UnaryOpType.ROUND });
var roundConfig = {
  kernelName: Round,
  backendName: "webgpu",
  kernelFunc: round
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Rsqrt.js
var rsqrt = unaryKernelFunc({ opType: UnaryOpType.RSQRT, cpuKernelImpl: rsqrtImplCPU });
var rsqrtConfig = {
  kernelName: Rsqrt,
  backendName: "webgpu",
  kernelFunc: rsqrt
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/scatter_webgpu.js
var ScatterProgram = class {
  constructor(flattenXShape, sliceDim, indicesRank, updatesRank, strides, shape, outputDtype, sumDupeIndices = true) {
    this.variableNames = ["updates", "indices"];
    this.workgroupSize = [64, 1, 1];
    this.atomic = true;
    this.outputShape = shape;
    this.type = outputDtype;
    this.sumDupeIndices = sumDupeIndices;
    this.dispatchLayout = flatDispatchLayout(flattenXShape);
    this.dispatch = computeDispatch(this.dispatchLayout, flattenXShape, this.workgroupSize);
    this.sliceDimGreaterThanOne = sliceDim > 1;
    this.shaderKey = `scatter_${indicesRank}_${updatesRank}_${this.sliceDimGreaterThanOne}_${outputDtype}_${sumDupeIndices}`;
    const stridesType = getCoordsDataType(strides.length);
    this.uniforms = `sliceDim : i32, strides: ${stridesType}, updatesSize: i32,`;
    this.updatesRank = updatesRank;
    this.indicesRank = indicesRank;
  }
  getUserCode() {
    let indicesString = "";
    if (this.indicesRank === 1) {
      indicesString = "coords[0]";
    } else if (this.indicesRank === 2) {
      indicesString = "coords[0], j";
    }
    const indicesSnippet = `getIndices(${indicesString})`;
    const strideString = this.sliceDimGreaterThanOne ? "uniforms.strides[j]" : "uniforms.strides";
    let outCoordsString = "";
    let getUpdatesCoordsFromFlatIndex = "";
    if (this.dispatchLayout.x.length === 1) {
      outCoordsString = "flattenedIndex";
      getUpdatesCoordsFromFlatIndex = `
      fn getUpdatesCoordsFromFlatIndex(index : i32) -> i32 {
        return index;
      }
      `;
    } else if (this.dispatchLayout.x.length === 2) {
      outCoordsString = "vec2<i32>(flattenedIndex, coords[1])";
      getUpdatesCoordsFromFlatIndex = `
      fn getUpdatesCoordsFromFlatIndex(index : i32) -> vec2<i32> {
        // N.B. |updates| could be a scalar tensor, conceptually representing a
        // 2D tensor with all values equal to that. By design, its size must be
        // the same as |outShape[1]| in one dimension, and |indicesShape[0]|
        // gives the other.
        let sliceSize = uniforms.outShape[1];
        let d0 = index / sliceSize;
        let d1 = index - d0 * sliceSize;
        return vec2<i32>(d0, d1);
      }
      `;
    }
    const updatesString = Array.from({ length: this.updatesRank }, (_, idx) => `coords[${idx}]`);
    const updatesSnippet = `getUpdates(${updatesString.join(", ")})`;
    const userCode = `
    ${getUpdatesCoordsFromFlatIndex}
      ${getMainHeaderString("index")} {
        if (index < uniforms.updatesSize) {
          let coords = getUpdatesCoordsFromFlatIndex(index);
          var flattenedIndex = 0;
          for (var j = 0; j < uniforms.sliceDim; j = j + 1) {
            let indexInside = i32(round(${indicesSnippet}));
            flattenedIndex = flattenedIndex + indexInside * ${strideString};
          }
          let updateValue =
              ${mapToWgslTypes(this.type, false)}(${updatesSnippet});
          let flatIndex = getOutputIndexFromCoords(${outCoordsString});

          ${this.sumDupeIndices ? atomicAddSnippet("&result[flatIndex]", "updateValue", this.type) : `atomicStore(&result[flatIndex], bitcast<i32>(updateValue));`}
        }
      }`;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/ScatterNd.js
function scatterNd(args) {
  const { inputs, backend, attrs } = args;
  const { indices, updates } = inputs;
  const { shape } = attrs;
  const { sliceRank, numUpdates, sliceSize, strides, outputSize } = backend_util_exports.calculateShapes(updates, indices, shape);
  const flattenShape = [outputSize / sliceSize, sliceSize];
  if (outputSize === 0) {
    return backend.makeTensorInfo(shape, indices.dtype);
  }
  const flattenIndices = reshape({ inputs: { x: indices }, backend, attrs: { shape: [numUpdates, sliceRank] } });
  const flattenX = reshape({ inputs: { x: updates }, backend, attrs: { shape: [numUpdates, sliceSize] } });
  const type = flattenX.dtype;
  const output = fill({ backend, attrs: { shape: flattenShape, value: 0, dtype: type } });
  const size = util_exports.sizeFromShape(flattenX.shape);
  const uniformData = [
    { type: "int32", data: [sliceRank] },
    { type: "int32", data: strides },
    { type: "int32", data: [size] }
  ];
  const program = new ScatterProgram(flattenX.shape, sliceRank, flattenIndices.shape.length, flattenX.shape.length, strides, flattenShape, type);
  const res = backend.runWebGPUProgram(program, [flattenX, flattenIndices], type, uniformData, output);
  const reshaped = reshape({ inputs: { x: res }, backend, attrs: { shape } });
  backend.disposeData(flattenIndices.dataId);
  backend.disposeData(flattenX.dataId);
  backend.disposeData(res.dataId);
  return reshaped;
}
var scatterNdConfig = {
  kernelName: ScatterNd,
  backendName: "webgpu",
  kernelFunc: scatterNd
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/search_sorted_webgpu.js
var SearchSortedProgram = class {
  constructor(outputShape, side) {
    this.outputShape = [];
    this.variableNames = ["sortedSequence", "values"];
    this.uniforms = "numInputs : i32,";
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = outputShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.side = side;
    this.shaderKey = `search_sorted_${side}`;
  }
  getUserCode() {
    const boundComparator = this.side === "left" ? "<" : "<=";
    const userCode = `
      fn findBound(batch: i32, value: f32) -> i32 {
        var left = i32(0);
        var right = uniforms.numInputs;
        while (left < right) {
          var mid = (left + right) / 2;
          if (getSortedSequence(batch, mid) ${boundComparator} value) {
            left = mid + 1;
          } else {
            right = mid;
          }
        }
        return right;
      }

      ${getMainHeaderString("index")} {
        if (index < uniforms.size) {
          let coords = getCoordsFromIndex(index);
          let value = getValuesByOutputIndex(index);
          setOutputAtIndexI32(index, findBound(coords[0], value));
        }
      }
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/SearchSorted.js
function searchSorted(args) {
  const { inputs, backend, attrs } = args;
  const { sortedSequence, values } = inputs;
  const { side } = attrs;
  const program = new SearchSortedProgram([values.shape[0], values.shape[1]], side);
  const uniformData = [{ type: "int32", data: [sortedSequence.shape[1]] }];
  return backend.runWebGPUProgram(program, [sortedSequence, values], "int32", uniformData);
}
var searchSortedConfig = {
  kernelName: SearchSorted,
  backendName: "webgpu",
  kernelFunc: searchSorted
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/select_webgpu.js
var SelectProgram = class {
  constructor(cRank, shape, rank) {
    this.variableNames = ["c", "a", "b"];
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = shape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.cRank = cRank;
    this.rank = rank;
    this.shaderKey = "select";
  }
  getUserCode() {
    let cCoords;
    let abCoords;
    if (this.rank > 4) {
      throw Error(`Where for rank ${this.rank} is not yet supported`);
    }
    if (this.rank === 1) {
      abCoords = `resRC`;
      cCoords = `resRC`;
    } else {
      const currentCoords = ["resRC.x", "resRC.y", "resRC.z", "resRC.w"];
      const cCoordVars = [];
      const abCoordVars = [];
      for (let i = 0; i < this.outputShape.length; i++) {
        abCoordVars.push(`${currentCoords[i]}`);
        if (i < this.cRank) {
          cCoordVars.push(`${currentCoords[i]}`);
        }
      }
      cCoords = cCoordVars.join();
      abCoords = abCoordVars.join();
    }
    const userCode = `
      ${getMainHeaderString("index")} {
        if (index < uniforms.size) {
          let resRC = getCoordsFromIndex(index);
          let cVal = getC(${cCoords});
          if (cVal >= 1.0) {
            setOutputAtIndex(index, getA(${abCoords}));
          } else {
            setOutputAtIndex(index, getB(${abCoords}));
          }
        }
      }
    `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Select.js
function select(args) {
  const { inputs, backend } = args;
  const { condition, t, e } = inputs;
  const program = new SelectProgram(condition.shape.length, t.shape, t.shape.length);
  return backend.runWebGPUProgram(program, [condition, t, e], upcastType(t.dtype, e.dtype));
}
var selectConfig = {
  kernelName: Select,
  backendName: "webgpu",
  kernelFunc: select
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Selu.js
var selu = unaryKernelFunc({ opType: UnaryOpType.SELU });
var seluConfig = {
  kernelName: Selu,
  backendName: "webgpu",
  kernelFunc: selu
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Sigmoid.js
var sigmoid = unaryKernelFunc({ opType: UnaryOpType.SIGMOID });
var sigmoidConfig = {
  kernelName: Sigmoid,
  backendName: "webgpu",
  kernelFunc: sigmoid
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Sign.js
var sign = unaryKernelFunc({ opType: UnaryOpType.SIGN });
var signConfig = {
  kernelName: Sign,
  backendName: "webgpu",
  kernelFunc: sign
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Sin.js
var sin = unaryKernelFunc({ opType: UnaryOpType.SIN });
var sinConfig = {
  kernelName: Sin,
  backendName: "webgpu",
  kernelFunc: sin
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Sinh.js
var sinh = unaryKernelFunc({ opType: UnaryOpType.SINH });
var sinhConfig = {
  kernelName: Sinh,
  backendName: "webgpu",
  kernelFunc: sinh
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Sub.js
var sub = binaryKernelFunc({ opType: BinaryOpType.SUB, cpuKernelImpl: subImplCPU, supportsComplex: true });
var subConfig = {
  kernelName: Sub,
  backendName: "webgpu",
  kernelFunc: sub
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Softmax.js
function softmax(args) {
  const { inputs, backend, attrs } = args;
  const { logits } = inputs;
  const { dim } = attrs;
  const axes = util_exports.parseAxisParam([dim], logits.shape);
  const maxLogit = max({
    inputs: { x: logits },
    backend,
    attrs: { reductionIndices: axes, keepDims: false }
  });
  const expandedShape = backend_util_exports.expandShapeToKeepDim(maxLogit.shape, axes);
  const maxLogitsReshaped = reshape({ inputs: { x: maxLogit }, backend, attrs: { shape: expandedShape } });
  const a = sub({ inputs: { a: logits, b: maxLogitsReshaped }, backend });
  const b = exp({ inputs: { x: a }, backend });
  const sumExp = sum({ inputs: { x: b }, backend, attrs: { axis: axes, keepDims: false } });
  const sumExpReshaped = reshape({ inputs: { x: sumExp }, backend, attrs: { shape: expandedShape } });
  const res = realDiv({ inputs: { a: b, b: sumExpReshaped }, backend });
  backend.disposeData(maxLogit.dataId);
  backend.disposeData(maxLogitsReshaped.dataId);
  backend.disposeData(a.dataId);
  backend.disposeData(b.dataId);
  backend.disposeData(sumExp.dataId);
  backend.disposeData(sumExpReshaped.dataId);
  return res;
}
var softmaxConfig = {
  kernelName: Softmax,
  backendName: "webgpu",
  kernelFunc: softmax
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Softplus.js
var softplus = unaryKernelFunc({ opType: UnaryOpType.SOFTPLUS });
var softplusConfig = {
  kernelName: Softplus,
  backendName: "webgpu",
  kernelFunc: softplus
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/SpaceToBatchND.js
var spaceToBatchND = (args) => {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { blockShape, paddings } = attrs;
  util_exports.assert(x.shape.length <= 4, () => "spaceToBatchND for rank > 4 with a WebGPU backend not implemented yet");
  const prod2 = blockShape.reduce((a, b) => a * b);
  const completePaddings = [[0, 0]];
  completePaddings.push(...paddings);
  for (let i = 1 + blockShape.length; i < x.shape.length; ++i) {
    completePaddings.push([0, 0]);
  }
  const toDispose = [];
  const paddedX = padV2({
    inputs: { x },
    backend,
    attrs: { paddings: completePaddings, constantValue: 0 }
  });
  const reshapedPaddedShape = backend_util_exports.getReshaped(paddedX.shape, blockShape, prod2, false);
  const permutedReshapedPaddedPermutation = backend_util_exports.getPermuted(reshapedPaddedShape.length, blockShape.length, false);
  const flattenShape = backend_util_exports.getReshapedPermuted(paddedX.shape, blockShape, prod2, false);
  const reshapedPaddedX = reshape({ inputs: { x: paddedX }, backend, attrs: { shape: reshapedPaddedShape } });
  const paddedXT = transpose({
    inputs: { x: reshapedPaddedX },
    backend,
    attrs: { perm: permutedReshapedPaddedPermutation }
  });
  const result = reshape({ inputs: { x: paddedXT }, backend, attrs: { shape: flattenShape } });
  toDispose.push(paddedX);
  toDispose.push(reshapedPaddedX);
  toDispose.push(paddedXT);
  toDispose.forEach((t) => backend.disposeData(t.dataId));
  return result;
};
var spaceToBatchNDConfig = {
  kernelName: SpaceToBatchND,
  backendName: "webgpu",
  kernelFunc: spaceToBatchND
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/tile_webgpu.js
var TileProgram = class {
  constructor(aShape, reps) {
    this.variableNames = ["A"];
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    const outputShape = new Array(aShape.length);
    for (let i = 0; i < outputShape.length; i++) {
      outputShape[i] = aShape[i] * reps[i];
    }
    this.outputShape = outputShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.rank = this.outputShape.length;
    this.shaderKey = "tile";
  }
  getUserCode() {
    const sourceCoords = getSourceCoords2(this.rank, "uniforms.");
    const userCode = `
      ${getMainHeaderString("index")} {
        if (index < uniforms.size) {
          let resRC = getCoordsFromIndex(index);
          setOutputAtIndex(index, getA(${sourceCoords}));
        }
      }
    `;
    return userCode;
  }
};
function getSourceCoords2(rank, uniformPrefix = "") {
  if (rank >= 5) {
    throw Error(`Tile for rank ${rank} is not yet supported`);
  }
  if (rank === 1) {
    return `(resRC % ${uniformPrefix}aShape)`;
  }
  const currentCoords = ["resRC.x", "resRC.y", "resRC.z", "resRC.w"];
  const sourceCoords = [];
  for (let i = 0; i < rank; i++) {
    sourceCoords.push(`(${currentCoords[i]} % ${uniformPrefix}aShape[${i}])`);
  }
  return sourceCoords.join();
}

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Tile.js
function tile(params) {
  const { inputs, backend, attrs } = params;
  const { x } = inputs;
  const { reps } = attrs;
  if (backend.shouldExecuteOnCPU([x]) || x.dtype === "string" || x.shape.length >= 5) {
    const data = backend.readSync(x.dataId);
    const value = x.dtype === "string" ? data.map((d) => util_exports.decodeString(d)) : data;
    const buf = buffer(x.shape, x.dtype, value);
    const outBuf = tileImplCPU(buf, reps);
    return backend.makeTensorInfo(outBuf.shape, outBuf.dtype, outBuf.values);
  }
  const program = new TileProgram(x.shape, reps);
  const output = backend.runWebGPUProgram(program, [x], x.dtype);
  return output;
}
var tileConfig = {
  kernelName: Tile,
  backendName: "webgpu",
  kernelFunc: tile
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/SparseToDense.js
function sparseToDense(args) {
  const { inputs, backend, attrs } = args;
  const { sparseIndices, sparseValues, defaultValue } = inputs;
  const { outputShape } = attrs;
  const { sliceRank, numUpdates, sliceSize, strides, outputSize } = backend_util_exports.calculateShapes(sparseValues, sparseIndices, outputShape);
  const sumDupeIndices = false;
  if (sparseValues.dtype === "string") {
    const indicesBuf = backend.bufferSync(sparseIndices);
    const updatesBuf = backend.bufferSync(sparseValues);
    const $defaultValue2 = util_exports.decodeString(backend.readSync(defaultValue.dataId)[0]);
    const outBuf = scatterImplCPU(indicesBuf, updatesBuf, outputShape, outputSize, sliceSize, numUpdates, sliceRank, strides, $defaultValue2, sumDupeIndices);
    return backend.makeTensorInfo(outputShape, outBuf.dtype, outBuf.values);
  }
  const flattenShape = [outputSize / sliceSize, sliceSize];
  const $sparseIndices = reshape({
    inputs: { x: sparseIndices },
    backend,
    attrs: { shape: [numUpdates, sliceRank] }
  });
  const $sparseValues = sparseValues.shape.length ? reshape({
    inputs: { x: sparseValues },
    backend,
    attrs: { shape: [numUpdates, sliceSize] }
  }) : identity({ inputs: { x: sparseValues }, backend });
  const type = $sparseValues.dtype;
  const zero = backend.makeTensorInfo([], type, util_exports.makeZerosTypedArray(1, type));
  const $defaultValue = reshape({
    inputs: { x: defaultValue },
    backend,
    attrs: { shape: Array(flattenShape.length).fill(1) }
  });
  const $denseValues = tile({ inputs: { x: $defaultValue }, backend, attrs: { reps: flattenShape } });
  const size = util_exports.sizeFromShape([numUpdates, sliceSize]);
  const uniformData = [
    { type: "int32", data: [sliceRank] },
    { type: "int32", data: strides },
    { type: "int32", data: [size] }
  ];
  switch (numUpdates) {
    case 0:
      break;
    case 1:
      if (true) {
        const program = new ScatterProgram([numUpdates, sliceSize], sliceRank, $sparseIndices.shape.length, $sparseValues.shape.length, strides, flattenShape, type, sumDupeIndices);
        backend.runWebGPUProgram(program, [$sparseValues, $sparseIndices], type, uniformData, $denseValues);
      }
      break;
    default:
      if (true) {
        const program = new ScatterProgram([numUpdates, sliceSize], sliceRank, $sparseIndices.shape.length, zero.shape.length, strides, flattenShape, type, sumDupeIndices);
        backend.runWebGPUProgram(program, [zero, $sparseIndices], type, uniformData, $denseValues);
      }
      {
        const program = new ScatterProgram([numUpdates, sliceSize], sliceRank, $sparseIndices.shape.length, $sparseValues.shape.length, strides, flattenShape, type);
        backend.runWebGPUProgram(program, [$sparseValues, $sparseIndices], type, uniformData, $denseValues);
      }
  }
  const denseValues = reshape({ inputs: { x: $denseValues }, backend, attrs: { shape: outputShape } });
  backend.disposeData($sparseIndices.dataId);
  backend.disposeData($sparseValues.dataId);
  backend.disposeData($defaultValue.dataId);
  backend.disposeData(zero.dataId);
  backend.disposeData($denseValues.dataId);
  return denseValues;
}
var sparseToDenseConfig = {
  kernelName: SparseToDense,
  backendName: "webgpu",
  kernelFunc: sparseToDense
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/SplitV.js
function splitV(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { numOrSizeSplits, axis } = attrs;
  const $axis = util_exports.parseAxisParam(axis, x.shape)[0];
  const splitSizes = backend_util_exports.prepareSplitSize(x, numOrSizeSplits, $axis);
  const xRank = x.shape.length;
  const begin = new Array(xRank).fill(0);
  const size = x.shape.slice();
  return splitSizes.map((s) => {
    const sliceSize = [...size];
    sliceSize[$axis] = s;
    const sliceT = slice({ inputs: { x }, backend, attrs: { begin, size: sliceSize } });
    begin[$axis] += s;
    return sliceT;
  });
}
var splitVConfig = {
  kernelName: SplitV,
  backendName: "webgpu",
  kernelFunc: splitV
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Sqrt.js
var sqrt = unaryKernelFunc({ opType: UnaryOpType.SQRT });
var sqrtConfig = {
  kernelName: Sqrt,
  backendName: "webgpu",
  kernelFunc: sqrt
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Square.js
var squareConfig = {
  kernelName: Square,
  backendName: "webgpu",
  kernelFunc: ({ inputs, backend }) => {
    const { x } = inputs;
    const webGPUBackend = backend;
    const program = new UnaryOpProgram(x.shape, UnaryOpType.SQUARE);
    return webGPUBackend.runWebGPUProgram(program, [x], x.dtype);
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/SquaredDifference.js
var squaredDifference = binaryKernelFunc({
  opType: BinaryOpType.SQUARED_DIFFERENCE
});
var squaredDifferenceConfig = {
  kernelName: SquaredDifference,
  backendName: "webgpu",
  kernelFunc: squaredDifference
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Step.js
function step({ inputs, attrs, backend }) {
  const { x } = inputs;
  const program = new UnaryOpProgram(x.shape, UnaryOpType.STEP, "stepAlpha : f32,");
  const uniformData = [{ type: "float32", data: [attrs.alpha] }];
  return backend.runWebGPUProgram(program, [x], x.dtype, uniformData);
}
var stepConfig = {
  kernelName: Step,
  backendName: "webgpu",
  kernelFunc: step
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/strided_slice_webgpu.js
var StridedSliceProgram = class {
  constructor(destSize) {
    this.variableNames = ["x"];
    this.workPerThread = 1;
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = destSize;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize, [this.workPerThread, 1, 1]);
    const dtype = getCoordsDataType(this.outputShape.length);
    this.uniforms = `begin : ${dtype},  strides : ${dtype}, `;
    this.shaderKey = "stridedSlice";
  }
  getUserCode() {
    const rank = this.outputShape.length;
    let newCoords = "";
    if (rank === 1) {
      newCoords = "coords * uniforms.strides + uniforms.begin";
    } else {
      let outputAxis = 0;
      newCoords = this.outputShape.map((_, i) => {
        outputAxis++;
        return this.outputShape.length === 1 ? `coords * uniforms.strides[${i}] + uniforms.begin[${i}]` : `coords[${outputAxis - 1}] * uniforms.strides[${i}] + uniforms.begin[${i}]`;
      }).join(",");
    }
    const userCode = `
       ${getMainHeaderString("index")} {
         if (index < uniforms.size) {
           let coords = getCoordsFromIndex(index);
           setOutputAtIndex(index, getX(${newCoords}));
         }
       }
     `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/StridedSlice.js
function stridedSlice(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { begin, end, strides, beginMask, endMask, ellipsisMask, newAxisMask, shrinkAxisMask } = attrs;
  const { finalShapeSparse, finalShape, isIdentity, sliceDim0, isSimpleSlice, begin: $begin, end: $end, strides: $strides } = slice_util_exports.sliceInfo(x.shape, begin, end, strides, beginMask, endMask, ellipsisMask, newAxisMask, shrinkAxisMask);
  let result;
  if (isIdentity) {
    result = reshape({ inputs: { x }, backend, attrs: { shape: finalShape } });
  } else if (sliceDim0 || isSimpleSlice) {
    util_exports.assert(x.shape.length >= 1, () => `Input must have rank at least 1, got: ${x.shape.length}`);
    const size = slice_util_exports.computeOutShape($begin, $end, $strides);
    const sliced = slice({ inputs: { x }, backend, attrs: { begin: $begin, size } });
    result = reshape({ inputs: { x: sliced }, backend, attrs: { shape: finalShape } });
    backend.disposeData(sliced.dataId);
  } else {
    const shouldExecuteOnCPU = backend.shouldExecuteOnCPU([x]);
    if (shouldExecuteOnCPU) {
      const values = backend.readSync(x.dataId);
      const xBuf = buffer(x.shape, x.dtype, values);
      const resultValues = stridedSliceImplCPU(finalShapeSparse, xBuf, $strides, $begin);
      result = backend.makeTensorInfo(finalShape, x.dtype, resultValues.values);
    } else {
      const program = new StridedSliceProgram(finalShapeSparse);
      const uniformData = [{ type: "int32", data: $begin }, { type: "int32", data: $strides }];
      const resultValues = backend.runWebGPUProgram(program, [x], x.dtype, uniformData);
      result = reshape({ inputs: { x: resultValues }, backend, attrs: { shape: finalShape } });
      backend.disposeData(resultValues.dataId);
    }
  }
  return result;
}
var stridedSliceConfig = {
  kernelName: StridedSlice,
  backendName: "webgpu",
  kernelFunc: stridedSlice
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/StringNGrams.js
function stringNGrams(args) {
  const { inputs, backend, attrs } = args;
  const { separator, nGramWidths, leftPad, rightPad, padWidth, preserveShortSequences } = attrs;
  const { data, dataSplits } = inputs;
  const $data = backend.readSync(data.dataId);
  const $dataSplits = backend.readSync(dataSplits.dataId);
  const [nGrams, nGramsSplits] = stringNGramsImplCPU($data, $dataSplits, separator, nGramWidths, leftPad, rightPad, padWidth, preserveShortSequences);
  return [
    backend.makeTensorInfo([nGrams.length], "string", nGrams),
    backend.makeTensorInfo(dataSplits.shape, "int32", nGramsSplits)
  ];
}
var stringNGramsConfig = {
  kernelName: StringNGrams,
  backendName: "webgpu",
  kernelFunc: stringNGrams
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Tan.js
var tan = unaryKernelFunc({ opType: UnaryOpType.TAN });
var tanConfig = {
  kernelName: Tan,
  backendName: "webgpu",
  kernelFunc: tan
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Tanh.js
var tanh = unaryKernelFunc({ opType: UnaryOpType.TANH });
var tanhConfig = {
  kernelName: Tanh,
  backendName: "webgpu",
  kernelFunc: tanh
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/top_k_webgpu.js
var SwapProgram = class {
  constructor(shape) {
    this.variableNames = ["x", "indices"];
    this.workgroupSize = [256, 1, 1];
    this.size = true;
    this.outputShape = shape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.uniforms = `inputSize : i32, firstPass : i32, negativeInf : f32,
        dir : i32, inc : i32,`;
    this.shaderKey = "swap";
  }
  getUserCode() {
    const userCode = `
        ${getMainHeaderString("index")} {
          if (index < uniforms.size) {
            let outC = getCoordsFromIndex(index);
            let batch = outC[0];
            let elemIdx = outC[1];
            // We compare elements pair-wise within a group of size 2 * inc.
            // The comparing rule for each group alternates between ascending
            // and descending. Within each group, we compare each pair at
            // positions i and i+inc. To decide whether an element at position i
            // is x0 or x1, we mod it by 2 * inc, if the result is smaller than
            // inc, it is in the first half of the group, we denote it as x0,
            // otherwise we denote it as x1.
            // For example, as shown in the Bitonic top K paper referenced
            // above, Figure5(a) shows that element[1] is in the second half of
            // the group when group size is 2, but it is in the first half of
            // the group when group size is 4.
            let isFirstInPair = elemIdx % (2 * uniforms.inc) < uniforms.inc;
            var i = 0;
            if (isFirstInPair) {
              i = elemIdx;
            } else {
              i = elemIdx - uniforms.inc;
            }

            var i0 = 0;
            if (uniforms.firstPass == 1) {
              i0 = i;
            } else {
              i0 = i32(getIndices(batch, i));
            }

            var i1 = 0;
            if (uniforms.firstPass == 1) {
              i1 = i + uniforms.inc;
            } else {
              i1 = i32(getIndices(batch, i + uniforms.inc));
            }

            var x0 = f32(0.0);
            var x1 = f32(0.0);
            if (i0 < uniforms.inputSize) {
              x0 = getX(batch, i0);
            } else {
              x0 = uniforms.negativeInf;
            }
            if (i1 < uniforms.inputSize) {
              x1 = getX(batch, i1);
            } else {
              x1 = uniforms.negativeInf;
            }

            let reverse = elemIdx % (2 * uniforms.dir) >= uniforms.dir;
            let isGreater = x0 > x1 || (x0 == x1 && i1 > i0);
            if (reverse == isGreater) {
              // Elements in opposite order of direction
              let iTemp = i0;
              i0 = i1;
              i1 = iTemp;
            }
            if (isFirstInPair) {
              setOutputAtIndex(index, f32(i0));
            } else {
              setOutputAtIndex(index, f32(i1));
            }
          }
        }
      `;
    return userCode;
  }
};
var MergeProgram = class {
  constructor(shape) {
    this.variableNames = ["x", "indices"];
    this.workgroupSize = [256, 1, 1];
    this.size = true;
    this.outputShape = shape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.uniforms = `inputSize : i32, firstPass : i32, k : i32,`;
    this.shaderKey = "merge";
  }
  getUserCode() {
    const userCode = `
        ${getMainHeaderString("index")} {
          if (index < uniforms.size) {
            let outC = getCoordsFromIndex(index);
            let batch = outC[0];
            let elemIdx = outC[1];
            // The output size is half of the previous size.
            // If the previous sequence is | | | | _ _ _ _  | | | |  _ _ _ _
            // (k=4), we only need to output the indices at positions |, the
            // indices at positions _ can be thrown away, see Figure5(b) After
            // Phase 2 (Merge phase) in the Bitonic Top K paper referenced
            // above.
            // For example, the paper shows we only need to output the orange
            // bars. The output sequence should look like this | | | | | | | |.
            // Because the sequence is halved, to map the output index back to
            // the previous sequence to find the corresponding value, we need
            // to double the index. When we double the index, we basically
            // interpolate a position, so 2i looks like
            // | _ | _ | _ | _ | _ | _ | _. We move the | to the first k
            // position of each 2k positions by - elemIdx % k. E.g. for output
            // at index 4,5,6,7, we want to get the corresponding element at
            // original index 8,9,10,11, for output at index 8,9,10,11,
            // we want to get the corresponding element at original index
            // 16,17,18,19, so on and so forth.

            var i = 0;
            if (elemIdx < uniforms.k) {
              i = elemIdx;
            } else {
              i = elemIdx * 2 - elemIdx % uniforms.k;
            }
            var i0 = 0;
            if (uniforms.firstPass == 1) {
              i0 = i;
            } else {
              i0 = i32(getIndices(batch, i));
            }
            var i1 = 0;
            if (uniforms.firstPass == 1) {
              i1 = i + uniforms.k;
            } else {
              i1 = i32(getIndices(batch, i + uniforms.k));
            }

            let x0 = getX(batch, i0);
            var x1 = f32(0.0);
            if (i1 < uniforms.inputSize) {
              x1 = getX(batch, i1);
            } else {
              x1 = x0;
            }

            if (x0 >= x1) {
              setOutputAtIndex(index, f32(i0));
            } else {
              setOutputAtIndex(index, f32(i1));
            }
          }
        }
      `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/TopK.js
function disposeIntermediateTensorInfoOrNull(backend, tensorInfo) {
  if (tensorInfo !== null) {
    backend.disposeData(tensorInfo.dataId);
  }
}
function roundUpToPow2(num) {
  let pow2 = 1;
  while (pow2 < num) {
    pow2 *= 2;
  }
  return pow2;
}
function topK(args) {
  const { inputs, backend, attrs } = args;
  const { x } = inputs;
  const { k, sorted } = attrs;
  const xShape = x.shape;
  const lastDim = xShape[xShape.length - 1];
  if (backend.shouldExecuteOnCPU([x])) {
    const xVals = backend.readSync(x.dataId);
    const [allTopKVals, allTopKIndices] = topKImplCPU(xVals, xShape, x.dtype, k, sorted);
    return [
      backend.makeTensorInfo(allTopKVals.shape, allTopKVals.dtype, allTopKVals.values),
      backend.makeTensorInfo(allTopKIndices.shape, allTopKIndices.dtype, allTopKIndices.values)
    ];
  }
  if (k === 0) {
    xShape[xShape.length - 1] = 0;
    return [
      backend.makeTensorInfo(xShape, x.dtype, []),
      backend.makeTensorInfo(xShape, "int32", [])
    ];
  }
  if (lastDim === 1) {
    return [
      x,
      fill({ attrs: { shape: xShape, dtype: "int32", value: 0 }, backend })
    ];
  }
  const xSize = util_exports.sizeFromShape(xShape);
  const batch = xSize / lastDim;
  const x2D = reshape({ inputs: { x }, attrs: { shape: [batch, lastDim] }, backend });
  const kPow2 = roundUpToPow2(k);
  const lastDimPow2 = roundUpToPow2(lastDim);
  let indices = null;
  const getInputs = () => indices === null ? [x2D, x2D] : [x2D, indices];
  const runSwap = (dir, inc, shape) => {
    const inputs2 = getInputs();
    const program = new SwapProgram(shape);
    const firstPass = indices === null ? 1 : 0;
    const uniformDataSwap = [
      { type: "int32", data: [lastDim] },
      { type: "int32", data: [firstPass] },
      { type: "float32", data: [Number.NEGATIVE_INFINITY] },
      { type: "int32", data: [dir] },
      { type: "int32", data: [inc] }
    ];
    const prevIndices2 = indices;
    indices = backend.runWebGPUProgram(program, inputs2, "int32", uniformDataSwap);
    disposeIntermediateTensorInfoOrNull(backend, prevIndices2);
  };
  for (let len = 1; len < kPow2; len *= 2) {
    const dir = len * 2;
    for (let inc = len; inc >= 1; inc /= 2) {
      runSwap(dir, inc, [batch, lastDimPow2]);
    }
  }
  for (let indicesSize = lastDimPow2; indicesSize > kPow2; indicesSize /= 2) {
    const inputs2 = getInputs();
    const mergeProgram = new MergeProgram([batch, indicesSize / 2]);
    const firstPass = indices === null ? 1 : 0;
    const uniformDataMerge = [
      { type: "int32", data: [lastDim] },
      { type: "int32", data: [firstPass] },
      { type: "int32", data: [kPow2] }
    ];
    const prevIndices2 = indices;
    indices = backend.runWebGPUProgram(mergeProgram, inputs2, "int32", uniformDataMerge);
    disposeIntermediateTensorInfoOrNull(backend, prevIndices2);
    const len = kPow2 / 2;
    const dir = len * 2;
    for (let inc = len; inc >= 1; inc /= 2) {
      runSwap(dir, inc, indices.shape);
    }
  }
  let prevIndices = indices;
  indices = slice({ inputs: { x: indices }, backend, attrs: { begin: 0, size: [batch, k] } });
  disposeIntermediateTensorInfoOrNull(backend, prevIndices);
  let values = gatherV2({ inputs: { x: x2D, indices }, backend, attrs: { axis: 1, batchDims: 1 } });
  disposeIntermediateTensorInfoOrNull(backend, x2D);
  const newShape = xShape.slice(0, -1);
  newShape.push(k);
  prevIndices = indices;
  indices = reshape({ inputs: { x: indices }, attrs: { shape: newShape }, backend });
  disposeIntermediateTensorInfoOrNull(backend, prevIndices);
  const prevValues = values;
  values = reshape({ inputs: { x: values }, attrs: { shape: newShape }, backend });
  disposeIntermediateTensorInfoOrNull(backend, prevValues);
  return [values, indices];
}
var topKConfig = {
  kernelName: TopK,
  backendName: "webgpu",
  kernelFunc: topK
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/transform_webgpu.js
var TransformProgram = class {
  constructor(outShape) {
    this.variableNames = ["Image", "Transforms"];
    this.uniforms = "interpolationModeId : i32, fillModeId : i32, fillValue : f32,";
    this.workgroupSize = [64, 1, 1];
    this.size = true;
    this.outputShape = outShape;
    this.dispatchLayout = flatDispatchLayout(this.outputShape);
    this.dispatch = computeDispatch(this.dispatchLayout, this.outputShape, this.workgroupSize);
    this.shaderKey = "transform";
  }
  getUserCode() {
    const userCode = `
          fn mapCoord(outCoord : f32, len : f32) -> f32{
            var inCoord = outCoord;
            if(uniforms.fillModeId == 2) {
              if (inCoord < 0.0) {
                if (len <= 1.0) {
                  inCoord = 0.0;
                } else {
                  let sz2 = 2.0 * len;
                  if (inCoord < sz2) {
                    inCoord = sz2 * f32(i32(f32(-inCoord / sz2))) +
                    inCoord;
                  }
                  if (inCoord < -len) {
                    inCoord = inCoord + sz2;
                  } else {
                    inCoord = -inCoord - 1.0;
                  }
                }
              } else if (inCoord > len - 1.0) {
                if (len <= 1.0) {
                  inCoord = 0.0;
                } else {
                  let sz2 = 2.0 * len;
                  inCoord = inCoord - sz2 * f32(i32(f32(inCoord / sz2)));
                  if (inCoord >= len) {
                    inCoord = sz2 - inCoord - 1.0;
                  }
                }
              }
              return clamp(inCoord, 0.0, len - 1.0);
            } else if (uniforms.fillModeId == 3) {
              if (inCoord < 0.0) {
                if (len <= 1.0) {
                  inCoord = 0.0;
                } else {
                  let sz = len - 1.0;
                  inCoord = inCoord + len * (f32(i32(f32(-inCoord / sz))) + 1.0);
                }
              } else if (inCoord > len - 1.0) {
                if (len <= 1.0) {
                  inCoord = 0.0;
                } else {
                  let sz = len - 1.0;
                  inCoord = inCoord - len * f32(i32(f32(inCoord / sz)));
                }
              }
              return clamp(inCoord, 0.0, len - 1.0);
            } else if (uniforms.fillModeId == 4) {
              return clamp(outCoord, 0.0, len - 1.0);
            }
            return outCoord;
          }
          fn readWithFillValue(batch : i32, coordY : i32, coordX : i32,
            channel : i32) -> f32 {
            var outputValue : f32;
            if (0 <= coordY && coordY < uniforms.imageShape[1] && 0 <= coordX && coordX < uniforms.imageShape[2]) {
                outputValue = getImage(batch, coordY, coordX, channel);
            } else {
              outputValue = uniforms.fillValue;
            }
            return outputValue;
          }

          ${getMainHeaderString("index")} {
            if (index < uniforms.size) {
              let coords = getCoordsFromIndex(index);
              var outputValue : f32;
              let batch = coords[0];
              let x = coords[2];
              let y = coords[1];
              let channel = coords[3];
              let xf = f32(x);
              let yf = f32(y);
              let a1 = getTransforms(batch, 0);
              let a2 = getTransforms(batch, 1);
              let a3 = getTransforms(batch, 2);
              let b1 = getTransforms(batch, 3);
              let b2 = getTransforms(batch, 4);
              let b3 = getTransforms(batch, 5);
              let c1 = getTransforms(batch, 6);
              let c2 = getTransforms(batch, 7);
              let projection = c1 * xf + c2 * yf + 1.0;
              if (projection == 0.0) {
                outputValue = uniforms.fillValue;
              } else {
                let inX = (a1 * xf + a2 * yf + a3) / projection;
                let inY = (b1 * xf + b2 * yf + b3) / projection;
                let mapX = mapCoord(inX, f32(uniforms.imageShape[2]));
                let mapY = mapCoord(inY, f32(uniforms.imageShape[1]));

                if (uniforms.interpolationModeId == 1) {
                  let coordY = i32(round(mapY));
                  let coordX = i32(round(mapX));
                  outputValue = readWithFillValue(batch, coordY, coordX,
                    channel);
                } else {
                  let yFloor = floor(mapY);
                  let xFloor = floor(mapX);
                  let yCeil = yFloor + 1.0;
                  let xCeil = xFloor + 1.0;
                  let valueYFloor = (xCeil - mapX) *
                  readWithFillValue(batch, i32(yFloor), i32(xFloor), channel) +
                  (mapX - xFloor) *
                  readWithFillValue(batch, i32(yFloor), i32(xCeil), channel);
                  let valueYCeil = (xCeil - mapX) *
                  readWithFillValue(batch, i32(yCeil), i32(xFloor), channel) +
                  (mapX - xFloor) *
                  readWithFillValue(batch, i32(yCeil), i32(xCeil), channel);
                  outputValue = (yCeil - mapY) * valueYFloor +
                  (mapY - yFloor) * valueYCeil;
                }
              }
              setOutputAtIndex(index, outputValue);
            }
          }
        `;
    return userCode;
  }
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Transform.js
function transform(args) {
  const { inputs, backend, attrs } = args;
  const { image, transforms } = inputs;
  const { interpolation, fillMode, fillValue, outputShape } = attrs;
  const [batch, imageHeight, imageWidth, numChannels] = image.shape;
  const [outHeight, outWidth] = outputShape != null ? outputShape : [imageHeight, imageWidth];
  const outShape = [
    batch,
    outHeight,
    outWidth,
    numChannels
  ];
  const program = new TransformProgram(outShape);
  const interpolationModeId = interpolation === "nearest" ? 1 : 2;
  let fillModeId;
  switch (fillMode) {
    case "constant":
      fillModeId = 1;
      break;
    case "reflect":
      fillModeId = 2;
      break;
    case "wrap":
      fillModeId = 3;
      break;
    case "nearest":
      fillModeId = 4;
      break;
    default:
      fillModeId = 1;
      break;
  }
  const uniformData = [
    { type: "int32", data: [interpolationModeId] },
    { type: "int32", data: [fillModeId] },
    { type: "float32", data: [fillValue] }
  ];
  return backend.runWebGPUProgram(program, [image, transforms], "float32", uniformData);
}
var transformConfig = {
  kernelName: Transform,
  backendName: "webgpu",
  kernelFunc: transform
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/kernels/Unpack.js
function unpack(args) {
  const { inputs, backend, attrs } = args;
  const { value } = inputs;
  let { axis } = attrs;
  if (axis < 0) {
    axis += value.shape.length;
  }
  const x = value;
  const xRank = x.shape.length;
  const num = value.shape[axis];
  const outShape = new Array(xRank - 1);
  let outIndex = 0;
  for (let i = 0; i < xRank; i++) {
    if (i !== axis) {
      outShape[outIndex++] = x.shape[i];
    }
  }
  const toDispose = [];
  const begin = new Array(xRank).fill(0);
  const size = x.shape.slice();
  size[axis] = 1;
  const res = new Array(num);
  for (let i = 0; i < res.length; i++) {
    begin[axis] = i;
    const sliced = slice({ inputs: { x }, backend, attrs: { begin, size } });
    const reshaped = reshape({ inputs: { x: sliced }, backend, attrs: { shape: outShape } });
    res[i] = reshaped;
    toDispose.push(sliced);
  }
  toDispose.forEach((t) => backend.disposeData(t.dataId));
  return res;
}
var unpackConfig = {
  kernelName: Unpack,
  backendName: "webgpu",
  kernelFunc: unpack
};

// node_modules/@tensorflow/tfjs-backend-webgpu/dist/register_all_kernels.js
var kernelConfigs = [
  _fusedMatMulConfig,
  absConfig,
  acosConfig,
  acoshConfig,
  addConfig,
  addNConfig,
  allConfig,
  anyConfig,
  argMaxConfig,
  argMinConfig,
  asinConfig,
  asinhConfig,
  atanConfig,
  atan2Config,
  atanhConfig,
  avgPoolConfig,
  avgPoolGradConfig,
  batchMatMulConfig,
  batchToSpaceNDConfig,
  bincountConfig,
  castConfig,
  ceilConfig,
  clipByValueConfig,
  complexConfig,
  concatConfig,
  conv2DConfig,
  conv2DBackpropFilterConfig,
  conv2DBackpropInputConfig,
  cosConfig,
  coshConfig,
  cropAndResizeConfig,
  cumprodConfig,
  cumsumConfig,
  denseBincountConfig,
  depthToSpaceConfig,
  depthwiseConv2dNativeConfig,
  diagConfig,
  dilation2DConfig,
  einsumConfig,
  eluConfig,
  equalConfig,
  erfConfig,
  expConfig,
  expandDimsConfig,
  expm1Config,
  fftConfig,
  fillConfig,
  flipLeftRightConfig,
  fromPixelsConfig,
  floorConfig,
  floorDivConfig,
  fusedBatchNormConfig,
  fusedConv2DConfig,
  fusedDepthwiseConv2DConfig,
  gatherNdConfig,
  gatherV2Config,
  greaterConfig,
  greaterEqualConfig,
  identityConfig,
  ifftConfig,
  imagConfig,
  isFiniteConfig,
  isInfConfig,
  isNaNConfig,
  leakyReluConfig,
  lessConfig,
  lessEqualConfig,
  linSpaceConfig,
  log1pConfig,
  logConfig,
  logicalAndConfig,
  logicalNotConfig,
  logicalOrConfig,
  lrnConfig,
  maxConfig,
  maximumConfig,
  maxPoolConfig,
  meanConfig,
  minConfig,
  minimumConfig,
  mirrorPadConfig,
  modConfig,
  multiplyConfig,
  negConfig,
  nonMaxSuppressionV3Config,
  nonMaxSuppressionV5Config,
  notEqualConfig,
  oneHotConfig,
  onesLikeConfig,
  packConfig,
  padV2Config,
  powConfig,
  preluConfig,
  prodConfig,
  rangeConfig,
  realConfig,
  realDivConfig,
  reciprocalConfig,
  reluConfig,
  relu6Config,
  reshapeConfig,
  resizeBilinearConfig,
  resizeNearestNeighborConfig,
  reverseConfig,
  rotateWithOffsetConfig,
  roundConfig,
  rsqrtConfig,
  scatterNdConfig,
  searchSortedConfig,
  selectConfig,
  seluConfig,
  sigmoidConfig,
  signConfig,
  sinConfig,
  sinhConfig,
  sliceConfig,
  stepConfig,
  stridedSliceConfig,
  stringNGramsConfig,
  softmaxConfig,
  softplusConfig,
  spaceToBatchNDConfig,
  sparseToDenseConfig,
  splitVConfig,
  sqrtConfig,
  squareConfig,
  squaredDifferenceConfig,
  subConfig,
  sumConfig,
  tanConfig,
  tanhConfig,
  tileConfig,
  topKConfig,
  transformConfig,
  transposeConfig,
  unpackConfig,
  zerosLikeConfig
];
for (const kernelConfig of kernelConfigs) {
  registerKernel(kernelConfig);
}
export {
  WebGPUBackend,
  webgpu_util_exports as webgpu_util
};
/*! Bundled license information:

@tensorflow/tfjs-backend-webgpu/dist/flags_webgpu.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/adapter_info.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/buffer_manager.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/texture_manager.js:
  (**
   * @license
   * Copyright 2022 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/shader_util.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/webgpu_program.js:
  (**
   * @license
   * Copyright 2022 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/webgpu_util.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/backend_webgpu.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/webgpu.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/base.js:
  (**
   * @license
   * Copyright 2022 Google Inc. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/binary_op_util.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/unary_op_util.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/activation_util.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/matmul_packed_webgpu.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/matmul_reduce_webgpu.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/matmul_small_output_size_webgpu.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/matmul_splitK_webgpu.js:
  (**
   * @license
   * Copyright 2022 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/fill_webgpu.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Fill.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Reshape.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/BatchMatMul_impl.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/_FusedMatMul.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/binary_op_complex_webgpu.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/binary_op_webgpu.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Identity.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Complex.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/unary_op_webgpu.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernel_utils/kernel_funcs_utils.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernel_utils/shared.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Abs.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Acos.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Acosh.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Add.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/addn_packed_webgpu.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/AddN.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/transpose_shared_webgpu.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/transpose_webgpu.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Transpose.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/reduce_webgpu.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernel_utils/reduce.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/All.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Any.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/argminmax_webgpu.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/ArgMax.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/ArgMin.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Asin.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Asinh.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Atan.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Atan2.js:
  (**
   * @license
   * Copyright 2022 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Atanh.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/pool2d_webgpu.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/pool_filtersizeone_webgpu.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Max.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Mean.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Pool_impl.js:
  (**
   * @license
   * Copyright 2022 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/AvgPool.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/avg_pool2d_backprop_webgpu.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/AvgPoolGrad.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/BatchMatMul.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/slice_webgpu.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Slice.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/BatchToSpaceND.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/bincount_webgpu.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Bincount.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/NotEqual.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Real.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernel_utils/int.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Cast.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Ceil.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/clip_vec4_webgpu.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/clip_webgpu.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/ClipByValue.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/concat_webgpu.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Imag.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Concat_impl.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Concat.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/conv2d_mm_webgpu.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/conv2d_naive_webgpu.js:
  (**
   * @license
   * Copyright 2022 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/im2col_webgpu.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Conv2D_impl.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Conv2D.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/conv_backprop_webgpu.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Conv2DBackpropFilter.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/conv_backprop_mm_webgpu.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Conv2DBackpropInput.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Cos.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Cosh.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/crop_and_resize_webgpu.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/CropAndResize.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/cum_webgpu.js:
  (**
   * @license
   * Copyright 2022 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Cum_impl.js:
  (**
   * @license
   * Copyright 2022 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Cumprod.js:
  (**
   * @license
   * Copyright 2022 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Cumsum.js:
  (**
   * @license
   * Copyright 2022 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/DenseBincount.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/depth_to_space_webgpu.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/DepthToSpace.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/depthwise_conv2d_nchw_shared_webgpu.js:
  (**
   * @license
   * Copyright 2022 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/depthwise_conv2d_vec4_webgpu.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/depthwise_conv2d_webgpu.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/DepthwiseConv2dNative.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/diag_webgpu.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Diag.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/dilation_webgpu.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Dilation2D.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Multiply.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Sum.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Einsum.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Elu.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Equal.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Erf.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Exp.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/ExpandDims.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the License);
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an AS IS BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Expm1.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/fft_webgpu.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/FFT_impl.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/FFT.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/flip_left_right_webgpu.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/FlipLeftRight.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Floor.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/FloorDiv.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/from_pixels_webgpu.js:
  (**
   * @license
   * Copyright 2022 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/FromPixels.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use backend file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/batchnorm_webgpu.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/FusedBatchNorm.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/FusedConv2D.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/FusedDepthwiseConv2D.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/gather_nd_webgpu.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/GatherNd.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/gather_webgpu.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/GatherV2.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Greater.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/GreaterEqual.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/IFFT.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/IsFinite.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/IsInf.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/IsNaN.js:
  (**
   * @license
   * Copyright 2022 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/LeakyRelu.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Less.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/LessEqual.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/lin_space_webgpu.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/LinSpace.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Log.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Log1p.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/LogicalAnd.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/LogicalNot.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/LogicalOr.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/lrn_webgpu.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/LRN.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Maximum.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/MaxPool.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Min.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Minimum.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/mirror_pad_webgpu.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/MirrorPad.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Mod.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Neg.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/NonMaxSuppressionV3.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/NonMaxSuppressionV5.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/onehot_webgpu.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/OneHot.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/ZerosLike.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/OnesLike.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Pack.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/pad_webgpu.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/PadV2.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Pow.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Prelu.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Prod.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Range.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/RealDiv.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Reciprocal.js:
  (**
   * @license
   * Copyright 2022 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Relu.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Relu6.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/resize_bilinear_webgpu.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/ResizeBilinear.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/resize_nearest_neighbor_webgpu.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/ResizeNearestNeighbor.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/reverse_webgpu.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Reverse.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/rotate_webgpu.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/RotateWithOffset.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Round.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Rsqrt.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/scatter_webgpu.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/ScatterNd.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/search_sorted_webgpu.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/SearchSorted.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/select_webgpu.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Select.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Selu.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Sigmoid.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Sign.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Sin.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Sinh.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Sub.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Softmax.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Softplus.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/SpaceToBatchND.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/tile_webgpu.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Tile.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/SparseToDense.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/SplitV.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Sqrt.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Square.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/SquaredDifference.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Step.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/strided_slice_webgpu.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/StridedSlice.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/StringNGrams.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Tan.js:
  (**
   * @license
   * Copyright 2022 Google LLC.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Tanh.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/top_k_webgpu.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/TopK.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/transform_webgpu.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Transform.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/kernels/Unpack.js:
  (**
   * @license
   * Copyright 2021 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/register_all_kernels.js:
  (**
   * @license
   * Copyright 2020 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)

@tensorflow/tfjs-backend-webgpu/dist/index.js:
  (**
   * @license
   * Copyright 2019 Google LLC. All Rights Reserved.
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   * http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   * =============================================================================
   *)
*/
//# sourceMappingURL=@tensorflow_tfjs-backend-webgpu.js.map
